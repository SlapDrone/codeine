{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac8c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import os\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "import nest_asyncio\n",
    "import langchain.schema\n",
    "from llama_index import download_loader, GPTVectorStoreIndex\n",
    "from llama_index.readers import GithubRepositoryReader\n",
    "#from llama_index.readers.llamahub_modules.github_repo import GithubClient, GithubRepositoryReader\n",
    "nest_asyncio.apply()\n",
    "#download_loader(\"GithubRepositoryReader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f01de4",
   "metadata": {},
   "source": [
    "# [Building a Chatbot](https://gpt-index.readthedocs.io/en/latest/guides/tutorials/building_a_chatbot.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e4d979",
   "metadata": {},
   "source": [
    "## Get documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5746a8a5",
   "metadata": {},
   "source": [
    "Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb6ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7165de0",
   "metadata": {},
   "source": [
    "Imports:\n",
    "\n",
    "### [`StorageContext`](https://github.com/jerryjliu/llama_index/blob/main/llama_index/storage/storage_context.py) \n",
    "...is a utility container for storing nodes, indices and vectors.\n",
    "\n",
    "Has attributes:\n",
    "\n",
    "- `docstore` (default: `SimpleDocumentStore()`)\n",
    "- `index_store` (default: `SimpleIndexStore()`)\n",
    "- `vector_store` (default: `SimpleVectorStore()`)\n",
    "    \n",
    "And persists these to/loads these from a file.\n",
    "\n",
    "### [`ServiceContext`](https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/service_context.py) \n",
    "...is a utility container for index and query classes. \n",
    "\n",
    "Has attributes:\n",
    "\n",
    "- `llm_predictor` (default: `LLMPredictor()`)\n",
    "- `prompt_helper` (default: `PromptHelper.from_llm_predictor(..., chunk_size_limit)`)\n",
    "- `embed_model` (default: `OpenAIEmbedding()`)\n",
    "    - Not used in all indices\n",
    "- `node_parser` (default: `SimpleNodeParser(text_splitter=TokenTextSplitter(...), callback_manager=...)`)\n",
    "    - Splits a `Document` into `Node`s using a `TextSplitter`\n",
    "- `llama_logger` (default: `LlamaLogger()`)\n",
    "- `callback_manager` (default: `CallbackManager()`)\n",
    "- `chunk_size_limit` (default: `none`) \n",
    "    - Goes to the node parser and prompt helper\n",
    "    - The `SimpleNodeParser` has `text_splitter` equal to `TokenTextSplitter` with default chunk size if None, otherwise the chunk_size_limit arg is passed to it\n",
    "    - The `PromptHelper` also instantiates a `TokenTextSplitter` with it.\n",
    "\n",
    "\n",
    "So by instantiating one of these you basically have an object for interfaces to *all* related services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451309e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    download_loader,\n",
    "    GPTVectorStoreIndex,\n",
    "    ServiceContext, \n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42782e6",
   "metadata": {},
   "source": [
    "Create Documents from files in Github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b4e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = None\n",
    "if os.path.exists(\"docs.pkl\"):\n",
    "    with open(\"docs.pkl\", \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "\n",
    "if docs is None:\n",
    "    github_client = GithubClient(os.getenv(\"GITHUB_TOKEN\"))\n",
    "    loader = GithubRepositoryReader(\n",
    "        github_client,\n",
    "        owner =                  \"microsoft\",\n",
    "        repo =                   \"Cream\",\n",
    "        filter_directories =     ([\"TinyViT\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
    "        filter_file_extensions = ([\".py\", \".md\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
    "        verbose =                True,\n",
    "        concurrent_requests =    10,\n",
    "    )\n",
    "\n",
    "    docs = loader.load_data(branch=\"main\")\n",
    "\n",
    "    with open(\"docs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(docs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3f2df",
   "metadata": {},
   "source": [
    "## Create vector index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef88763",
   "metadata": {},
   "source": [
    "Setting up vector indices for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3da2fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser.simple import SimpleNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c92d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 168176 tokens\n"
     ]
    }
   ],
   "source": [
    "embeddings_storage = Path(f\"./storage/TinyViT\")\n",
    "\n",
    "if embeddings_storage.exists():\n",
    "    # load document/index/vector store from storage\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=embeddings_storage)\n",
    "    cur_index = load_index_from_storage(storage_context=storage_context)\n",
    "else:\n",
    "    # generate document/index/vector store from documents, and store\n",
    "    # we need an embedding model, hence we create a service context\n",
    "    # the chunk_size_limit internally propagates to the textsplitter (which in turn goes to \n",
    "    # the node parser) and limits the number of tokens in each node\n",
    "    chunk_size_limit, chunk_overlap = 768, 64\n",
    "    text_splitter = TokenTextSplitter(chunk_size=chunk_size_limit, chunk_overlap=chunk_overlap)\n",
    "    node_parser = SimpleNodeParser(text_splitter=text_splitter)\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        node_parser=node_parser,\n",
    "        chunk_size_limit=chunk_size_limit\n",
    "    ) \n",
    "    #index_set = {}\n",
    "\n",
    "    # just a flat index for now, could loop and create multiple, hierarchically for example\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    # from_documents uses the service_context's nodeparser to translate documents into nodes\n",
    "\n",
    "    cur_index = GPTVectorStoreIndex.from_documents(\n",
    "        docs,#doc_set[key]\n",
    "        service_context=service_context,\n",
    "        storage_context=storage_context\n",
    "    )\n",
    "    #index_set[key] = cur_index\n",
    "    storage_context.persist(persist_dir=embeddings_storage)\n",
    "    \n",
    "# since we're just doing one\n",
    "index_set = cur_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca274c",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "\n",
    "### Here we select subset of nodes by semantic similarity from vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "016aabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    ResponseSynthesizer,\n",
    "    LLMPredictor\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "#index = GPTVectorStoreIndex.from_documents(docs, storage_context=...)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=cur_index,\n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248cc717",
   "metadata": {},
   "source": [
    "![bla bla](./vector_store_query.webp \"this\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bce714",
   "metadata": {},
   "source": [
    "## ResponseSynthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182694ed",
   "metadata": {},
   "source": [
    "### Decide how to process nodes returned by retriever into answer\n",
    "\n",
    "[ResponseMode](https://gpt-index.readthedocs.io/en/latest/reference/query/response_synthesizer.html#llama_index.indices.response.type.ResponseMode)\n",
    "\n",
    "Response modes of the response builder (and synthesizer).\n",
    "\n",
    "ACCUMULATE = 'accumulate'\n",
    "\n",
    "    Synthesize a response for each text chunk, and then return the concatenation.\n",
    "\n",
    "COMPACT = 'compact'\n",
    "\n",
    "    Compact and refine mode first combine text chunks into larger consolidated chunks that more fully utilize the available context window, then refine answers across them. This mode is faster than refine since we make fewer calls to the LLM.\n",
    "\n",
    "GENERATION = 'generation'\n",
    "\n",
    "    Ignore context, just use LLM to generate a response.\n",
    "\n",
    "NO_TEXT = 'no_text'\n",
    "\n",
    "    Return the retrieved context nodes, without synthesizing a final response.\n",
    "\n",
    "REFINE = 'refine'\n",
    "\n",
    "    Refine is an iterative way of generating a response. We first use the context in the first node, along with the query, to generate an initial answer. We then pass this answer, the query, and the context of the second node as input into a “refine prompt” to generate a refined answer. We refine through N-1 nodes, where N is the total number of nodes.\n",
    "\n",
    "SIMPLE_SUMMARIZE = 'simple_summarize'\n",
    "\n",
    "    Merge all text chunks into one, and make a LLM call. This will fail if the merged text chunk exceeds the context window size.\n",
    "\n",
    "TREE_SUMMARIZE = 'tree_summarize'\n",
    "\n",
    "    Build a tree index over the set of candidate nodes, with a summary prompt seeded with the query. The tree is built in a bottoms-up fashion, and in the end the root node is returned as the response\n",
    "\n",
    "\n",
    "Constructing a `ResponseSynthesizer` with a given `response_mode` will build one of the  `BaseResponseBuilder` subclasses that will handle the synthesis logic. These are delegated to [here](https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/response/factory.py#L24).\n",
    "\n",
    "Each of these interally use prompt templates for e.g. \"summarisation\" and \"refine\" steps which can be tweaked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4be0a2d",
   "metadata": {},
   "source": [
    "We may use a different `ServiceContext` here because we can choose to involve an LLM in the response synthesis.\n",
    "\n",
    "Internally a `PromptHelper` is either passed explicitly or created `from_llm_predictor`. This takes a `chunk_size_limit`, which is used to figure out how many nodes can be squeezed into one prompt, so this can be taken from the same service context with which we generated the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a58ad",
   "metadata": {},
   "source": [
    "\n",
    "`compact` is a synonym for \"compact and refine\". Here, the retrieved nodes are consolidated into chunks big enough to fill out the prompt. The query is issued with the first chunk of context to get an answer, and then this answer is provided with the original query and the context of the second node, and so on.\n",
    "$$\n",
    "(Q, C1) \\rightarrow_{\\text{LLM}} A1 \\\\\n",
    "(Q, A1, C2) \\rightarrow_{\\text{LLM}} A2 \\\\\n",
    "(Q, A2, C3) \\rightarrow_{\\text{LLM}} A3 \\\\\n",
    "(Q, A3, C4) \\rightarrow_{\\text{LLM}} A4 \\\\\n",
    "$$\n",
    "\n",
    "`tree_summarize` also seems interesting.\n",
    "\n",
    "The summary tree topology in this implementation appears to be a form of hierarchical agglomerative clustering, where each node in the tree represents a chunk of text and the summary of its child nodes. This type of clustering is hierarchical because each chunk of text starts as its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n",
    "\n",
    "Here's how the tree topology is defined in the provided `GPTTreeIndexBuilder` class:\n",
    "\n",
    "- Leaves: The leaf nodes in the tree are chunks of the original input text. These are the smallest units of text that the system works with.\n",
    "\n",
    "- Intermediate Nodes: The intermediate nodes are summaries of their children nodes. Each intermediate node represents a chunk of summarized text that is a summary of the chunks represented by its children. The summarization process is performed by a language model (specified by `llm_predictor` in the service context).\n",
    "\n",
    "- Root: The root of the tree is a single node that represents the entire document. Its text is a summary of all the text in the document. If the tree is fully built, this root node is a summary of its children, which are summaries of their children, and so forth, down to the leaves which are the original chunks of text.\n",
    "\n",
    "The **granularity of the summarization** is controlled by the `num_children` parameter of the `GPTTreeIndexBuilder` class. This parameter determines how many child nodes each parent node should have. By adjusting this parameter, you can control how much text each intermediate node is responsible for summarizing. A smaller num_children value would lead to finer granularity (i.e., more detail and shorter summaries in each node), while a larger value would lead to coarser granularity (i.e., less detail and longer summaries in each node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e133bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ca43e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response synthesis service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=LLMPredictor(ChatOpenAI(temperature=0, model_name=\"gpt-4\")),\n",
    "    chunk_size_limit=cur_index.service_context.chunk_size_limit\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    service_context=service_context,\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=.7)\n",
    "    ],\n",
    "    response_mode=\"compact\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab934ff",
   "metadata": {},
   "source": [
    "Now use Langchain to set up the outer chatbot agent, which has access to a set of `Tool`s.\n",
    "\n",
    "LlamaIndex provides wrappers around indices and graphs so that they can be used within `Tool` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72f8302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from llama_index.langchain_helpers.agents import (\n",
    "    LlamaToolkit,\n",
    "    create_llama_chat_agent,\n",
    "    create_llama_agent,\n",
    "    IndexToolConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0dba89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4aac0",
   "metadata": {},
   "source": [
    "We define a separate tool for each index. We're just using the vector index defined over the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82bab8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_config = IndexToolConfig(\n",
    "    query_engine=query_engine, \n",
    "    name=f\"Vector Index (TinyViT GitHub)\",\n",
    "    description=(\n",
    "        \"useful for when you want to answer queries about the TinyViT vision transformer implementation\"\n",
    "    ),\n",
    "    tool_kwargs={\"return_direct\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410181d",
   "metadata": {},
   "source": [
    "Put all the tool configs into the LlamaToolkit (just one for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ba1dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = LlamaToolkit(\n",
    "    index_configs=[tool_config],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab168ee",
   "metadata": {},
   "source": [
    "Finally, create the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "570eb1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9dff334",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = create_llama_agent(\n",
    "    toolkit=toolkit,\n",
    "    memory=memory,\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    llm=ChatOpenAI(temperature=0, model_name=\"gpt-4\"),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e7fcdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.chat_models.openai:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Vector Index (TinyViT GitHub)\",\n",
      "    \"action_input\": \"training TinyViT\"\n",
      "}\n",
      "```\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 4 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 3621 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mTo train a TinyViT model, follow these steps:\n",
      "\n",
      "1. Save teacher sparse logits: Before training a TinyViT model, you need to save the teacher sparse logits. Follow the instructions in [How to save teacher sparse logits?](./docs/SAVE_TEACHER_LOGITS.md).\n",
      "\n",
      "2. Train a TinyViT model: After saving the teacher sparse logits, you can train a TinyViT model using the proposed fast pretraining distillation. Follow the instructions in [Let's train a TinyViT model](./docs/TRAINING.md).\n",
      "\n",
      "Make sure you have installed the necessary dependencies and prepared the datasets as mentioned in the [Preparation](./docs/PREPARATION.md) guide.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To train a TinyViT model, follow these steps:\\n\\n1. Save teacher sparse logits: Before training a TinyViT model, you need to save the teacher sparse logits. Follow the instructions in [How to save teacher sparse logits?](./docs/SAVE_TEACHER_LOGITS.md).\\n\\n2. Train a TinyViT model: After saving the teacher sparse logits, you can train a TinyViT model using the proposed fast pretraining distillation. Follow the instructions in [Let's train a TinyViT model](./docs/TRAINING.md).\\n\\nMake sure you have installed the necessary dependencies and prepared the datasets as mentioned in the [Preparation](./docs/PREPARATION.md) guide.\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input=\"Tell me about how the TinyViT model is trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0ca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Can you summarize the training instructions explicitly here?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"To train a TinyViT model, follow these steps: 1. Save teacher sparse logits by following the instructions in the TinyViT repository. 2. Train a TinyViT model using the proposed fast pretraining distillation method, as described in the TinyViT repository. Make sure to install necessary dependencies and prepare the datasets as mentioned in the preparation guide.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: To train a TinyViT model, follow these steps: 1. Save teacher sparse logits by following the instructions in the TinyViT repository. 2. Train a TinyViT model using the proposed fast pretraining distillation method, as described in the TinyViT repository. Make sure to install necessary dependencies and prepare the datasets as mentioned in the preparation guide.\n",
      "User: I would like you to elaborate in more detail.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Vector Index (TinyViT GitHub)\",\n",
      "    \"action_input\": \"training instructions\"\n",
      "}\n",
      "```\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 2 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 3604 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mTo train a TinyViT model, follow these steps:\n",
      "\n",
      "1. Install the required dependencies by running:\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "2. Prepare the ImageNet-1k and ImageNet-22k datasets as described in the [Preparation document](./docs/PREPARATION.md).\n",
      "\n",
      "3. Save the teacher sparse logits by following the tutorial in the [Save Teacher Logits document](./docs/SAVE_TEACHER_LOGITS.md).\n",
      "\n",
      "4. Pretrain the model on ImageNet-22k with the proposed fast pretraining distillation. Assuming the teacher sparse soft labels are stored in `./teacher_logits/` and the IN-22k dataset is stored in `./ImageNet-22k`, run the following command:\n",
      "\n",
      "```bash\n",
      "python -m torch.distributed.launch --master_addr=$MASTER_ADDR --nproc_per_node 8 --nnodes=4 --node_rank=$NODE_RANK main.py --cfg configs/22k_distill/tiny_vit_21m_22k_distill.yaml --data-path ./ImageNet-22k --batch-size 128 --output ./output --opts DISTILL.TEACHER_LOGITS_PATH ./teacher_logits/\n",
      "```\n",
      "\n",
      "Replace `$NODE_RANK` and `$MASTER_ADDR` with the rank of the node and the IP address of the master node, respectively.\n",
      "\n",
      "5. Finetune the pretrained model from IN-22k to IN-1k by running:\n",
      "\n",
      "```bash\n",
      "python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/22kto1k/tiny_vit_21m_22kto1k.yaml --data-path ./ImageNet --batch-size 128 --pretrained ./checkpoints/tiny_vit_21m_22k_distill.pth --output ./output\n",
      "```\n",
      "\n",
      "Replace `tiny_vit_21m_22k.pth` with the checkpoint of the pretrained TinyViT-21M on the IN-22k dataset.\n",
      "\n",
      "6. Optionally, finetune the model with higher resolution progressively (224 -> 384 -> 512) by following the instructions in the [Training document](./docs/TRAINING.md).\n",
      "\n",
      "7. To train the model from scratch on ImageNet-1k, run:\n",
      "\n",
      "```bash\n",
      "python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/1k/tiny_vit_21m.yaml --data-path ./ImageNet --batch-size 128 --output ./output\n",
      "```\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: To train a TinyViT model, follow these steps:\n",
      "\n",
      "1. Install the required dependencies by running:\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "2. Prepare the ImageNet-1k and ImageNet-22k datasets as described in the [Preparation document](./docs/PREPARATION.md).\n",
      "\n",
      "3. Save the teacher sparse logits by following the tutorial in the [Save Teacher Logits document](./docs/SAVE_TEACHER_LOGITS.md).\n",
      "\n",
      "4. Pretrain the model on ImageNet-22k with the proposed fast pretraining distillation. Assuming the teacher sparse soft labels are stored in `./teacher_logits/` and the IN-22k dataset is stored in `./ImageNet-22k`, run the following command:\n",
      "\n",
      "```bash\n",
      "python -m torch.distributed.launch --master_addr=$MASTER_ADDR --nproc_per_node 8 --nnodes=4 --node_rank=$NODE_RANK main.py --cfg configs/22k_distill/tiny_vit_21m_22k_distill.yaml --data-path ./ImageNet-22k --batch-size 128 --output ./output --opts DISTILL.TEACHER_LOGITS_PATH ./teacher_logits/\n",
      "```\n",
      "\n",
      "Replace `$NODE_RANK` and `$MASTER_ADDR` with the rank of the node and the IP address of the master node, respectively.\n",
      "\n",
      "5. Finetune the pretrained model from IN-22k to IN-1k by running:\n",
      "\n",
      "```bash\n",
      "python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/22kto1k/tiny_vit_21m_22kto1k.yaml --data-path ./ImageNet --batch-size 128 --pretrained ./checkpoints/tiny_vit_21m_22k_distill.pth --output ./output\n",
      "```\n",
      "\n",
      "Replace `tiny_vit_21m_22k.pth` with the checkpoint of the pretrained TinyViT-21M on the IN-22k dataset.\n",
      "\n",
      "6. Optionally, finetune the model with higher resolution progressively (224 -> 384 -> 512) by following the instructions in the [Training document](./docs/TRAINING.md).\n",
      "\n",
      "7. To train the model from scratch on ImageNet-1k, run:\n",
      "\n",
      "```bash\n",
      "python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/1k/tiny_vit_21m.yaml --data-path ./ImageNet --batch-size 128 --output ./output\n",
      "```\n",
      "User: can you give me a step-by-step explanation of the fast pretraining distillation process?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Vector Index (TinyViT GitHub)\",\n",
      "    \"action_input\": \"fast pretraining distillation\"\n",
      "}\n",
      "```\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 5 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 3474 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mFast pretraining distillation is a method used to train small and efficient vision transformers by transferring knowledge from large pretrained models to smaller ones. This is achieved by saving the sparse soft labels (logits) of the large teacher models in advance, which helps in reducing memory cost and computational overheads. The saved logits can then be used to distill the knowledge into smaller student models without the need for re-forwarding the large teacher model. This approach enables faster and more efficient training of smaller vision transformers while maintaining high accuracy.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: Fast pretraining distillation is a method used to train small and efficient vision transformers by transferring knowledge from large pretrained models to smaller ones. This is achieved by saving the sparse soft labels (logits) of the large teacher models in advance, which helps in reducing memory cost and computational overheads. The saved logits can then be used to distill the knowledge into smaller student models without the need for re-forwarding the large teacher model. This approach enables faster and more efficient training of smaller vision transformers while maintaining high accuracy.\n",
      "User: Can you explain the implementation of the training loop?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Vector Index (TinyViT GitHub)\",\n",
      "    \"action_input\": \"training loop\"\n",
      "}\n",
      "```\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 2 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 3797 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe training loop in the given context can be found in the `train_one_epoch_distill_using_saved_logits` function. It iterates through the data_loader for each epoch, processes the samples and targets, performs forward and backward passes, updates the model parameters, and computes the loss and accuracy metrics.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent: The training loop in the given context can be found in the `train_one_epoch_distill_using_saved_logits` function. It iterates through the data_loader for each epoch, processes the samples and targets, performs forward and backward passes, updates the model parameters, and computes the loss and accuracy metrics.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    text_input = input(\"User: \")\n",
    "    response = agent_chain.run(input=text_input)\n",
    "    print(f'Agent: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc867dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c69b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ed750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec7a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
