{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d39b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import typing as ty\n",
    "\n",
    "import aioitertools\n",
    "from langchain.input import get_color_mapping\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.agent import AgentAction, AgentFinish\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForChainRun\n",
    "from langchain.utilities.asyncio import asyncio_timeout\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from codeine.chatbot import build_chat_engine, service_context\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "chat_engine = build_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a51690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def intermediate_steps_generator(\n",
    "    self,\n",
    "    inputs: dict[str, str],\n",
    "    run_manager: ty.Optional[AsyncCallbackManagerForChainRun] = None,\n",
    ") -> ty.AsyncIterator[tuple[AgentAction, str]]:\n",
    "    \"\"\"Generator function that yields intermediate steps (thoughts, actions, and observations).\"\"\"\n",
    "\n",
    "    logger.debug(\"Starting generator\")\n",
    "\n",
    "    name_to_tool_map = {tool.name: tool for tool in self.tools}\n",
    "    color_mapping = get_color_mapping(\n",
    "        [tool.name for tool in self.tools], excluded_colors=[\"green\"]\n",
    "    )\n",
    "    intermediate_steps: List[Tuple[AgentAction, str]] = []\n",
    "    iterations = 0\n",
    "    time_elapsed = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    async with asyncio_timeout(self.max_execution_time):\n",
    "        try:\n",
    "            while self._should_continue(iterations, time_elapsed):\n",
    "                logger.debug(f\"Iteration {iterations}\")\n",
    "\n",
    "                next_step_output = await self._atake_next_step(\n",
    "                    name_to_tool_map,\n",
    "                    color_mapping,\n",
    "                    inputs,\n",
    "                    intermediate_steps,\n",
    "                    run_manager=run_manager,\n",
    "                )\n",
    "                if isinstance(next_step_output, AgentFinish):\n",
    "                    # Yield the final answer\n",
    "                    final_answer = next_step_output.return_values[\"output\"]\n",
    "                    yield next_step_output, final_answer\n",
    "                    logger.debug(\"Agent finished\")\n",
    "                    break\n",
    "\n",
    "                intermediate_steps.extend(next_step_output)\n",
    "                if len(next_step_output) == 1:\n",
    "                    next_step_action = next_step_output[0]\n",
    "                    tool_return = self._get_tool_return(next_step_action)\n",
    "                    if tool_return is not None:\n",
    "                        logger.debug(\"Tool returned\")\n",
    "                        break\n",
    "\n",
    "                # Yield the latest intermediate step(s)\n",
    "                for step_output in next_step_output:\n",
    "                    logger.debug(f\"Yielding step: {step_output}\")\n",
    "                    yield step_output\n",
    "\n",
    "                iterations += 1\n",
    "                time_elapsed = time.time() - start_time\n",
    "        except TimeoutError:\n",
    "            logger.debug(\"TimeoutError\")\n",
    "            pass\n",
    "    logger.debug(\"Generator finished\")\n",
    "\n",
    "AgentExecutor.intermediate_steps_generator = intermediate_steps_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fad13fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _acall(\n",
    "    self,\n",
    "    inputs: dict[str, str],\n",
    "    run_manager: ty.Optional[AsyncCallbackManagerForChainRun] = None,\n",
    ") -> dict[str, str]:\n",
    "    \"\"\"Run text through and get agent response.\"\"\"\n",
    "    intermediate_steps: list[tuple[AgentAction, str]] = []\n",
    "\n",
    "    async for step in self.intermediate_steps_generator(inputs, run_manager):\n",
    "        intermediate_steps.append(step)\n",
    "\n",
    "    output = self.agent.return_stopped(intermediate_steps)\n",
    "    return output\n",
    "\n",
    "AgentExecutor._acall = _acall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7be37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "async def _atake_next_step(\n",
    "    self,\n",
    "    name_to_tool_map: dict[str, Tool],\n",
    "    color_mapping: dict[str, str],\n",
    "    inputs: dict[str, str],\n",
    "    intermediate_steps: list[tuple[AgentAction, str]],\n",
    "    run_manager: ty.Optional[AsyncCallbackManagerForChainRun] = None,\n",
    ") -> list[tuple[AgentAction, str]] | AgentFinish:\n",
    "    \"\"\"Take a single step in the thought-action-observation loop.\"\"\"\n",
    "\n",
    "    logger.debug(\"Taking next step\")\n",
    "\n",
    "    try:\n",
    "        # Call the LLM to see what to do.\n",
    "        output = await self.agent.aplan(\n",
    "            intermediate_steps,\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "            **inputs,\n",
    "        )\n",
    "    except OutputParserException as e:\n",
    "        if isinstance(self.handle_parsing_errors, bool):\n",
    "            raise_error = not self.handle_parsing_errors\n",
    "        else:\n",
    "            raise_error = False\n",
    "        if raise_error:\n",
    "            raise e\n",
    "        text = str(e)\n",
    "        if isinstance(self.handle_parsing_errors, bool):\n",
    "            if e.send_to_llm:\n",
    "                observation = str(e.observation)\n",
    "                text = str(e.llm_output)\n",
    "            else:\n",
    "                observation = \"Invalid or incomplete response\"\n",
    "        elif isinstance(self.handle_parsing_errors, str):\n",
    "            observation = self.handle_parsing_errors\n",
    "        elif callable(self.handle_parsing_errors):\n",
    "            observation = self.handle_parsing_errors(e)\n",
    "        else:\n",
    "            raise ValueError(\"Got unexpected type of `handle_parsing_errors`\")\n",
    "        output = AgentAction(\"_Exception\", observation, text)\n",
    "        if run_manager:\n",
    "            run_manager.on_agent_action(output, color=\"green\")\n",
    "        tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "        observation = ExceptionTool().run(\n",
    "            output.tool_input,\n",
    "            verbose=self.verbose,\n",
    "            color=None,\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "            **tool_run_kwargs,\n",
    "        )\n",
    "        return [(output, observation)]\n",
    "\n",
    "    logger.debug(f\"Agent output: {output}\")\n",
    "\n",
    "    # If the tool chosen is the finishing tool, then we end and return.\n",
    "    if isinstance(output, AgentFinish):\n",
    "        return output\n",
    "    actions: List[AgentAction]\n",
    "    if isinstance(output, AgentAction):\n",
    "        actions = [output]\n",
    "    else:\n",
    "        actions = output\n",
    "\n",
    "    async def _aperform_agent_action(\n",
    "        agent_action: AgentAction,\n",
    "    ) -> tuple[AgentAction, str]:\n",
    "        if run_manager:\n",
    "            await run_manager.on_agent_action(\n",
    "                agent_action, verbose=self.verbose, color=\"green\"\n",
    "            )\n",
    "        # Otherwise we lookup the tool\n",
    "        if agent_action.tool in name_to_tool_map:\n",
    "            tool = name_to_tool_map[agent_action.tool]\n",
    "            return_direct = tool.return_direct\n",
    "            color = color_mapping[agent_action.tool]\n",
    "            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "            if return_direct:\n",
    "                tool_run_kwargs[\"llm_prefix\"] = \"\"\n",
    "            # We then call the tool on the tool input to get an observation\n",
    "            observation = await tool.arun(\n",
    "                agent_action.tool_input,\n",
    "                verbose=self.verbose,\n",
    "                color=color,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **tool_run_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "            observation = await InvalidTool().arun(\n",
    "                agent_action.tool,\n",
    "                verbose=self.verbose,\n",
    "                color=None,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **tool_run_kwargs,\n",
    "            )\n",
    "        return agent_action, observation\n",
    "\n",
    "    # Use asyncio.gather to run multiple tool.arun() calls concurrently\n",
    "    result = await asyncio.gather(\n",
    "        *[_aperform_agent_action(agent_action) for agent_action in actions]\n",
    "    )\n",
    "\n",
    "    return list(result)\n",
    "\n",
    "\n",
    "AgentExecutor._atake_next_step = _atake_next_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd371fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine._agent.return_intermediate_steps = True\n",
    "chat_engine._agent.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3bbbe9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine._service_context.llm_predictor.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe12c66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Starting generator\n",
      "DEBUG:__main__:Iteration 0\n",
      "DEBUG:__main__:Taking next step\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\\\\n\\\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\\\n\\\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\\\n\\\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\"}, {\"role\": \"user\", \"content\": \"TOOLS\\\\n------\\\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\\\n\\\\n> Codeine Source Code Search: Useful for when you should answer questions about the codeine source code.\\\\n\\\\nRESPONSE FORMAT INSTRUCTIONS\\\\n----------------------------\\\\n\\\\nWhen responding to me, please output a response in one of two formats:\\\\n\\\\n**Option 1:**\\\\nUse this if you want the human to use a tool.\\\\nMarkdown code snippet formatted in the following schema:\\\\n\\\\n```json\\\\n{\\\\n    \\\\\"action\\\\\": string, \\\\\\\\ The action to take. Must be one of Codeine Source Code Search\\\\n    \\\\\"action_input\\\\\": string \\\\\\\\ The input to the action\\\\n}\\\\n```\\\\n\\\\n**Option #2:**\\\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\\\n\\\\n```json\\\\n{\\\\n    \\\\\"action\\\\\": \\\\\"Final Answer\\\\\",\\\\n    \\\\\"action_input\\\\\": string \\\\\\\\ You should put what you want to return to use here\\\\n}\\\\n```\\\\n\\\\nUSER\\'S INPUT\\\\n--------------------\\\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\\\n\\\\nHow does TinyViT work?\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0, \"stop\": [\"\\\\nObservation:\", \"\\\\n\\\\tObservation:\"]}' message='Post details'\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=None request_id=ac21c2b80aa8a6241975372c3f5bab46 response_code=401\n",
      "INFO:openai:error_code=invalid_api_key error_message= error_param=None error_type=invalid_request_error message='OpenAI API error received' stream_error=False\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'OutputParserException' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 17\u001b[0m, in \u001b[0;36m_atake_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39maplan(\n\u001b[1;32m     18\u001b[0m         intermediate_steps,\n\u001b[1;32m     19\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Handle parsing errors (omitted for brevity)\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/agents/agent.py:464\u001b[0m, in \u001b[0;36mAgent.aplan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m full_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 464\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mapredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(full_output)\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:235\u001b[0m, in \u001b[0;36mLLMChain.apredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m        completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macall(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks))[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/chains/base.py:198\u001b[0m, in \u001b[0;36mChain.acall\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/chains/base.py:192\u001b[0m, in \u001b[0;36mChain.acall\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 192\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs)\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:200\u001b[0m, in \u001b[0;36mLLMChain._acall\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acall\u001b[39m(\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    197\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    198\u001b[0m     run_manager: Optional[AsyncCallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 200\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:95\u001b[0m, in \u001b[0;36mLLMChain.agenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     94\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m     96\u001b[0m     prompts, stop, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m )\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:178\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    179\u001b[0m     prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:151\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    152\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:141\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    143\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(m, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(m, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    146\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[1;32m    147\u001b[0m         ]\n\u001b[1;32m    148\u001b[0m     )\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:403\u001b[0m, in \u001b[0;36mChatOpenAI._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m acompletion_with_retry(\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m, messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    405\u001b[0m     )\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:93\u001b[0m, in \u001b[0;36macompletion_with_retry\u001b[0;34m(llm, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39macreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/tenacity/_asyncio.py:88\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masync_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/tenacity/_asyncio.py:47\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/tenacity/_asyncio.py:50\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:91\u001b[0m, in \u001b[0;36macompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39macreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:45\u001b[0m, in \u001b[0;36mChatCompletion.acreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39macreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:217\u001b[0m, in \u001b[0;36mEngineAPIResource.acreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    203\u001b[0m (\n\u001b[1;32m    204\u001b[0m     deployment_id,\n\u001b[1;32m    205\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    216\u001b[0m )\n\u001b[0;32m--> 217\u001b[0m response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m requestor\u001b[38;5;241m.\u001b[39marequest(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m     url,\n\u001b[1;32m    220\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    221\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    222\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    223\u001b[0m     request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    224\u001b[0m     request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/openai/api_requestor.py:382\u001b[0m, in \u001b[0;36mAPIRequestor.arequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    372\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marequest_raw(\n\u001b[1;32m    373\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    374\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    381\u001b[0m     )\n\u001b[0;32m--> 382\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_async_response(result, stream)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/openai/api_requestor.py:726\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_async_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    724\u001b[0m     util\u001b[38;5;241m.\u001b[39mlog_warn(e, body\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 726\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    733\u001b[0m )\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: <empty message>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow does TinyViT work?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m : []\n\u001b[1;32m      4\u001b[0m }\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ix, step \u001b[38;5;129;01min\u001b[39;00m aioitertools\u001b[38;5;241m.\u001b[39menumerate(chat_engine\u001b[38;5;241m.\u001b[39m_agent\u001b[38;5;241m.\u001b[39mintermediate_steps_generator(\n\u001b[1;32m      7\u001b[0m     inputs,\n\u001b[1;32m      8\u001b[0m )):\n\u001b[1;32m      9\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==========================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/codeine/.venv/lib/python3.10/site-packages/aioitertools/builtins.py:192\u001b[0m, in \u001b[0;36menumerate\u001b[0;34m(itr, start)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03mConsume a mixed iterable and yield the current index and item.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m index \u001b[38;5;241m=\u001b[39m start\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(itr):\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m index, item\n\u001b[1;32m    194\u001b[0m     index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[29], line 24\u001b[0m, in \u001b[0;36mintermediate_steps_generator\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m     22\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atake_next_step(\n\u001b[1;32m     25\u001b[0m         name_to_tool_map,\n\u001b[1;32m     26\u001b[0m         color_mapping,\n\u001b[1;32m     27\u001b[0m         inputs,\n\u001b[1;32m     28\u001b[0m         intermediate_steps,\n\u001b[1;32m     29\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# Yield the final answer\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         final_answer \u001b[38;5;241m=\u001b[39m next_step_output\u001b[38;5;241m.\u001b[39mreturn_values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[31], line 22\u001b[0m, in \u001b[0;36m_atake_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39maplan(\n\u001b[1;32m     18\u001b[0m         intermediate_steps,\n\u001b[1;32m     19\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mOutputParserException\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Handle parsing errors (omitted for brevity)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     26\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OutputParserException' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"input\": \"How does TinyViT work?\",\n",
    "    \"chat_history\" : []\n",
    "}\n",
    "\n",
    "async for ix, step in aioitertools.enumerate(chat_engine._agent.intermediate_steps_generator(\n",
    "    inputs,\n",
    ")):\n",
    "    logging.info(\"==========================================\")\n",
    "    logging.info(f\"Step: {ix}\")\n",
    "    if isinstance(step, str): # agent is finished\n",
    "        text = step\n",
    "    else: # agent has a tool to use\n",
    "        action, text = step\n",
    "        logging.info(f\"Action: {action}\")\n",
    "    logging.info(f\"Text: {text}\")\n",
    "    logging.info(\"==========================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cda87eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Training TinyViT, a smaller version of the Vision Transformer (ViT) model, involves a similar process to training the original ViT. The main steps include: 1. Preparing a dataset of images and their corresponding labels. 2. Initializing the TinyViT model with a smaller architecture compared to the original ViT. 3. Feeding the images through the model and computing the loss based on the model's predictions and the true labels. 4. Updating the model's weights using an optimization algorithm, such as Adam or SGD, to minimize the loss. 5. Repeating steps 3 and 4 for multiple epochs until the model converges or reaches a satisfactory performance level. The primary difference between TinyViT and the original ViT is the model's size, which makes TinyViT more efficient and faster to train, while still maintaining competitive performance on various computer vision tasks.\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5fd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
