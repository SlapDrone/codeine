{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23c7686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import typing as ty\n",
    "\n",
    "import aioitertools\n",
    "from langchain.input import get_color_mapping\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.agent import AgentAction, AgentFinish\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForChainRun\n",
    "from langchain.utilities.asyncio import asyncio_timeout\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from codeine.chatbot import build_chat_engine, service_context\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "chat_engine = build_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fa7de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def intermediate_steps_generator(\n",
    "    self,\n",
    "    inputs: dict[str, str],\n",
    "    run_manager: ty.Optional[AsyncCallbackManagerForChainRun] = None,\n",
    ") -> ty.AsyncIterator[tuple[AgentAction, str]]:\n",
    "    \"\"\"Generator function that yields intermediate steps (thoughts, actions, and observations).\"\"\"\n",
    "\n",
    "    logger.debug(\"Starting generator\")\n",
    "\n",
    "    name_to_tool_map = {tool.name: tool for tool in self.tools}\n",
    "    color_mapping = get_color_mapping(\n",
    "        [tool.name for tool in self.tools], excluded_colors=[\"green\"]\n",
    "    )\n",
    "    intermediate_steps: List[Tuple[AgentAction, str]] = []\n",
    "    iterations = 0\n",
    "    time_elapsed = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    async with asyncio_timeout(self.max_execution_time):\n",
    "        try:\n",
    "            while self._should_continue(iterations, time_elapsed):\n",
    "                logger.debug(f\"Iteration {iterations}\")\n",
    "\n",
    "                next_step_output = await self._atake_next_step(\n",
    "                    name_to_tool_map,\n",
    "                    color_mapping,\n",
    "                    inputs,\n",
    "                    intermediate_steps,\n",
    "                    run_manager=run_manager,\n",
    "                )\n",
    "                if isinstance(next_step_output, AgentFinish):\n",
    "                    # Yield the final answer\n",
    "                    final_answer = next_step_output.return_values[\"output\"]\n",
    "                    yield next_step_output, final_answer\n",
    "                    logger.debug(\"Agent finished\")\n",
    "                    break\n",
    "\n",
    "                intermediate_steps.extend(next_step_output)\n",
    "                if len(next_step_output) == 1:\n",
    "                    next_step_action = next_step_output[0]\n",
    "                    tool_return = self._get_tool_return(next_step_action)\n",
    "                    if tool_return is not None:\n",
    "                        logger.debug(\"Tool returned\")\n",
    "                        break\n",
    "\n",
    "                # Yield the latest intermediate step(s)\n",
    "                for step_output in next_step_output:\n",
    "                    logger.debug(f\"Yielding step: {step_output}\")\n",
    "                    yield step_output\n",
    "\n",
    "                iterations += 1\n",
    "                time_elapsed = time.time() - start_time\n",
    "        except TimeoutError:\n",
    "            logger.debug(\"TimeoutError\")\n",
    "            pass\n",
    "    logger.debug(\"Generator finished\")\n",
    "\n",
    "AgentExecutor.intermediate_steps_generator = intermediate_steps_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92782326",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _acall(\n",
    "    self,\n",
    "    inputs: dict[str, str],\n",
    "    run_manager: ty.Optional[AsyncCallbackManagerForChainRun] = None,\n",
    ") -> dict[str, str]:\n",
    "    \"\"\"Run text through and get agent response.\"\"\"\n",
    "    intermediate_steps: list[tuple[AgentAction, str]] = []\n",
    "\n",
    "    async for step in self.intermediate_steps_generator(inputs, run_manager):\n",
    "        intermediate_steps.append(step)\n",
    "\n",
    "    output = self.agent.return_stopped(intermediate_steps)\n",
    "    return output\n",
    "\n",
    "AgentExecutor._acall = _acall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5ae82cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "async def _atake_next_step(\n",
    "    self,\n",
    "    name_to_tool_map: dict[str, Tool],\n",
    "    color_mapping: dict[str, str],\n",
    "    inputs: dict[str, str],\n",
    "    intermediate_steps: list[tuple[AgentAction, str]],\n",
    "    run_manager: ty.Optional[AsyncCallbackManagerForChainRun] = None,\n",
    ") -> list[tuple[AgentAction, str]] | AgentFinish:\n",
    "    \"\"\"Take a single step in the thought-action-observation loop.\"\"\"\n",
    "\n",
    "    logger.debug(\"Taking next step\")\n",
    "\n",
    "    try:\n",
    "        # Call the LLM to see what to do.\n",
    "        output = await self.agent.aplan(\n",
    "            intermediate_steps,\n",
    "            callbacks=run_manager.get_child() if run_manager else None,\n",
    "            **inputs,\n",
    "        )\n",
    "    except OutputParserException as e:\n",
    "        # Handle parsing errors (omitted for brevity)\n",
    "        pass\n",
    "\n",
    "    logger.debug(f\"Agent output: {output}\")\n",
    "\n",
    "    # If the tool chosen is the finishing tool, then we end and return.\n",
    "    if isinstance(output, AgentFinish):\n",
    "        return output\n",
    "    actions: List[AgentAction]\n",
    "    if isinstance(output, AgentAction):\n",
    "        actions = [output]\n",
    "    else:\n",
    "        actions = output\n",
    "\n",
    "    async def _aperform_agent_action(\n",
    "        agent_action: AgentAction,\n",
    "    ) -> tuple[AgentAction, str]:\n",
    "        if run_manager:\n",
    "            await run_manager.on_agent_action(\n",
    "                agent_action, verbose=self.verbose, color=\"green\"\n",
    "            )\n",
    "        # Otherwise we lookup the tool\n",
    "        if agent_action.tool in name_to_tool_map:\n",
    "            tool = name_to_tool_map[agent_action.tool]\n",
    "            return_direct = tool.return_direct\n",
    "            color = color_mapping[agent_action.tool]\n",
    "            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "            if return_direct:\n",
    "                tool_run_kwargs[\"llm_prefix\"] = \"\"\n",
    "            # We then call the tool on the tool input to get an observation\n",
    "            observation = await tool.arun(\n",
    "                agent_action.tool_input,\n",
    "                verbose=self.verbose,\n",
    "                color=color,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **tool_run_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "            observation = await InvalidTool().arun(\n",
    "                agent_action.tool,\n",
    "                verbose=self.verbose,\n",
    "                color=None,\n",
    "                callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **tool_run_kwargs,\n",
    "            )\n",
    "        return agent_action, observation\n",
    "\n",
    "    # Use asyncio.gather to run multiple tool.arun() calls concurrently\n",
    "    result = await asyncio.gather(\n",
    "        *[_aperform_agent_action(agent_action) for agent_action in actions]\n",
    "    )\n",
    "\n",
    "    return list(result)\n",
    "\n",
    "\n",
    "AgentExecutor._atake_next_step = _atake_next_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e0cb4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine._agent.return_intermediate_steps = True\n",
    "chat_engine._agent.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "944802f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine._service_context.llm_predic tor.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d749868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Starting generator\n",
      "DEBUG:__main__:Iteration 0\n",
      "DEBUG:__main__:Taking next step\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\\\\n\\\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\\\n\\\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\\\n\\\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\"}, {\"role\": \"user\", \"content\": \"TOOLS\\\\n------\\\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\\\n\\\\n> Codeine Source Code Search: Useful for when you should answer questions about the codeine source code.\\\\n\\\\nRESPONSE FORMAT INSTRUCTIONS\\\\n----------------------------\\\\n\\\\nWhen responding to me, please output a response in one of two formats:\\\\n\\\\n**Option 1:**\\\\nUse this if you want the human to use a tool.\\\\nMarkdown code snippet formatted in the following schema:\\\\n\\\\n```json\\\\n{\\\\n    \\\\\"action\\\\\": string, \\\\\\\\ The action to take. Must be one of Codeine Source Code Search\\\\n    \\\\\"action_input\\\\\": string \\\\\\\\ The input to the action\\\\n}\\\\n```\\\\n\\\\n**Option #2:**\\\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\\\n\\\\n```json\\\\n{\\\\n    \\\\\"action\\\\\": \\\\\"Final Answer\\\\\",\\\\n    \\\\\"action_input\\\\\": string \\\\\\\\ You should put what you want to return to use here\\\\n}\\\\n```\\\\n\\\\nUSER\\'S INPUT\\\\n--------------------\\\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\\\n\\\\nHow does TinyViT work?\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0, \"stop\": [\"\\\\nObservation:\", \"\\\\n\\\\tObservation:\"]}' message='Post details'\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1177 request_id=4555b51978a4e27770345bc64ddd89bb response_code=200\n",
      "DEBUG:__main__:Agent output: AgentAction(tool='Codeine Source Code Search', tool_input='TinyViT', log='{\\n    \"action\": \"Codeine Source Code Search\",\\n    \"action_input\": \"TinyViT\"\\n}')\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"TinyViT\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=677 request_id=a2a43c4172e59aaa0f13851f0f35d390 response_code=200\n",
      "DEBUG:llama_index.indices.utils:> Top 5 nodes:\n",
      "> [Node 780f7796-b295-4b10-8f83-a81e4a9c7f69] [Similarity score:             0.816011] file_path: TinyViT/README.md\n",
      "file_name: README.md\n",
      "\n",
      "# TinyViT: Fast Pretraining Distillation for S...\n",
      "> [Node 653a4775-bb81-4d20-b4ea-51ace4807e9e] [Similarity score:             0.807499] file_path: TinyViT/README.md\n",
      "file_name: README.md\n",
      "\n",
      "![](./.figure/distill.png)      | IN-22k   |22...\n",
      "> [Node b972e24c-2150-418b-8f3e-ec5ff2769b71] [Similarity score:             0.802945] file_path: TinyViT/README.md\n",
      "file_name: README.md\n",
      "\n",
      "![](./.figure/distill.png)       | IN-22k   |2...\n",
      "> [Node a88109df-e161-4cf9-ab7d-bacf92be82d5] [Similarity score:             0.802512] file_path: TinyViT/README.md\n",
      "file_name: README.md\n",
      "\n",
      "                                | IN-1k    |22...\n",
      "> [Node 4429ec00-63a0-4ad1-a50a-9531295074e5] [Similarity score:             0.801144] file_path: TinyViT/models/tiny_vit.py\n",
      "file_name: tiny_vit.py\n",
      "\n",
      "                 )\n",
      "            for ...\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 4 tokens\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"Context information is below. \\\\n---------------------\\\\nfile_path: TinyViT/README.md\\\\nfile_name: README.md\\\\n\\\\n# TinyViT: Fast Pretraining Distillation for Small Vision Transformers [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Tiny%20vision%20transformer%20models,%20SOTA%20performance!!&url=https://github.com/microsoft/Cream/tree/main/TinyViT&via=houwen_peng&hashtags=ViT,tiny,efficient)\\\\n\\\\n\\\\n:pushpin: This is an official PyTorch implementation of **[ECCV 2022]** - [TinyViT: Fast Pretraining Distillation for Small Vision Transformers](https://arxiv.org/pdf/2207.10666.pdf).\\\\n\\\\nTinyViT is a new family of **tiny and efficient** vision transformers pretrained on **large-scale** datasets with our proposed **fast distillation framework**. The central idea is to **transfer knowledge** from **large pretrained models** to small ones. The logits of large teacher models are sparsified and stored in disk in advance to **save the memory cost and computation overheads**.\\\\n\\\\n:rocket: TinyViT with **only 21M parameters** achieves **84.8%** top-1 accuracy on ImageNet-1k, and **86.5%** accuracy under 512x512 resolutions.\\\\n\\\\n<div align=\\\\\"center\\\\\">\\\\n    <img width=\\\\\"80%\\\\\" alt=\\\\\"TinyViT overview\\\\\" src=\\\\\".figure/framework.png\\\\\"/>\\\\n</div>\\\\n\\\\n:sunny: Hiring research interns for neural architecture search, tiny transformer design, model compression projects: houwen.peng@microsoft.com.\\\\n\\\\n## Highlights\\\\n\\\\n<div align=\\\\\"center\\\\\">\\\\n    <img width=\\\\\"80%\\\\\" src=\\\\\".figure/performance.png\\\\\"/>\\\\n</div>\\\\n\\\\n* TinyViT-21M ![](./.figure/distill.png) on IN-22k achieves **84.8%** top-1 accuracy on IN-1k, and **86.5%** accuracy under 512x512 resolutions.\\\\n* TinyViT-21M **trained from scratch on IN-1k** without distillation achieves **83.1** top-1 accuracy, under **4.3 GFLOPs** and **1,571 images/s** throughput on V100 GPU.\\\\n* TinyViT-5M ![](./.figure/distill.png) reaches **80.7%** top-1 accuracy on IN-1k under 3,060 images/s throughput.\\\\n* Save teacher logits **once**, and **reuse** the saved sparse logits to distill **arbitrary students without overhead** of teacher model. It takes **16 GB / 481 GB** storage space for IN-1k (300 epochs) and IN-22k (90 epochs), respectively.\\\\n\\\\n## Features\\\\n1. **Efficient Distillation**. The teacher logits can be saved in parallel and reused for arbitrary student models, to avoid re-forwarding cost of\\\\n\\\\nfile_path: TinyViT/README.md\\\\nfile_name: README.md\\\\n\\\\n![](./.figure/distill.png)      | IN-22k   |224x224| 84.8  | 97.3  | 21M     | 4.3G | 1,571|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22k_distill.pth)/[config](./configs/22k_distill/tiny_vit_21m_22k_distill.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22k_distill.log)|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_distill.pth)/[config](./configs/22kto1k/tiny_vit_21m_22kto1k.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_distill.log)\\\\nTinyViT-21M-384 ![](./.figure/distill.png)  | IN-22k   |384x384| 86.2  | 97.8  | 21M     | 13.8G| 394  | - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_384_distill.pth)/[config](./configs/higher_resolution/tiny_vit_21m_224to384.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_384_distill.log)\\\\nTinyViT-21M-512 ![](./.figure/distill.png)  | IN-22k   |512x512| 86.5  | 97.9  | 21M     | 27.0G| 167  | - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_512_distill.pth)/[config](./configs/higher_resolution/tiny_vit_21m_384to512.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_512_distill.log)\\\\nTinyViT-5M                                 | IN-1k    |224x224| 79.1\\\\n\\\\nfile_path: TinyViT/README.md\\\\nfile_name: README.md\\\\n\\\\n![](./.figure/distill.png)       | IN-22k   |224x224| 80.7  | 95.6  | 5.4M    | 1.3G | 3,060|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22k_distill.pth)/[config](./configs/22k_distill/tiny_vit_5m_22k_distill.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22k_distill.log)|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22kto1k_distill.pth)/[config](./configs/22kto1k/tiny_vit_5m_22kto1k.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22kto1k_distill.log)\\\\nTinyViT-11M ![](./.figure/distill.png)      | IN-22k   |224x224| 83.2  | 96.5  | 11M     | 2.0G | 2,468|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22k_distill.pth)/[config](./configs/22k_distill/tiny_vit_11m_22k_distill.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22k_distill.log)|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22kto1k_distill.pth)/[config](./configs/22kto1k/tiny_vit_11m_22kto1k.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22kto1k_distill.log)\\\\nTinyViT-21M ![](./.figure/distill.png)      | IN-22k   |224x224| 84.8  | 97.3  | 21M     | 4.3G |\\\\n\\\\nfile_path: TinyViT/README.md\\\\nfile_name: README.md\\\\n\\\\n                                | IN-1k    |224x224| 79.1  | 94.8  | 5.4M    | 1.3G | 3,060| - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_1k.pth)/[config](./configs/1k/tiny_vit_5m.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_1k.log)\\\\nTinyViT-11M                                | IN-1k    |224x224| 81.5  | 95.8  | 11M     | 2.0G | 2,468| - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_1k.pth)/[config](./configs/1k/tiny_vit_11m.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_1k.log)\\\\nTinyViT-21M                                | IN-1k    |224x224| 83.1  | 96.5  | 21M     | 4.3G | 1,571| - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_1k.pth)/[config](./configs/1k/tiny_vit_21m.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_1k.log)\\\\n\\\\nImageNet-22k (IN-22k) is the same as ImageNet-21k (IN-21k), where the number of classes is 21,841.\\\\n\\\\nThe models with ![](./.figure/distill.png) are pretrained on ImageNet-22k with the distillation of CLIP-ViT-L/14-22k, then finetuned on ImageNet-1k.\\\\n\\\\nWe finetune the 1k models on IN-1k to higher resolution progressively (224 -> 384 -> 512) [[detail]](./docs/TRAINING.md), without any IN-1k knowledge distillation.\\\\n\\\\n## Getting Started\\\\n:beginner: Here is the setup tutorial and evaluation scripts.\\\\n\\\\n### Install dependencies and prepare datasets\\\\n-\\\\n\\\\nfile_path: TinyViT/models/tiny_vit.py\\\\nfile_name: tiny_vit.py\\\\n\\\\n                 )\\\\n            for i in range(depth)])\\\\n\\\\n        # patch merging layer\\\\n        if downsample is not None:\\\\n            self.downsample = downsample(\\\\n                input_resolution, dim=dim, out_dim=out_dim, activation=activation)\\\\n        else:\\\\n            self.downsample = None\\\\n\\\\n    def forward(self, x):\\\\n        for blk in self.blocks:\\\\n            if self.use_checkpoint:\\\\n                x = checkpoint.checkpoint(blk, x)\\\\n            else:\\\\n                x = blk(x)\\\\n        if self.downsample is not None:\\\\n            x = self.downsample(x)\\\\n        return x\\\\n\\\\n    def extra_repr(self) -> str:\\\\n        return f\\\\\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\\\\\"\\\\n\\\\n\\\\nclass TinyViT(nn.Module):\\\\n    def __init__(self, img_size=224, in_chans=3, num_classes=1000,\\\\n                 embed_dims=[96, 192, 384, 768], depths=[2, 2, 6, 2],\\\\n                 num_heads=[3, 6, 12, 24],\\\\n                 window_sizes=[7, 7, 14, 7],\\\\n                 mlp_ratio=4.,\\\\n                 drop_rate=0.,\\\\n                 drop_path_rate=0.1,\\\\n                 use_checkpoint=False,\\\\n                 mbconv_expand_ratio=4.0,\\\\n                 local_conv_size=3,\\\\n                 layer_lr_decay=1.0,\\\\n                 ):\\\\n        super().__init__()\\\\n---------------------\\\\nGiven the context information and not prior knowledge, answer the question: TinyViT\\\\n\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=2800 request_id=a4d124a50406df217bcea15c21e79357 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:What is TinyViT?\n",
      "\n",
      "TinyViT is a family of tiny and efficient vision transformers pretrained on large-scale datasets with a proposed fast distillation framework, which transfers knowledge from large pretrained models to small ones. It achieves high accuracy with only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects.\n",
      "DEBUG:llama_index.indices.response.base_builder:> Initial prompt template: Context information is below. \n",
      "---------------------\n",
      "file_path: TinyViT/README.md\n",
      "file_name: README.md\n",
      "\n",
      "# TinyViT: Fast Pretraining Distillation for Small Vision Transformers [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Tiny%20vision%20transformer%20models,%20SOTA%20performance!!&url=https://github.com/microsoft/Cream/tree/main/TinyViT&via=houwen_peng&hashtags=ViT,tiny,efficient)\n",
      "\n",
      "\n",
      ":pushpin: This is an official PyTorch implementation of **[ECCV 2022]** - [TinyViT: Fast Pretraining Distillation for Small Vision Transformers](https://arxiv.org/pdf/2207.10666.pdf).\n",
      "\n",
      "TinyViT is a new family of **tiny and efficient** vision transformers pretrained on **large-scale** datasets with our proposed **fast distillation framework**. The central idea is to **transfer knowledge** from **large pretrained models** to small ones. The logits of large teacher models are sparsified and stored in disk in advance to **save the memory cost and computation overheads**.\n",
      "\n",
      ":rocket: TinyViT with **only 21M parameters** achieves **84.8%** top-1 accuracy on ImageNet-1k, and **86.5%** accuracy under 512x512 resolutions.\n",
      "\n",
      "<div align=\"center\">\n",
      "    <img width=\"80%\" alt=\"TinyViT overview\" src=\".figure/framework.png\"/>\n",
      "</div>\n",
      "\n",
      ":sunny: Hiring research interns for neural architecture search, tiny transformer design, model compression projects: houwen.peng@microsoft.com.\n",
      "\n",
      "## Highlights\n",
      "\n",
      "<div align=\"center\">\n",
      "    <img width=\"80%\" src=\".figure/performance.png\"/>\n",
      "</div>\n",
      "\n",
      "* TinyViT-21M ![](./.figure/distill.png) on IN-22k achieves **84.8%** top-1 accuracy on IN-1k, and **86.5%** accuracy under 512x512 resolutions.\n",
      "* TinyViT-21M **trained from scratch on IN-1k** without distillation achieves **83.1** top-1 accuracy, under **4.3 GFLOPs** and **1,571 images/s** throughput on V100 GPU.\n",
      "* TinyViT-5M ![](./.figure/distill.png) reaches **80.7%** top-1 accuracy on IN-1k under 3,060 images/s throughput.\n",
      "* Save teacher logits **once**, and **reuse** the saved sparse logits to distill **arbitrary students without overhead** of teacher model. It takes **16 GB / 481 GB** storage space for IN-1k (300 epochs) and IN-22k (90 epochs), respectively.\n",
      "\n",
      "## Features\n",
      "1. **Efficient Distillation**. The teacher logits can be saved in parallel and reused for arbitrary student models, to avoid re-forwarding cost of\n",
      "\n",
      "file_path: TinyViT/README.md\n",
      "file_name: README.md\n",
      "\n",
      "![](./.figure/distill.png)      | IN-22k   |224x224| 84.8  | 97.3  | 21M     | 4.3G | 1,571|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22k_distill.pth)/[config](./configs/22k_distill/tiny_vit_21m_22k_distill.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22k_distill.log)|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_distill.pth)/[config](./configs/22kto1k/tiny_vit_21m_22kto1k.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_distill.log)\n",
      "TinyViT-21M-384 ![](./.figure/distill.png)  | IN-22k   |384x384| 86.2  | 97.8  | 21M     | 13.8G| 394  | - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_384_distill.pth)/[config](./configs/higher_resolution/tiny_vit_21m_224to384.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_384_distill.log)\n",
      "TinyViT-21M-512 ![](./.figure/distill.png)  | IN-22k   |512x512| 86.5  | 97.9  | 21M     | 27.0G| 167  | - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_512_distill.pth)/[config](./configs/higher_resolution/tiny_vit_21m_384to512.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_512_distill.log)\n",
      "TinyViT-5M                                 | IN-1k    |224x224| 79.1\n",
      "\n",
      "file_path: TinyViT/README.md\n",
      "file_name: README.md\n",
      "\n",
      "![](./.figure/distill.png)       | IN-22k   |224x224| 80.7  | 95.6  | 5.4M    | 1.3G | 3,060|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22k_distill.pth)/[config](./configs/22k_distill/tiny_vit_5m_22k_distill.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22k_distill.log)|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22kto1k_distill.pth)/[config](./configs/22kto1k/tiny_vit_5m_22kto1k.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22kto1k_distill.log)\n",
      "TinyViT-11M ![](./.figure/distill.png)      | IN-22k   |224x224| 83.2  | 96.5  | 11M     | 2.0G | 2,468|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22k_distill.pth)/[config](./configs/22k_distill/tiny_vit_11m_22k_distill.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22k_distill.log)|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22kto1k_distill.pth)/[config](./configs/22kto1k/tiny_vit_11m_22kto1k.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22kto1k_distill.log)\n",
      "TinyViT-21M ![](./.figure/distill.png)      | IN-22k   |224x224| 84.8  | 97.3  | 21M     | 4.3G |\n",
      "\n",
      "file_path: TinyViT/README.md\n",
      "file_name: README.md\n",
      "\n",
      "                                | IN-1k    |224x224| 79.1  | 94.8  | 5.4M    | 1.3G | 3,060| - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_1k.pth)/[config](./configs/1k/tiny_vit_5m.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_1k.log)\n",
      "TinyViT-11M                                | IN-1k    |224x224| 81.5  | 95.8  | 11M     | 2.0G | 2,468| - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_1k.pth)/[config](./configs/1k/tiny_vit_11m.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_1k.log)\n",
      "TinyViT-21M                                | IN-1k    |224x224| 83.1  | 96.5  | 21M     | 4.3G | 1,571| - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_1k.pth)/[config](./configs/1k/tiny_vit_21m.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_1k.log)\n",
      "\n",
      "ImageNet-22k (IN-22k) is the same as ImageNet-21k (IN-21k), where the number of classes is 21,841.\n",
      "\n",
      "The models with ![](./.figure/distill.png) are pretrained on ImageNet-22k with the distillation of CLIP-ViT-L/14-22k, then finetuned on ImageNet-1k.\n",
      "\n",
      "We finetune the 1k models on IN-1k to higher resolution progressively (224 -> 384 -> 512) [[detail]](./docs/TRAINING.md), without any IN-1k knowledge distillation.\n",
      "\n",
      "## Getting Started\n",
      ":beginner: Here is the setup tutorial and evaluation scripts.\n",
      "\n",
      "### Install dependencies and prepare datasets\n",
      "-\n",
      "\n",
      "file_path: TinyViT/models/tiny_vit.py\n",
      "file_name: tiny_vit.py\n",
      "\n",
      "                 )\n",
      "            for i in range(depth)])\n",
      "\n",
      "        # patch merging layer\n",
      "        if downsample is not None:\n",
      "            self.downsample = downsample(\n",
      "                input_resolution, dim=dim, out_dim=out_dim, activation=activation)\n",
      "        else:\n",
      "            self.downsample = None\n",
      "\n",
      "    def forward(self, x):\n",
      "        for blk in self.blocks:\n",
      "            if self.use_checkpoint:\n",
      "                x = checkpoint.checkpoint(blk, x)\n",
      "            else:\n",
      "                x = blk(x)\n",
      "        if self.downsample is not None:\n",
      "            x = self.downsample(x)\n",
      "        return x\n",
      "\n",
      "    def extra_repr(self) -> str:\n",
      "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
      "\n",
      "\n",
      "class TinyViT(nn.Module):\n",
      "    def __init__(self, img_size=224, in_chans=3, num_classes=1000,\n",
      "                 embed_dims=[96, 192, 384, 768], depths=[2, 2, 6, 2],\n",
      "                 num_heads=[3, 6, 12, 24],\n",
      "                 window_sizes=[7, 7, 14, 7],\n",
      "                 mlp_ratio=4.,\n",
      "                 drop_rate=0.,\n",
      "                 drop_path_rate=0.1,\n",
      "                 use_checkpoint=False,\n",
      "                 mbconv_expand_ratio=4.0,\n",
      "                 local_conv_size=3,\n",
      "                 layer_lr_decay=1.0,\n",
      "                 ):\n",
      "        super().__init__()\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the question: TinyViT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.indices.response.base_builder:> Initial response: What is TinyViT?\n",
      "\n",
      "TinyViT is a family of tiny and efficient vision transformers pretrained on large-scale datasets with a proposed fast distillation framework, which transfers knowledge from large pretrained models to small ones. It achieves high accuracy with only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects.\n",
      "DEBUG:llama_index.indices.response.refine:> Refine context: depth={self.depth}\"\n",
      "\n",
      "\n",
      "class TinyViT(nn.Module):...\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"TinyViT\"}, {\"role\": \"assistant\", \"content\": \"What is TinyViT?\\\\n\\\\nTinyViT is a family of tiny and efficient vision transformers pretrained on large-scale datasets with a proposed fast distillation framework, which transfers knowledge from large pretrained models to small ones. It achieves high accuracy with only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects.\"}, {\"role\": \"user\", \"content\": \"We have the opportunity to refine the above answer (only if needed) with some more context below.\\\\n------------\\\\ndepth={self.depth}\\\\\"\\\\n\\\\n\\\\nclass TinyViT(nn.Module):\\\\n    def __init__(self, img_size=224, in_chans=3, num_classes=1000,\\\\n                 embed_dims=[96, 192, 384, 768], depths=[2, 2, 6, 2],\\\\n                 num_heads=[3, 6, 12, 24],\\\\n                 window_sizes=[7, 7, 14, 7],\\\\n                 mlp_ratio=4.,\\\\n                 drop_rate=0.,\\\\n                 drop_path_rate=0.1,\\\\n                 use_checkpoint=False,\\\\n                 mbconv_expand_ratio=4.0,\\\\n                 local_conv_size=3,\\\\n                 layer_lr_decay=1.0,\\\\n                 ):\\\\n        super().__init__()\\\\n\\\\n        self.num_classes = num_classes\\\\n------------\\\\nGiven the new context, refine the original answer to better answer the question. If the context isn\\'t useful, output the original answer again.\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4999 request_id=1276aae381e9363f35e426197a36a168 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:TinyViT is a neural network architecture designed for computer vision tasks. It is a family of tiny and efficient vision transformers that are pretrained on large-scale datasets. The architecture has only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects. The TinyViT model has several hyperparameters, including the number of embedding dimensions, depths, number of heads, window sizes, and drop rates. These hyperparameters can be adjusted to optimize the model for specific tasks. Additionally, TinyViT supports a fast distillation framework that transfers knowledge from large pretrained models to small ones.\n",
      "DEBUG:llama_index.indices.response.base_builder:> Refined prompt template: Human: TinyViT\n",
      "AI: What is TinyViT?\n",
      "\n",
      "TinyViT is a family of tiny and efficient vision transformers pretrained on large-scale datasets with a proposed fast distillation framework, which transfers knowledge from large pretrained models to small ones. It achieves high accuracy with only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects.\n",
      "Human: We have the opportunity to refine the above answer (only if needed) with some more context below.\n",
      "------------\n",
      "depth={self.depth}\"\n",
      "\n",
      "\n",
      "class TinyViT(nn.Module):\n",
      "    def __init__(self, img_size=224, in_chans=3, num_classes=1000,\n",
      "                 embed_dims=[96, 192, 384, 768], depths=[2, 2, 6, 2],\n",
      "                 num_heads=[3, 6, 12, 24],\n",
      "                 window_sizes=[7, 7, 14, 7],\n",
      "                 mlp_ratio=4.,\n",
      "                 drop_rate=0.,\n",
      "                 drop_path_rate=0.1,\n",
      "                 use_checkpoint=False,\n",
      "                 mbconv_expand_ratio=4.0,\n",
      "                 local_conv_size=3,\n",
      "                 layer_lr_decay=1.0,\n",
      "                 ):\n",
      "        super().__init__()\n",
      "\n",
      "        self.num_classes = num_classes\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the question. If the context isn't useful, output the original answer again.\n",
      "DEBUG:llama_index.indices.response.base_builder:> Refined response: TinyViT is a neural network architecture designed for computer vision tasks. It is a family of tiny and efficient vision transformers that are pretrained on large-scale datasets. The architecture has only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects. The TinyViT model has several hyperparameters, including the number of embedding dimensions, depths, number of heads, window sizes, and drop rates. These hyperparameters can be adjusted to optimize the model for specific tasks. Additionally, TinyViT supports a fast distillation framework that transfers knowledge from large pretrained models to small ones.\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 4438 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n",
      "DEBUG:__main__:Yielding step: (AgentAction(tool='Codeine Source Code Search', tool_input='TinyViT', log='{\\n    \"action\": \"Codeine Source Code Search\",\\n    \"action_input\": \"TinyViT\"\\n}'), 'TinyViT is a neural network architecture designed for computer vision tasks. It is a family of tiny and efficient vision transformers that are pretrained on large-scale datasets. The architecture has only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects. The TinyViT model has several hyperparameters, including the number of embedding dimensions, depths, number of heads, window sizes, and drop rates. These hyperparameters can be adjusted to optimize the model for specific tasks. Additionally, TinyViT supports a fast distillation framework that transfers knowledge from large pretrained models to small ones.')\n",
      "INFO:root:==========================================\n",
      "INFO:root:Step: 0\n",
      "INFO:root:Action: AgentAction(tool='Codeine Source Code Search', tool_input='TinyViT', log='{\\n    \"action\": \"Codeine Source Code Search\",\\n    \"action_input\": \"TinyViT\"\\n}')\n",
      "INFO:root:Text: TinyViT is a neural network architecture designed for computer vision tasks. It is a family of tiny and efficient vision transformers that are pretrained on large-scale datasets. The architecture has only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects. The TinyViT model has several hyperparameters, including the number of embedding dimensions, depths, number of heads, window sizes, and drop rates. These hyperparameters can be adjusted to optimize the model for specific tasks. Additionally, TinyViT supports a fast distillation framework that transfers knowledge from large pretrained models to small ones.\n",
      "INFO:root:==========================================\n",
      "DEBUG:__main__:Iteration 1\n",
      "DEBUG:__main__:Taking next step\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\\\\n\\\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\\\n\\\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\\\n\\\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\"}, {\"role\": \"user\", \"content\": \"TOOLS\\\\n------\\\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\\\n\\\\n> Codeine Source Code Search: Useful for when you should answer questions about the codeine source code.\\\\n\\\\nRESPONSE FORMAT INSTRUCTIONS\\\\n----------------------------\\\\n\\\\nWhen responding to me, please output a response in one of two formats:\\\\n\\\\n**Option 1:**\\\\nUse this if you want the human to use a tool.\\\\nMarkdown code snippet formatted in the following schema:\\\\n\\\\n```json\\\\n{\\\\n    \\\\\"action\\\\\": string, \\\\\\\\ The action to take. Must be one of Codeine Source Code Search\\\\n    \\\\\"action_input\\\\\": string \\\\\\\\ The input to the action\\\\n}\\\\n```\\\\n\\\\n**Option #2:**\\\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\\\n\\\\n```json\\\\n{\\\\n    \\\\\"action\\\\\": \\\\\"Final Answer\\\\\",\\\\n    \\\\\"action_input\\\\\": string \\\\\\\\ You should put what you want to return to use here\\\\n}\\\\n```\\\\n\\\\nUSER\\'S INPUT\\\\n--------------------\\\\nHere is the user\\'s input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\\\n\\\\nHow does TinyViT work?\"}, {\"role\": \"assistant\", \"content\": \"{\\\\n    \\\\\"action\\\\\": \\\\\"Codeine Source Code Search\\\\\",\\\\n    \\\\\"action_input\\\\\": \\\\\"TinyViT\\\\\"\\\\n}\"}, {\"role\": \"user\", \"content\": \"TOOL RESPONSE: \\\\n---------------------\\\\nTinyViT is a neural network architecture designed for computer vision tasks. It is a family of tiny and efficient vision transformers that are pretrained on large-scale datasets. The architecture has only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects. The TinyViT model has several hyperparameters, including the number of embedding dimensions, depths, number of heads, window sizes, and drop rates. These hyperparameters can be adjusted to optimize the model for specific tasks. Additionally, TinyViT supports a fast distillation framework that transfers knowledge from large pretrained models to small ones.\\\\n\\\\nUSER\\'S INPUT\\\\n--------------------\\\\n\\\\nOkay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else.\"}], \"model\": \"gpt-3.5-turbo\", \"max_tokens\": null, \"stream\": false, \"n\": 1, \"temperature\": 0.0, \"stop\": [\"\\\\nObservation:\", \"\\\\n\\\\tObservation:\"]}' message='Post details'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5881 request_id=39856748fb402b641e9de6897e945fed response_code=200\n",
      "DEBUG:__main__:Agent output: AgentFinish(return_values={'output': 'TinyViT is a neural network architecture designed for computer vision tasks. It is a family of tiny and efficient vision transformers that are pretrained on large-scale datasets. The architecture has only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects. The TinyViT model has several hyperparameters, including the number of embedding dimensions, depths, number of heads, window sizes, and drop rates. These hyperparameters can be adjusted to optimize the model for specific tasks. Additionally, TinyViT supports a fast distillation framework that transfers knowledge from large pretrained models to small ones.'}, log='{\\n    \"action\": \"Final Answer\",\\n    \"action_input\": \"TinyViT is a neural network architecture designed for computer vision tasks. It is a family of tiny and efficient vision transformers that are pretrained on large-scale datasets. The architecture has only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects. The TinyViT model has several hyperparameters, including the number of embedding dimensions, depths, number of heads, window sizes, and drop rates. These hyperparameters can be adjusted to optimize the model for specific tasks. Additionally, TinyViT supports a fast distillation framework that transfers knowledge from large pretrained models to small ones.\"\\n}')\n",
      "INFO:root:==========================================\n",
      "INFO:root:Step: 1\n",
      "INFO:root:Text: TinyViT is a neural network architecture designed for computer vision tasks. It is a family of tiny and efficient vision transformers that are pretrained on large-scale datasets. The architecture has only 21M parameters and is designed for neural architecture search, tiny transformer design, and model compression projects. The TinyViT model has several hyperparameters, including the number of embedding dimensions, depths, number of heads, window sizes, and drop rates. These hyperparameters can be adjusted to optimize the model for specific tasks. Additionally, TinyViT supports a fast distillation framework that transfers knowledge from large pretrained models to small ones.\n",
      "INFO:root:==========================================\n",
      "DEBUG:__main__:Agent finished\n",
      "DEBUG:__main__:Generator finished\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"input\": \"How does TinyViT work?\",\n",
    "    \"chat_history\" : []\n",
    "}\n",
    "\n",
    "async for ix, step in aioitertools.enumerate(chat_engine._agent.intermediate_steps_generator(\n",
    "    inputs,\n",
    ")):\n",
    "    logging.info(\"==========================================\")\n",
    "    logging.info(f\"Step: {ix}\")\n",
    "    if isinstance(step, str): # agent is finished\n",
    "        text = step\n",
    "    else: # agent has a tool to use\n",
    "        action, text = step\n",
    "        logging.info(f\"Action: {action}\")\n",
    "    logging.info(f\"Text: {text}\")\n",
    "    logging.info(\"==========================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f8cb72b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Training TinyViT, a smaller version of the Vision Transformer (ViT) model, involves a similar process to training the original ViT. The main steps include: 1. Preparing a dataset of images and their corresponding labels. 2. Initializing the TinyViT model with a smaller architecture compared to the original ViT. 3. Feeding the images through the model and computing the loss based on the model's predictions and the true labels. 4. Updating the model's weights using an optimization algorithm, such as Adam or SGD, to minimize the loss. 5. Repeating steps 3 and 4 for multiple epochs until the model converges or reaches a satisfactory performance level. The primary difference between TinyViT and the original ViT is the model's size, which makes TinyViT more efficient and faster to train, while still maintaining competitive performance on various computer vision tasks.\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff2035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
