{"docstore/metadata": {"43ec6c26abfd881fbb1ce8e697bb7a78ae0963e3": {"doc_hash": "7f500373ecb1a384bf65b451d5d98672a376d65b54628e84c60eae4614bb8331"}, "c852740c01eb74de22419b87d0b7fc94fc57a994": {"doc_hash": "b461a9d2d86d4f3625fc25c627e4ffeded0807da056b88d8a9a5e14e95addfa9"}, "dd06953cd65f7ba27366e9c04dd9ec710c99174c": {"doc_hash": "edd7d8eb303691e0b40704618f054500df6738b8563f036fd68d69b09bc846ae"}, "57766cb8a52b49ee6997575754b70edcfea2c920": {"doc_hash": "7ee972d01204cff6eb8a3c19890eded6764f70b3e714e5dcb366ac3d2d11bfed"}, "7d3cb2b4d7e823aabb1d55781149579eeb94b024": {"doc_hash": "bdadf076048694d832dc022c7e45caf52d4798c3979e9cdefb16d367155f6f6e"}, "056e3a9c4e6e31c8a3672ca02c5bbcea6c126ce3": {"doc_hash": "a3b5904d84b772312ba9c477e7c67b1ebb233d6f77886b7569dac982a830bca5"}, "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c": {"doc_hash": "1bf0faeaea33f58f2d2b969be276a4eeaaf3f9a3413a4ffa578561e40dea8542"}, "8e2cd24b26ca26e258c0a6467618f9276d76d7ea": {"doc_hash": "516ff31716f7ed617ac4c0ceeb1aad5ed793d6338e878af2ef3461ff4a007551"}, "38f5689a707f5602e38cb717ed6115f26a0d7ea2": {"doc_hash": "b1ada3a3bb71eeb59f7d35361be663f42ff1fdff46274c44c3107f0096f26e47"}, "d6d4a01b0316989a3f5142167f1e384b098bc930": {"doc_hash": "ed35710b20551fb2a689386e49f6e5ab1cb1679c6252bd516a7b5a1fb5842235"}, "d3603a233d98d3e390dfa6aaaa13ef40099fe4ab": {"doc_hash": "f4365ac3b3ab194c09ca309c456dad810372af837feb088ed90c5d753dc163f0"}, "9cf00d6c245d6b46fc5d4361e291f58af9b9c0c3": {"doc_hash": "45bf9b6d9cc5ed6982426d06b471358c0f1b94a91b3465e5c2119ae0aadb952f"}, "8822c47d450662f967e1d57ce97d484e26fe5e51": {"doc_hash": "be3c81ff83a190f06c784caa8fa6bf059f63071fac35f0b8b9ad7ee2599800c9"}, "1cefc31d6f034e7e4e60c0a8de11087691b5734f": {"doc_hash": "55d31137cf41a2b8831761fdab6db8aba3d5396a7dc6628783f75bb201c9bae2"}, "876ae5f4623f99c9aa9e6aa33913cbe43bf4ca53": {"doc_hash": "98010219a01aa297889e49d6c6af32f0960bffc7804afe24772b90b9cd318b7e"}, "c9d44c50bf5f555c9d52e9e036b0bb282677c276": {"doc_hash": "79689cfc95264728a863dd87e01d24e60962d4b82438de2c2f8f523d2794cff7"}, "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129": {"doc_hash": "6a0b476e613e38b314755d77bdab3bb547a91b538867e45f5d73ab1593f68b68"}, "eeb44e3714eff75028e15214e0e65bf2afebd86c": {"doc_hash": "ee03388453f7d960e1d857f0124f539fef9f7aae049f488e57a64bcba6d607d2"}, "6b6fe453a48d2d8db874d230632216cbd4b01753": {"doc_hash": "e8fb317ca35bc9b243334ec256951c1ecde24babab7e6f4ae0d31c38ba16f75c"}, "e7ba484e729b7ac976b2cedaa43be1c3b308eeeb": {"doc_hash": "58947e6617a7c84deadda2c63a4c1d5adedc0e277ab1483e74b8c5d5bb796e2b"}, "76ab6d18283644702424d0ff2af5832d6d6dd3b7": {"doc_hash": "5a3b3116b3e8d69857de8619e7c4aec02b876d245010f03a7d128c45cc2886fb"}, "892090adb78ca5157a44170e79d0e3b239129f72": {"doc_hash": "aff3f9007df58f1180bd839cda992038e81386d733860ae596af74f199e5a659"}, "ed349009a4caed92e290e05637eca20e46cc275b": {"doc_hash": "0bfeedcf7825c016320c8e287365df58113fc7d8f64fbda018fa926f3caaa0d7"}, "c6ada962ca96eaa7d770014ba130c6de5b36b6ec": {"doc_hash": "ab240c523ae3a664e7911e84cc942f52c46c56ad7b07b3d87848c2da2eaca5d4"}, "467537f479873bbc09fa2b576cdbde9d2a956e7b": {"doc_hash": "cc4fd94df4835e926c2f3c7102736f1f285b9fd711be4a19ecc3a86cdf36fd8b"}, "ee5893c4c84a555eaf98dad03a0cd30c7dafb25e": {"doc_hash": "a9500ba8dce26b3f5a7eef5132440b7d3047f6b245817b06daae0eb459ce7395"}, "e2c035190289caef297e1a952a8411835f67dd8e": {"doc_hash": "8334df1d4597a4669103607a365b849fca90930d5bf2cefa267c0840344bad0a"}, "939c34867e7915ce3e4cc7da04a5bc1653ec4f2c": {"doc_hash": "cf389685b21488063151e66b8b47b5059b0c405d7d71923a1a574f9ecc5b00e8"}, "44b4a3af7372c6865b1cdddda0a8da0ccc6b93a0": {"doc_hash": "b3e2431feed49e5234fed1bbc8ea5ef9a36b329b40e5fdcb4855e8e363d7c25c"}, "a482298d3978271732a6e4db9737656e71b984f2": {"doc_hash": "acc04252d2b9dbab0285e906b4e065f521ac0d23f0e99ff675c1ae2945cafce7"}, "defea4b7d942fa503736f79e36e2bd9fa09a526a": {"doc_hash": "746057ed72f94ab63e4d488b954c2f6d045083de52059806689f47ee1d87fc58"}, "7ccb1a8daeca307f49d81084ee91d87fd57ae80c": {"doc_hash": "d05bd01f41309e06a5e932d02dcc2bc89016da6cc20c1d667a6e1700d4fdf885"}, "1d864c9d7b306b8e4d9c114de5b2005f8de5545b": {"doc_hash": "4cb15657ebac824a7ee1177fb5519a68a991bd1e98dc67954b077a381f6ebdb8"}, "46e3d7596893011787f432b40d6a1df8350a2a5d": {"doc_hash": "37fe26e2389acff66c1c3dd6787927a05a244ab7b9f6522b60dd80032934dc4d"}, "81184ca5ff0b547ad19b0d8cd380b448d2ea8d45": {"doc_hash": "8e3d97d1cf499b84c698fa9c4896d6b4a9537c5751c37849129d6549d722af8e"}, "d16cc4fd2d0959e625fff46701a9a26e2631b212": {"doc_hash": "186327e396279ff89ab170aed0d86731c4b6b4d0fe4d1daa71dc93f0451265b6"}, "8db5d1e6d439df72d2e2288b334620e1c466d4b2": {"doc_hash": "63f7794fa933634bff46692078fde72518487477d9a9c4b5d404973e18263814"}, "3997001189ab390bb5556fbaa4f192d21a5ca97e": {"doc_hash": "1e857308a295f59505655bdf8ebd5f14ccc7150f46529107805f7d57824ec859"}, "ea407d8f75c82d2340a4822768dea1b1d1dc5f3b": {"doc_hash": "6dd5dbeb69c43b0cd283dfc3511b4823270205123dc94212722c92655fa762ed"}, "320cb3c16cf85f2501d99a058f1de1500cf8b7c5": {"doc_hash": "b1c1215b845556e3b3949f6ffba401e37d8a0850dcfcf0b2acc0e606a170e09c"}, "944c74eec7af84e779aad540f55ce7dd5f22f943": {"doc_hash": "d473f9fc8740bd5010ca5bf98f5ba71473dd8e25ceeae32165f651251c8e0950"}, "59774f75d33735d65860d860c995b1268238da57": {"doc_hash": "53f429ce830920960a68e7ba70d4c502cf67e12b21cd811fd81e8795670dce1a"}, "181b8f2a3b6e340f6c3b6836765827ef093fb03a": {"doc_hash": "e6bbba299b20d801dc4c1fa90432f3b21595faa1162139aa66f44eb89610be70"}, "2cf913651adb3d5b107864c0de10d3792c1e5a80": {"doc_hash": "929fdf82659de28352a8f10edd91b18ab955309e2467c6afe76c7e9fd254331b"}, "e1863d3d8cc4a4f6e4a3e679efb1150554856a4d": {"doc_hash": "fd5476451b94d7a24abf2e9f3c649d9164c57eca359746eaff7c44ccb750be52"}, "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de": {"doc_hash": "ae07fdf348a6ea749ffb16124edd94504277c768d2278451d34431812f86aaeb"}, "6adc3ba5024f48d99774adf85a588b43ffa72bb4": {"doc_hash": "0d5ca9c3686c2326e07e0b1a45525abcb2bfdec8e875d28d81a364044e0dea70"}, "f11be944ef8b97431d556374a854175bf1b872ad": {"doc_hash": "32185f532bb5452f9b60ccbb6392876fde490ddbc11b76bc63d170709136c2a4"}, "454ab3769cc57ab87c75c448164278d5eea20f68": {"doc_hash": "d93dbc46151c779eac156c8586973d57818e43395a3c72b9d71d0e49fccac1b4"}, "2b9eabbd39588943350b9978b45bb5b23103fa62": {"doc_hash": "7d26d3c489e0d3693a3e653db4f9dd23aba19a572db7ee4c800c5c6ee6329ae6"}, "5d96fea36c825558719097764f90aae4ad97c4fa": {"doc_hash": "efc331f3be4235852789ad0c88b5b9a8143e826df2e5619161ff0c65c29d9314"}, "0b54bce457ea9eb3886a328e8c755629fdfb9965": {"doc_hash": "028f1873b441c7b1feb39dc723bbf384b771e68e97ae46b32b5bcf8dc955dd77"}, "4635f189-1158-49a1-a556-91c606c167e5": {"doc_hash": "05722304fac3bd967412a7bcb24767fd5189ac9d10b504606d04ed2bb7efb92c"}, "f1d90cf4-57ff-4285-a17d-6a9df23be39c": {"doc_hash": "d0540d28095305e0b6f91636b912dbb0cabf63849e13c10346d875db013be47c"}, "ed1617b5-66d6-4d3a-ba26-7be2b4ae5255": {"doc_hash": "e176bef703f895f1217b6ea19fff15a35ac114799f747159dfba3b9697cddefe"}, "4284f696-ebad-47f2-b396-b2f747cf3a26": {"doc_hash": "9e6f8ab686effa800497c1fcf50e630bb1e7d352fc4f0b52dc9c8d6d33a5fe9d"}, "c390d2d1-e9e8-4991-af7e-f828bdd04a2f": {"doc_hash": "ee7d8fb4b288423c13b7301617c498054824a5ac58b0815f2624e6856db35bf2"}, "22f3ec44-1a3c-4a9e-a417-2162c119ee49": {"doc_hash": "0a79629b7200db01135bde170eeb26de72b5933066e763c49acd4545926798d4"}, "2e3bc64a-325f-4725-9f92-a4e46b56d60b": {"doc_hash": "44b4cbd335da28533f5242ba5ac89d2fd5cc88b9a9fb6be16d484042eb856873"}, "2a3cfe7c-439a-40da-b795-d986680faec8": {"doc_hash": "83716859bdfcdcadaf69613a72941165c41e7d0841f871b64a37fa79976eb0f9"}, "d8a08c82-1bef-4e97-bbaa-cfb9dfe43ce9": {"doc_hash": "e322e193cb94a93e4c18437e8ba7d214cd824159ea6dd111434bca517777d4af"}, "2fbb5ed3-2e8c-49b7-9c85-3b2ff2976817": {"doc_hash": "36568591ee35d1ee410178468d2938c592d99d5ff00df2ac049a331fec96ac01"}, "e6da9cda-be5c-4047-86f4-3de8c466ffaa": {"doc_hash": "e816a09b2d09e320cb606e91229e57734acfe9565aeac62115db4dd6466e7b2c"}, "59d1b062-86c9-4a52-a8db-6c9065d042d8": {"doc_hash": "edd7d8eb303691e0b40704618f054500df6738b8563f036fd68d69b09bc846ae"}, "4cecc9a2-81c0-4320-a46f-08ae413658c0": {"doc_hash": "7ee972d01204cff6eb8a3c19890eded6764f70b3e714e5dcb366ac3d2d11bfed"}, "9e5848cd-2ea5-41c3-b957-42858921f610": {"doc_hash": "bdadf076048694d832dc022c7e45caf52d4798c3979e9cdefb16d367155f6f6e"}, "7f9d39e6-2488-4db0-b644-4a22dfd0f0f6": {"doc_hash": "a3b5904d84b772312ba9c477e7c67b1ebb233d6f77886b7569dac982a830bca5"}, "16aede74-9972-434e-bc64-c6a537a14eac": {"doc_hash": "4f00ee343ce5da1e830a1040b77e9e7f857b7b8e3b31cea6d941b42cd0f1de6a"}, "ae9d0196-05dc-42d9-8fdd-d0e9b41504ba": {"doc_hash": "ecf2ad95b232e9fdaac4988f63236fa1d4e5bcef054ad02949abb91a74ccc3a0"}, "b8846b64-9555-497c-abfd-66f438a00b11": {"doc_hash": "0514c7946068c627da828f1fcd9d6ed15c022b64938f5869871a907aef160498"}, "3d67f6a6-90a7-41fa-aaad-864df1de4ce7": {"doc_hash": "8956f3540386c71f08c599e562ddd8934962089ce3a2f450d4ee3f83b88db2a3"}, "e547455a-37f3-4146-904f-ea3596ea15a4": {"doc_hash": "42514ae34e9f6f577caffd77b47948ec8b4b5b1c4ff74f662012f7449e41db30"}, "7168fdfa-4084-4a20-b02a-926086ab3ead": {"doc_hash": "aa5e63d451bdad9caeda65f059dcafa2d2b1bcbb8932ced83dfa99b35317d836"}, "58e4b1c9-61bc-40e4-bdf9-30fa95aa0c97": {"doc_hash": "223ac5427ed9bb7d06b13ab6f633786393a71407f4a199dc8740068d2819bf4a"}, "95cf1b1a-366f-4ace-bd97-8c09267b1422": {"doc_hash": "35dcb44bed90f9ea50b5ace6d91584e3c84d02703498c37bc43c560628942f84"}, "affa04fa-bfe3-4603-b8da-875f9740b27c": {"doc_hash": "fdf81cb75058c56bbed6d5d793c9eb5ee8b440b612d171ce030bef7e9699752e"}, "7061537c-6bb1-431e-85ba-5340b6d7c544": {"doc_hash": "8a051c6b45c0fc4ba17888daf56e55483cb20382f96a00f097384c0e25c15c71"}, "540a9298-44b6-452b-89f3-e1df09535535": {"doc_hash": "aa4d64821f20faa263d02d044d2289f538be76def980c7f7a3a74c8344e4c80f"}, "80642d4a-8e8e-46d7-b681-15dbc2374ac5": {"doc_hash": "3e499be502a4fbf277dbee822883ab27291a68818154e3a8dccb485d62c79678"}, "26fb4783-fe37-41cd-adb1-4d99ebd205a5": {"doc_hash": "2aa6fbfacb0114acfb65b9c8e28c3118a2d81c070b845924311adbb21b1186de"}, "f501ca90-0c88-4bb2-83d0-e43e634ac8ec": {"doc_hash": "1056d4921724c34b8871b58603f179013df777f1abad2420c06a887285ea4a4f"}, "9adfaa02-f3ce-47c3-a6cf-4d194103593b": {"doc_hash": "8d149c13bed9e7dc0c01ff62716e265c216ac54ce1ea86fba59efa1c66325957"}, "2860cdec-aea3-4d39-8bbc-70474632b43f": {"doc_hash": "eee25ec340ee6f097c52a25c70e3f26e733250830c297d17f24f0c17e6c326bd"}, "cd472d9f-d039-4f43-98c6-76bd2c1d7897": {"doc_hash": "4e187a68109e391573b2c6a69d892deb58d65ff23d0a95e2601d479d7041c05c"}, "3089486b-5a50-4f76-bd8c-c3e64f07d38b": {"doc_hash": "ba7e7f31f167d35a1a8070d1174f2bd3d54f9d2e66e4d113f9fe8b428f292223"}, "453ea2ec-86e5-4578-8f7c-0a1730ccf858": {"doc_hash": "7e3c2f487d9fe772afdfdc999eb424189397896753ed950ee1a1ef8406cd688f"}, "b04dd427-f9af-4d3e-9079-4f04aff00e95": {"doc_hash": "9d3276124e1b6aedcc93046a5684f5c4aef29b4bbb7c111142e77e15469919d4"}, "8d1beeac-9d78-4a70-996b-3a28ddf18e63": {"doc_hash": "58ccd4a9047696151bc658f6cf80bb3c2f833a5b4694bba5ead31a6122c03335"}, "a2b1b96d-455c-4033-8160-aa5a25f0e2b3": {"doc_hash": "743eadd83fca789cd631d6cc59c5d063bbc5b9174e10705b96e176fbbdbd414d"}, "b2aba21d-fd17-40f3-a618-496cf2a7978a": {"doc_hash": "c9ac45f55d44c022afe25ff48ca1bbda3c51a960762a098cc0056a6fda2a68c8"}, "2b4d5910-9827-414f-8633-9fc80a1f7916": {"doc_hash": "9c0f502d6e25799b203b0a4ca5975805d3e2f973b1a8df2072841f32d9ac523c"}, "c2d5978e-0caf-4bd4-b711-12539f419076": {"doc_hash": "66285e2d9c71acd2c0d8e2040e0429e6911ea41aae1ee7d66bf9811c5b196cc4"}, "19264a5a-5884-452c-972c-7dafe4289fb4": {"doc_hash": "f87ebf9b983b58fe2b2721c12e6c18dcc9fbe17be321f3ba4fcc8bd0cb66f1b1"}, "2ca8223f-e33b-4003-bba1-0e251330e1ee": {"doc_hash": "9486ec6b85483d722685be637b4ca58e27b1dd9895525f06e0acdbaa9113068b"}, "f4e01d7e-2f1c-4053-b334-f0d904c11ee8": {"doc_hash": "6c4a0ffdb41f729c35811d9779e543b16ef9d47c0a31ec6b4f06af48a209735f"}, "82c4ea39-60d9-4867-987d-6457f2f528cd": {"doc_hash": "0a86c2d136560d85cab88a8d8120bcc81339c1e268fef303d744faf19fcbcd8d"}, "8768b9ac-ce46-4c2f-9aa7-b1f8bb3d6822": {"doc_hash": "ede7d62e3cb64a0346e4b4eae31aca6747796af31a5ab57832898832953a7fd2"}, "5254fa30-8598-4d3c-b041-5760105955bd": {"doc_hash": "3953ee04d1354839de4f600484322ec778aeddd24c7a0782887f73ab790af6f2"}, "dae0f160-8532-4c16-87d5-2a0bd6f9de00": {"doc_hash": "e62e0775b3c60873a235db51e05a97c993b66d6b7c4c80342a8bc63cf43b1a67"}, "8faf17ab-c765-4460-896d-8fbe9a486e6f": {"doc_hash": "bcf0acd411ce8400c5996c103a79e3b81ecf3475e64d7efe4a6ffef9cbea3478"}, "dda4c406-8745-4eb8-b6e8-991d4bb6a30c": {"doc_hash": "30f2a9497758ce4591f76d7b8f95d5bb4ccaa5d878574bd5af0d6a7a0d488a54"}, "47eebb8d-91ef-4aa9-990e-5718c03e843d": {"doc_hash": "ffc17d1e2b61b040be828daef5d3ace4af460b36389049f26487e7882d443737"}, "b0ac782c-40a8-4bfd-aaf6-09a205e5ffc8": {"doc_hash": "a91359614a8f4dd8fdf4f19fa66a272a49649e9884ce6fdcbf571d988677d038"}, "bbf88635-cd9f-462b-acaf-3aed4838e1ad": {"doc_hash": "da6baab307a13942e5216b0244aeceafee73f1d859329aa78663dfeca2b171db"}, "e870ec1c-89c7-4bf9-95d2-476717b255fa": {"doc_hash": "099e020b070e08df14667b913e767c701ed6e7124a08843e176c2bfd8a673d09"}, "fa2fc4ed-c7a7-4739-9ca6-fe49ed3172b0": {"doc_hash": "12671c6b2065b4096f164a6954f01955e32638c941e578957693e6a5141f09d7"}, "e14d3638-fd21-433d-85ad-ac529d57e347": {"doc_hash": "2a63cf1ae01ed17bf3388ccbf91cc22f278fcefb8324841d6bacf6f7cb55389a"}, "814dbf86-b67c-4d1b-ad63-ade3f972fd15": {"doc_hash": "4329b31b8b07b9c72faa398bc0350c71685a676216b3f2796b648579946bf7e2"}, "a220f99a-22ca-49a3-876d-c14327f109a8": {"doc_hash": "b55b1d0ca9bd782cdf26bbca5da7b65fcae71d188d58dff42262f4eee6c31f6b"}, "5fe951b4-00ce-4ede-a3d5-fd278561704d": {"doc_hash": "e426db9e685a4ace00b9574677fd9effaeb7ab991db7d7991ea475d850a3d4b5"}, "6b572289-a953-4bbd-b667-9c82623f4dc8": {"doc_hash": "0e8d7ea71173f3ad76dd90f6bc1f97739377dc7494d48e0b52cdbd301e79f6fb"}, "ced66210-a50f-4d41-ba13-aa02b5ac0c9f": {"doc_hash": "44d751fad7bb69a8817c3fd3b879cfa0e07dd88bd9c2008e04bdd8d3e607cd33"}, "5b1d9ee3-8e67-4b44-888b-e2ebcb791882": {"doc_hash": "0f1e271c9918dd87a74a3ec6aa37c9f40f795fb16f4545a9d600d7409fd5e205"}, "305a13de-a0cf-4aa7-9b42-e37d8872190c": {"doc_hash": "22a8e2124408b8cb1798601828235cb6b0fc723736f23018cf92a1ca7cae3f88"}, "641e91e0-ca24-4a61-972e-4e43c7b79b17": {"doc_hash": "e808ccb7a93317d47fb8fe3d5e86bbf4c77cd9463f7201f7b753d422fe5c8a39"}, "648760d1-9933-44a9-a969-81c4a28f1be5": {"doc_hash": "5441253a6517456495691e4b4609ce009e7f4832eb2e030e6121eb0c0d7b28e3"}, "539bfd56-684c-4635-b1b7-15a16f5c6ed5": {"doc_hash": "ede7d9588ee5637f99f2d2e9b5e4a74e6f31402d8658414a0398ce1231f1c523"}, "30ac438d-5302-4574-9475-3c5fc7aeb259": {"doc_hash": "bd33a52a6fb771b49954a91b6c49a1b8c6dbc63487d0e4b09a3c800679db2981"}, "221304b4-db53-49fa-b8dd-fd51f5bc3da1": {"doc_hash": "ed35710b20551fb2a689386e49f6e5ab1cb1679c6252bd516a7b5a1fb5842235"}, "04b17466-e501-4e79-b28d-ceaa306a8d42": {"doc_hash": "815e74673653e1dff03588d9e5a50196cec0802b9295d5d2c8a23e1b691602d3"}, "d324bfe8-bfb6-49aa-a867-d975022178d9": {"doc_hash": "ca9872e8185a18547e5a90bf53f98700d26c0561b6b82b99a8a635155c6c2606"}, "62cf7729-e7bc-45dd-a70f-375233356f91": {"doc_hash": "d74519ba549237a8760ba7b9faf999821d19521c6d6cfc4be4f13e8087e85496"}, "98ecca9c-0fd6-4349-8500-4885a80ffbda": {"doc_hash": "13d84650c82042d7e96c877ee03174abd6ec6e2c240fbb263783bc129ab827d4"}, "73a540a6-48cb-414f-b781-b9a1a8560143": {"doc_hash": "fc7acc3c067e98b7bdd9f15cfdfda2415aa7c22487d41993da32c83e6f717f70"}, "9d4bdd0b-320b-4b4b-941d-c912ae2573e8": {"doc_hash": "704d654b9dc7d159b87fcf2197b5d7f645b967104c0138559e6820f1c16ed582"}, "1d723cd4-5557-4cda-9117-9416abfe3162": {"doc_hash": "6e6d9e0a4f72791e8cb1acde72d7056a2bb8464a93ce20bfa2664b60e78accc3"}, "95ff0e9f-afb1-4ab3-b476-19ab30cfa108": {"doc_hash": "347e06950e53bca927248ac2afb8bcf79ecb6131e8eb8acd3990be8fb66efa78"}, "0983c8fd-230f-4142-846a-29abf68ccc3c": {"doc_hash": "add5f7c13729db51531adf8611de2839e0e8b17c3fcb7833b0d37e7ac37f4260"}, "996157d5-7d73-4df1-ae0e-f2e84c4063ac": {"doc_hash": "c1e539c8ed68db34094dd7625e42bc2036ce20a4d3540b6fa5932ff78d8b1e34"}, "811fcd0d-e4c7-49fa-b9fb-5ca0b11a6a50": {"doc_hash": "295742afaf2bdd27e1c4956fbc9f5976fcc88ee91c7325dc2a0a05f2b20cd9a3"}, "e42f73ff-1ddd-42f0-b40b-af7d93b2e5fd": {"doc_hash": "eae84c7ed998231c2bbfee88e963a0a78fa655f3d8cdf3a18f6ae85e0616a92b"}, "17e8b352-cf63-410b-ae27-4a493abdaaeb": {"doc_hash": "ac1bf100a94bf1d55327713eb7111be934518c4df96f7a1ba7d14437a412e6b8"}, "eb960038-85a8-4e26-a44b-b110986ab674": {"doc_hash": "a49619071730c4ada516368343abcefc49b13653d1b948719e5dbf3bf44c7f91"}, "ae984af3-aa50-4ec2-84ee-cd2c36b99dd2": {"doc_hash": "c5a2ce3cd71c9f2af3d4c2a189785d0f0be535953ca30481085bd76b36c5c864"}, "832ea36b-97c8-4bb6-a6d8-042b718f2665": {"doc_hash": "676e3903c6d61efe1c12777b5c056c225311619e74c9738487a10cca4055f5ac"}, "cca49118-4bf6-4e34-9dd1-19af8b5fc58a": {"doc_hash": "9c9b6856974c6fa1e1e98fb5c24d8cc8f6996f715b344802d87e2b1cc190a0be"}, "4a45bd57-03a9-49b4-9f80-3f948e0d65b4": {"doc_hash": "8d6470d55669c73b5dd181f3065f9b5d9d5be9b34a7e19976b15c1b2ba4cb267"}, "4887be70-54e6-4457-911c-d67376492e07": {"doc_hash": "60fdbcfcfd83a025946461cee2a1afdbe67a638731ae04cdfbcb29a0a1994e98"}, "883de9a3-ad84-4860-a698-fa863f008dce": {"doc_hash": "504805d9bfa2c51e7840e1c71ebb6ba969f794d5bde7e36412d11027ace06338"}, "5e789ec1-6363-4684-9f5c-caa44a7d1ff3": {"doc_hash": "297f872503eb20da76bd90b0a56e8646b1c17042d3aa75d7268108bc3e8a4b3f"}, "48a2df4b-b862-4625-89bd-519c23b62d97": {"doc_hash": "4a41cc71a9a98977c5b4cc9577daff141ad3f153a91134aeaeb587f20e97e79d"}, "9f8f5602-1a3e-480e-8f45-ea15cd143c9e": {"doc_hash": "d1cecaa2d051d9c2b9bdff3eaa7091bbc6e6552dc0a5c423bd198fa1101aaba6"}, "e54b12eb-bdb0-4384-8828-236fbbad53ea": {"doc_hash": "046ab305d723da9a23ca01d10ce3d1788461de1718abe523e03bfdd2068bc90e"}, "2042c87f-6d15-4225-b93c-ca0dbf9653c5": {"doc_hash": "6baf9e20a8dace76391e6a0ec62185faf1adf5ad533d2bc4b28410c221e9c93a"}, "2bc9688f-1bea-49b5-851b-b5188e603994": {"doc_hash": "9226f14845ba36f32c185936885bd43eb955401d4b025fd6f6b6d2e0f580a5f6"}, "13fd19eb-f1d9-4598-ba45-dc9bf4ac578b": {"doc_hash": "fb21646063ba9c008025a111ee6f5b3f98a3590996dbe836b2f95edabec43abf"}, "007d169c-8b7e-4234-b68b-7ceb7fc88f1f": {"doc_hash": "5ea46e5b8c14532882a06206fc47bccb3325a1826de6cf16b8a2074b568a7454"}, "2fe5fbaf-5b21-48e6-9436-fce92887f881": {"doc_hash": "2c72063991007273d06786876d5e59e76cb2057050b245a93a776faec69b71bb"}, "963572fc-7d7f-48d8-a5da-6f986f8c22ea": {"doc_hash": "72cdf2b44dc3fbf483e711d7bd74676ac1167de2d1d3c4cbd2329c6a72a09544"}, "735376d5-d5eb-4764-9d7c-c458c0319668": {"doc_hash": "0b77a3133c7224e6dc5b5834b876932a9369d413d4abf20067184c4ea49d44d1"}, "acd65ee3-f199-497f-ad7e-0c835e6da534": {"doc_hash": "d44c2f80b70419eb6029d8aa3589e981f616b977e99e6e4b2fc647d738f3a355"}, "fd1c2d4d-bd8f-4afd-8532-9727b9cfa6db": {"doc_hash": "fdb19e0eb4a054724de5175029c8363fa792fc063e0428c1a7801f2255f66b52"}, "6cc6666b-4ce7-4a72-8939-5dec06e880b9": {"doc_hash": "6b6420cf8345834d779d69805857eafc7b613ae53b37fc399eee5cab9e65d2b0"}, "4849a526-4b72-4c38-a00c-3d19ce8b0083": {"doc_hash": "032f04619b90b6ef76bc4f89f04d009b59160e93c0534d82db572fe8dcdc4b6d"}, "67ab8580-4e23-40ce-be15-b3eebe8165cf": {"doc_hash": "95aa1450e6ba08217998596e62e1aa57cafb8be17406ad4db8c493c9109e0e71"}, "aac418b7-a2f9-4d05-b013-23d1c0cadd30": {"doc_hash": "8350d1a0c12cb647c32cffdcb7a45bf96b0e316a71c08c7896ad9f1a5e6d7a0b"}, "0516f22f-f15e-412d-afc6-c369c38e7770": {"doc_hash": "329082ab1dfeca17746004f105df0ddbaf9f52f9ccebf4f0a551fe03ab86f86e"}, "de6ad467-b17f-421f-8d3a-eb53befdc293": {"doc_hash": "ee03388453f7d960e1d857f0124f539fef9f7aae049f488e57a64bcba6d607d2"}, "6db0684e-84d4-40b3-acfb-66d27fdd275c": {"doc_hash": "e8fb317ca35bc9b243334ec256951c1ecde24babab7e6f4ae0d31c38ba16f75c"}, "0167559f-bba8-4004-b227-e9ed43aed324": {"doc_hash": "58947e6617a7c84deadda2c63a4c1d5adedc0e277ab1483e74b8c5d5bb796e2b"}, "5affb53d-f7f9-47fe-8d28-ca463aae0857": {"doc_hash": "5a3b3116b3e8d69857de8619e7c4aec02b876d245010f03a7d128c45cc2886fb"}, "9aa0dbc2-9983-498a-a445-8bd38a71de17": {"doc_hash": "aff3f9007df58f1180bd839cda992038e81386d733860ae596af74f199e5a659"}, "e9906428-08f3-4084-8e68-891ed74e7e52": {"doc_hash": "304069846fecef0244c3f766ce0ddafcb3d61fd2a9f965ee78d56ca71677160a"}, "fbc292cd-e83d-4bc5-a6b1-9ab86afd0b57": {"doc_hash": "b039ad1fa1bb5681ecd87897e3bd9b7ddd2d2ca90760f9d816b338451ec53a2c"}, "5c7898ec-facb-4f3a-886c-af08fba1ea76": {"doc_hash": "b36c9d99dee669f07d9e74e18ef86fd1ed9ff921fe2f49400d070a3b5e43d006"}, "df2b9594-54a4-4b8d-929f-55b105551264": {"doc_hash": "62028fcdc45f7361763889c8e5e27506abcffd91fba291791479b4c10d497d65"}, "f2ced293-e6fd-4d8b-8e0c-edffadd66a84": {"doc_hash": "40aa0cfba66e881aaa53d972cb221e0865f3c2c58816a54df1b4e84c815ffc45"}, "dc23b975-1086-4ff4-8550-cfe41913dbe7": {"doc_hash": "6e990fd204fd657211588f9a77bb134d0979aef76dcda2c3c72602b531cf49fe"}, "9d091990-a81f-4829-b2a5-ce3982b79c92": {"doc_hash": "b85a56ef859729d476e62aea7d6b735e4f24f6980d438020425077a071cc93d1"}, "ee1616c6-8d5c-4442-b95a-a1d8b4bd0a7a": {"doc_hash": "606ce2c5ac3be645ee45e8302190639dae7898c3cf2a9e3ce29b5376e6be8ba6"}, "d03d8a03-a100-4f5f-81ad-a0937dcf3a6a": {"doc_hash": "41b9ec3c4f0a6c2f98b0344dd0f4b86d9a1b2ad36918a1cbeba56249fbc56b1e"}, "dc3fd654-af14-4f3d-8614-33d930a50759": {"doc_hash": "08d3c6f7f7334c07c69816610b1f4789ea701bc479235285f074d074e1dbeba7"}, "90722ff0-2c72-4294-84f0-6f803ca8f249": {"doc_hash": "be17d547414f4d6d7a66a28acee1dcb0ecb1a86c42207e1aa0fd6bb2ad90580e"}, "b5bddc95-0223-4e1a-adb9-299e83443d83": {"doc_hash": "d1a0631612f4e3fef119f9a932ccfc3deaff3319aac71da105f6cfc2dce55522"}, "98e8d743-2611-448a-8fa4-32a4796e4d73": {"doc_hash": "e0f16e33dad8719650ed014883d33e6d0758fbbb94258a8e77116a0e8788b750"}, "57125805-a724-43f2-8448-7915f58f4e3e": {"doc_hash": "97d066076416a263224d5c88865d1c9881a6b410e67345ec54a7f2445dda6ed6"}, "d7b286a9-25cc-48e7-bc5c-ff2592b3e85a": {"doc_hash": "b87749222697f0e65540521bc040835a9cdd44fdc0fa84583221e058abff9490"}, "0e2475e8-744f-4f3a-aa18-0db3bf6225ff": {"doc_hash": "ffa57ba1e5a69bf1145d7c31790f1b04041ab46cf872c7400156c7cfa8481df0"}, "bd533916-c65f-4efb-969f-78e60146d8af": {"doc_hash": "1899c67ee18d27d534ade9830fff893c610a0e5ff5b99580ef6ee3f06f63b042"}, "54f957d4-7d3c-4580-ba76-e886a0a79094": {"doc_hash": "d6e7dba7aca7927d566171005182a814d577c4278973a032715883f00c8b8383"}, "8bb32434-5d33-4217-a0d4-4972edb06bd5": {"doc_hash": "7d7a4aa505552d7cc1fa27c8ab8568e1b00037bc58d96fd9dea72768a0823a93"}, "5b33e043-9b68-4b59-9cca-60dd29da9fd8": {"doc_hash": "d182eef74379e923ab9ac71f522af54030217431be46e4ae841c5ffcab1f45cd"}, "35aa54c1-f2ae-45b2-82f1-0cd607f1846e": {"doc_hash": "7f16208d8a9bfc0b2d9f1b78e3620a30ce90aa59fa810f19767d7b99cd181181"}, "7c2f200e-2820-4e85-99c3-d728ee8a7f04": {"doc_hash": "4215a3d9ef1b5be7a9e0f660dc82d80711c31aa2b6dd45ffe65430dc5bbaedf9"}, "c7e240c8-115c-448a-8404-184ac239af09": {"doc_hash": "6396aa842c34242ba3f157c2dd7fe42773516818e471fbe299b53d9ff8163b23"}, "7811493a-bbb6-4c52-8f2b-52534f634851": {"doc_hash": "107be0eb8fed950287d978ccafe1fc7a6d10db37f2ce17e6b95b0ad0452c78c1"}, "81ce16e8-a033-4a7f-bdf0-d497d08c9531": {"doc_hash": "a2ad7ee99f8725e9f0ac33ef81bd9bb88d0bc53001dc620e8c2b6b28ddc3f5e7"}, "345bd739-daf2-4390-981e-9ae0bc2bfda8": {"doc_hash": "a26d4aa577ec9c6974b0598d305d8988499ea27e54ac32991a2355e66769781b"}, "7b228eea-75dd-4f59-bd60-cf8e4172b4c8": {"doc_hash": "03cbf7b9fddca5a14aaf8e5a553437ccee9601724c1f570d1112edd32696b7b5"}, "01653f63-c7d5-49ce-af09-40b0213f66fc": {"doc_hash": "df1d97ba546bff7c5bdd4a08783d2f5461cedbc4d4e2f2d34f6308cd880eb97b"}, "1d4e7247-4c68-41c3-a5b1-39e9f075ec5c": {"doc_hash": "c761e773aafb5624d6df56261dbcf00f181d2bd1bcccf4b12d8fb2e0d34934cf"}, "36579e79-9794-4cac-a9e3-6fcda655a8b4": {"doc_hash": "632aba40447a4ab74b21332b4b5883d041a41f21b2e0b5a327dc187f1281e942"}, "eb757c94-a8eb-44a7-b07d-f3da39308a32": {"doc_hash": "85660aa7e96c5dbfefb32a2a428747fdfe859626b53b77ab9173d49d111ebe02"}, "479a153c-da35-4d6a-8915-4e6b3fc8586b": {"doc_hash": "d0c7309b7ab1a8e4d2bb888b651878911c438d0be67391734766e1caf31f8b84"}, "ed887cf6-0891-4b10-a3ac-a39cc1759831": {"doc_hash": "bb067477fe13e0b009ba62ba48ddb397184d16c5489ebdeaa2ec7c92ec7ecb39"}, "c0d04927-bf51-4e0f-8067-cbd675a5778d": {"doc_hash": "1c68b28923292d5476a9a64ae40227e1c0ec08a950bef411235620198cf41730"}, "63e36658-7fe3-4f8d-b57c-ab37d776d9fa": {"doc_hash": "4939e2e0c49c1b213812fa63ad3b86f3fab562a35844aafc34ec06e9a99372c6"}, "dad04241-328d-49b2-bf34-583633f98936": {"doc_hash": "329afdb7106a52b7c290b0868b7396bdf04ec9e7fbc756499612597e57fa3414"}, "71c78f67-5c2c-4a8d-a07a-b21c4270eb30": {"doc_hash": "0d0f156abcad08dce9e993b7fd2753dce78e1f2fcec6a20562e8ba479b23192e"}, "46cd618b-9346-445f-a948-e8c00b8ec94f": {"doc_hash": "be1eb3963c090feaad03594e13967e6e48d69fc9fbe57fadf494cac3be436894"}, "148e5af5-e25a-45ef-80f5-84a1505ea4ed": {"doc_hash": "f2ad06e0915e65ddddf287ebe96485fb024286e7da1dfd86b5779e06a427f47f"}, "7aa851a2-ea74-48cd-8410-c0933f23757e": {"doc_hash": "718854bbd59097c36bcb5fa843e1a7b2b7818918057852bb64bf668d967e594a"}, "dce3e166-d0de-4be0-8538-044df38100cd": {"doc_hash": "5db68a68d642ed1cf5f55a1c976e2ab48a2b2e7c0d38c91cbdb3553fec3a106e"}, "74d3b28c-9e66-4700-876e-ad19cd492af8": {"doc_hash": "e5eb8283e13083207e54a014b67cb16661408cc4579a3f5544c24e43d606546f"}, "feb00c1d-0c65-42a1-b540-cc261c19c998": {"doc_hash": "3e857cfa976cae97fcd75c66fa3692156772ddbc5b4d2ad2681a95c726cce7bd"}, "5424a7dd-7d6e-4c51-81c7-04023f81b4d9": {"doc_hash": "b1e735f5ea3e8e6615cd2269a7b6e626adc1f0c0876730196ad460e9fcdfea92"}, "3e64d32c-6f0e-4e50-ab1d-4c5f0ebae469": {"doc_hash": "4b6e8dadf0454bd31bc8c973003839a1d061b794dc56c3fd3aba09b66b88f857"}, "1e3fd23c-efae-4242-bda6-378a889da189": {"doc_hash": "0db155cbda708d00d6255d51df7446d01a9a9c3a1308943b50381f59a904269a"}, "ba9c0a37-dcfb-4973-961c-293b3d8c4169": {"doc_hash": "94c170dbf9d5c45ea1d71a2605cf65044a509d49ed2cb5862303c393a61efe89"}, "ea8f4396-2ac7-429b-8e3c-6c8eaed77413": {"doc_hash": "700f4835f7c6d96e22bf05a8ad0856b55ef39b00cb422f12127b786d590dc50a"}, "0019eab9-59b1-443c-9108-21604c4e9767": {"doc_hash": "23fd011bd449a9cd45fdd402340ae182603ee8b434c65bd02d2559b2a80fdd33"}, "62ea21be-6db0-418b-9b1d-bba9531e982c": {"doc_hash": "a3f95c6fa3d2f17d5c6bc32206f7746dfd053f0a110ac89a3d8994b801536414"}, "4b18f380-abff-4db2-b9ad-fa22dedd76a1": {"doc_hash": "92d1e468635a7ac256815dbdca175421cc1d09063277757233baedf31efe5748"}, "353c4864-57af-4648-9132-3f924225f507": {"doc_hash": "cc003e0e28e559ccf2e024a6b14e84102a5dcd9a4d0dcb27d0425d3fb015a671"}, "4dfda049-5073-4029-a0e8-4da9966e3985": {"doc_hash": "c973a1e545ba7e163cf320c5b12ef672a875f5f46f091d6c62008db5d0031373"}, "59f45a9f-5cd7-4986-89b8-a7838c3fba9a": {"doc_hash": "d9501b67728ffd303933cec0da08c3c159e4814776de59ac3594181b0cb50f23"}, "83c854b5-359d-40db-87e1-bcb5731b662b": {"doc_hash": "ed0520074cf155436014fda6529d3a583607af48d0734fd491e1d7ab6d6dab50"}, "b409c2ab-bbc1-4596-a594-1b53621372bd": {"doc_hash": "0b5ffcd026c4a81e48bda688925c4110aeaf16932fbf0e8ec126aff5835c35e2"}, "b2c6f2d2-f75c-4c9a-addc-ea5f3189d5e8": {"doc_hash": "8fbe693004ed35b2222259c9197a0556511b347cf7dc5c420710dbe09936eccc"}, "8700e7eb-76f8-4ca1-a65f-b0e516f5e079": {"doc_hash": "7857b0edda7604820fdd40960f8c3c570b6da01a42917c9b1ee59584c9939f18"}, "b8702bf6-4034-433c-b27b-12fc12099b6e": {"doc_hash": "f8f9c341df410bc8440b30ed821b8ee5c9cfbd148233c28ea3d51f9853755d2a"}, "50237b84-7b73-43a9-8ba1-670a752adaeb": {"doc_hash": "d35979f8f52181a2fca0752346d1d8c6fcc167857ada1764e0cefc18b5f2fffc"}, "3a640ebb-98d9-4b35-8262-88d2cd2cad41": {"doc_hash": "186327e396279ff89ab170aed0d86731c4b6b4d0fe4d1daa71dc93f0451265b6"}, "efd68da9-04f6-40f3-82c9-609631199b25": {"doc_hash": "c19ade17e90dc6d264c10bbab92e3a756cde9928acf2d62b94c474d7d8a8703c"}, "4f5db08a-ecd2-40f9-b3b5-96d55954b7a3": {"doc_hash": "82f72b889bd34edd2e07b223075defecaced495e5ab379e2669e265df18cbfe3"}, "99c27247-dafc-48de-832b-e9ec1dd087bc": {"doc_hash": "370c42cd9b0ad0994161c57b66aa4ff2e6aca97de9cab28403417de44516d8d3"}, "5e6655a5-0b0e-463b-b383-1e5b6c51bf7f": {"doc_hash": "c2b33f57417e5b48c204c75cab4d8afb0983072d8d00f9266770d7bdf46c6ba1"}, "e1c5028b-c6ac-4360-8ca3-3a9ca2a3e271": {"doc_hash": "6dd5dbeb69c43b0cd283dfc3511b4823270205123dc94212722c92655fa762ed"}, "c99921b7-8bc1-4328-a4cd-2037b078b2ae": {"doc_hash": "369ec29b0f5d7c0b8e495f7d7ff3f695a0ada4b7b27c146e50992e144811fd19"}, "832f1438-4af1-4f17-8151-f8c1c0758516": {"doc_hash": "9c9148d0914c9deb50d1b036fc1083c792d830d0501c351ab761404480204913"}, "6ce7b357-b9e4-415f-a789-64cf4b1c792b": {"doc_hash": "323d6673d46cec29a66c5c1da3b74e74c5553dc2b8dc4ca22e350029f54dbab5"}, "c57337dc-f595-4db5-9a49-070a4c777c8d": {"doc_hash": "39dcd4de2ca1e3c7cc9b058934f88933792be7a63b8cfff525c5cd96a0c82908"}, "611d1788-3577-444c-a877-98961480fa18": {"doc_hash": "8d814c56ce87d8a73b773d74491a2f4e5d87fd52c52396c200436a391ea95f38"}, "2e797632-b08b-44fd-b688-806b424d8a49": {"doc_hash": "4c06ad9f56e7b561326d59863f54a2019ec6b9dd54dc2354843c43d6658a2a24"}, "6933b04f-451f-429a-b5c3-4888f773e05a": {"doc_hash": "21fdff41289a633fb3a606d9572de258658fed3807708db36c5e593d70603bf1"}, "d11afd8e-a3be-4dbb-b0cd-76d4926b9dd3": {"doc_hash": "3dad1e90ca8f66c9ad168d63c907ea4017e6b4f4c545ec56b49b653bf1e3e964"}, "7fa0b494-f58f-4488-ac8e-4dc3043b6d2b": {"doc_hash": "3abe5a82141e0a08379dabe09edabe523521b889c85528016811b4b5e7c65f56"}, "d0484994-2212-444c-9d91-370f052c5d56": {"doc_hash": "838643fcad2681a214eef6fa6936be73d028f43142a9b872489f1ff5dee86052"}, "f2dec06b-d9b0-49a8-b2c5-e22e3521d50d": {"doc_hash": "da9570e610302a2ea55488931bc89c98febbc0a050f144867419017a88c4e669"}, "fcba4aa7-15f7-4f80-99b6-c6ce8a295f9f": {"doc_hash": "33dcfda4793c9199b56b1e54a6768ba3ab9b5fb76bc241658076d2feb8cff046"}, "cf46471c-f41c-4630-b4d8-d58f38692d65": {"doc_hash": "d93a2e8b92d9f391e740f925d454b6caff9fb4e0628a9ec80b725b0f51713959"}, "0f23af1f-9916-4454-ac7f-37edc8161b2d": {"doc_hash": "b71145044b9cb7813326b53233b3040479fea7c5096eccc9dd2db096bb12a2be"}, "cee3d84b-a893-42a3-a4aa-b8db7ac38666": {"doc_hash": "ccb598dab78078e31714fbc69780874f967bdf25670b252933f24d287d9b9dd7"}, "de17ed8e-a338-468c-98aa-de7d440fef6d": {"doc_hash": "a175ef87a465420375c88d34b670609d42d24b7d259e9a3d51017b5d7d5a26f3"}, "69e99923-bc45-4146-a7ac-cf265829bd97": {"doc_hash": "f0cc291d43444e9a3aff2bcc75f4d880e7bd00a91bc968ab8bcf380c10d02e58"}, "dae1d2cc-8b94-4510-b1c8-04f77893cfb2": {"doc_hash": "a8fa0969455867fdcfdfaafc7bab4489448f85897ae9b94b28a24d975e908ceb"}, "fe23e9f1-52d9-491a-863f-372ee85f6a19": {"doc_hash": "fd418d39905ab729ce2563cd2ec5b067ffaf92999a122aa5d908ae0dbaef7878"}, "38f62aa5-439b-494f-bbe3-76874724a7a6": {"doc_hash": "53f429ce830920960a68e7ba70d4c502cf67e12b21cd811fd81e8795670dce1a"}, "a5268c29-8ad3-41e6-9fdf-6b82250d5486": {"doc_hash": "c722fb54ae6e9de7f8b67e2b354bb437d815b76c87d9d27b07d1d9f7f44d1651"}, "2beb1718-4cfe-4730-9eb9-f8bdee87580d": {"doc_hash": "e759fef7dfbb662eb7bbb18851ad8c84508af02fddcb5ac031f124da67306bf5"}, "6e173971-2ecf-4eb1-b117-6b9cc416a954": {"doc_hash": "f08b801031f6a4fb0c5348b0a90ce837172a60c324fb0c1e59348204eeda810a"}, "b607aef3-1e6e-4289-ac75-dfd558dc34b4": {"doc_hash": "f14f14c2f2667fc683054c78ee6b98f881384dba58dc9fe60ba9dd3533663743"}, "aac6ad79-0a81-44dd-91bb-a33242fab243": {"doc_hash": "293fa64c6f0d32004394e8eff175fd53d0d5de20e91dd153b315d3d8a3acf2d9"}, "b04bc038-7f5b-4d65-bcb1-6715c13b8c91": {"doc_hash": "c457a971573ae123e597c876d62b05f129be1c29c2c2d549e843f6c120ebfe18"}, "a36793cc-78b6-4902-817a-51b6d90b84a0": {"doc_hash": "fd5476451b94d7a24abf2e9f3c649d9164c57eca359746eaff7c44ccb750be52"}, "9758ed6a-8fbf-464e-8e9d-b709dbe4f3af": {"doc_hash": "525045969e10b4b95f944ca99d68085eab3e27b5fde2b54001de541ea0acb294"}, "b5bb56ff-f993-4449-ac09-6ece198678fc": {"doc_hash": "17edc50a29657174aaa8c6a8b9d86b989279e41937b4873fb3f90585e91b7827"}, "d189df59-6ef4-4403-92ac-7010119eb443": {"doc_hash": "e35361e5f9fd90556dc0a5ececf8f3d2ca6eb4f92fe14e89a53453dab0c092a5"}, "a076bffd-ea9e-4d88-819b-36fbe8bfad04": {"doc_hash": "4bc76d72af6404b1edce8ed6744217e08d1e0a6adc3a34b911bc4153814f74c2"}, "de339d77-5d63-4e26-8d56-10c13ffa5d56": {"doc_hash": "6ceeb6f28dc4fdbd2736f0684e9721fe3de903ace8e7e5dadefb6de5b1679cdc"}, "b4c1445b-a8cb-45ad-89b6-83450b257d79": {"doc_hash": "cb8ac6a23193cb2fdbb4851f20164e9a880011b2f170f06338b2c14a7d558826"}, "f0aacc91-81d9-4a58-a944-251ea27fe8c0": {"doc_hash": "f9a0b9bdae1bf5dcca4bc5c0b8f176b1240cc4ee3c0c51eb941887a376c22001"}, "1fe18a65-ef0b-49ec-96d6-0e3b1ab24665": {"doc_hash": "33128567e3eaec018bc6d39276b4b2c8ebf1f9883d8e4172932b7300855ecb70"}, "c8e93190-0346-4f8b-be86-e66fcefb4092": {"doc_hash": "54f6e25f451200dfda5e34750dda092250dd8e00719a0750bafdadaa498a6b95"}, "b03f73d1-85bd-4275-b1d9-375923cd615d": {"doc_hash": "12d1cb8d3ec1f3e813a7462abafe8a2238a8109f517f75c835efa3f6e9fdc91c"}, "129502a6-4bcf-430f-b361-95eb9a97ddde": {"doc_hash": "de243ad6c28a1ec258d01cdcb436df5fe6afc75aa2523e25238b6e12dc6b5f91"}, "47718423-3d3c-4b6f-98e8-60f5eafd4a5c": {"doc_hash": "0baa70dea62225df631a61e30337ad8d693262207361f92defb42c13e9fa1c21"}, "1aa73f39-003a-4c4f-8886-e782ca38de63": {"doc_hash": "8b643f118f8210c1846a2e38dfb27f1a6d6f973b9c143f9c4454720c3cfd7a7a"}, "b2956a09-0be4-4761-b21c-aaf337e213c9": {"doc_hash": "68e608a7c764c0fdbb8da6d51535e7913d120ef2349e0e6971e7b23b767a72c1"}, "d8b78180-b196-4360-b4df-703c76c3ffc6": {"doc_hash": "e78be04f8e1ceb853b6fbdab97f6c3663a01bcf3aa092dddede6a4bbe52df52f"}, "79953aa7-3d8d-494b-bd74-9cd184143299": {"doc_hash": "553df516a89c96dc20b04391823648d6776fe5188820b439ab5e35c2edc2797c"}, "86d60fbf-5e24-4fd5-aeec-46ad78509422": {"doc_hash": "53fb991a8dcd8a9b75c7afa1147c363cae355948b305e67aaa946f73379424f6"}, "ed83c936-f623-46f6-a8f6-e40741c2db37": {"doc_hash": "6288975b6ee4bd1c841ff9fb60ceb433832dad36cdf54a89f44625ecfce0db96"}, "1de98e46-838d-464b-89a6-5ef514bf5db5": {"doc_hash": "aa8667d9449951e875bfb60fb773587ac80976a61269f532f1bbf727103cd466"}, "a1b44000-5750-4ce9-8ffa-4be6eed05285": {"doc_hash": "68edc1d878c3c60f3dba01417ec6467be60d15500d66d3d63d6a3dbcf0ae4d8e"}, "408a2bf2-00bb-4002-a978-6997aeb4f3f9": {"doc_hash": "f0ac0106248c87ffb3a665591b6dcee42549263f4a967eca439d20292c91afaa"}, "340cb617-a242-4eda-b8ad-d601eb6bafe6": {"doc_hash": "217977a68bca578233683b7c414b87c80c236889406c1ca9094b6059bf2f3ea9"}, "4c45c491-13a9-4005-a6ed-ffa4f2018eac": {"doc_hash": "e2b9d627ef2cfa8e950b812b59bd56b6b21099fb32a2ba1873ecb4426a80fa79"}, "3968e46e-1efb-4e96-8849-8bf92f3b6c96": {"doc_hash": "1bebe808462446eb386f40be1f06a53c0efb377b64f651809011c68133793666"}, "6701df11-6331-44f3-bf2f-7755215a1ccb": {"doc_hash": "8dfa262badd22dfce79dcd69129aba89fa30cff21a2ac6f64287813cb0a8b464"}, "39b3974a-36c7-40f3-a3d6-a959ccf7fae2": {"doc_hash": "d4bc2353f9e17ace22accbc31c26a76863ec667982cd9dbf7a549c3b0518ee53"}, "2cb85a72-9586-47fa-a1c9-2413e955fb86": {"doc_hash": "cd19eded39882f1dde82ebcf8009991196dc56641d18d90ad95dfc7539f8a370"}, "ee10ad96-45b1-436e-aac6-59fbf7d3f280": {"doc_hash": "3f6a5912d0368b4a061cc039749dccb2b244040ef2e6f2402d4785f8a5574bc1"}, "4467b060-d6a6-4f99-81dd-baa8a35ce6e4": {"doc_hash": "ca0e6288b2dd65f51c2b01c1e46563a419e161dbbc75c25f260cc83a72e763ef"}, "91dc85b1-470c-4a02-b95e-d093a9f60b78": {"doc_hash": "423ff2d67a40f218157d9e50889e8dfdfaea352c9b66402cd0ccd56ed8f9cbb6"}, "a1b9d042-b426-4729-a483-72c140af999b": {"doc_hash": "8ab29d5d63c336a754686c472d6cb5ecb112faecd0f18db588aca6063f6e8598"}, "d0f0b28b-31f2-43d0-bd8d-0e9e6f5f39f4": {"doc_hash": "a119fb9ed403579eca8789dddedd878695eed3bae0dd5f241436ce061f585976"}, "32e64f03-0e47-456e-a6a3-079afb09c1c9": {"doc_hash": "d237d046e57d5ad1b245de65ab38f2bd345e1c6b97022d63cd0a8b8fc40346bb"}, "49be0348-152e-407c-b516-73ef761984e6": {"doc_hash": "545f0c1293d8cbd8e99ca1428a613096b355ce92f4cb338f47bf8a3f6b729b5b"}, "019d2e5f-2ca7-48d7-a2c3-3ba674ebfd06": {"doc_hash": "bab0d20f7183ec0ba2b11954cd58416997a5926ef96d9abed77fbe54886743ec"}, "b41026ba-5d35-4b89-9e7d-73c497779f0f": {"doc_hash": "92f46ac951ad0b99ff275c33e9b87b70d8dedfa06f51e9c1371aced730c7d17e"}, "42a616ce-b425-4c02-97d2-eecd91a56612": {"doc_hash": "251ef5ee191ae5bfec219eb3324d3ea9a69542aaa05a0f6a48f57a9860953a03"}, "68ac267b-f7bc-4eaf-a8ef-48cd01574e90": {"doc_hash": "e759389860e569aa139eeb1684c4459de4251396e00ac892d9cee30673793ba5"}, "3198c62b-4dde-4d68-b2bc-9843c82c139f": {"doc_hash": "e5e926e2fbb5cd500b3c07123c0fce5eb61b6562fd581ed9385c9031d68e3340"}, "122fc486-8bbf-4e9d-b6db-5961497ec53e": {"doc_hash": "a374f3f2ffb042976d4faded37e27440af5015893e13f01748e7fcaa048eac02"}, "c9e513c6-8f85-4ad3-831c-90cef62742b1": {"doc_hash": "0966129b0bb64c2b40102e14b7c75ed9c31b6d2e0248872c1a7c0549c1a18ef3"}, "1a90d616-e7a4-4fe1-9130-aff6f2eb5c44": {"doc_hash": "140ead90969534c7e7b089df5200d8ae72a2fbef360ec6cc86dd4732d60d7a0e"}, "24bfafb8-6524-456e-bbda-b4dbd04b92b7": {"doc_hash": "99050237d5104c74bb3838e67a43d09f07a81adb80a6854a78194b29d0abc272"}, "465f315f-049e-41cb-bab0-8db4f5e820f9": {"doc_hash": "72189cffd313e0795800bd7ef182bc0bcae3543cad3fd1ae31e663fdb561a140"}, "84f3b64f-6122-4e62-9705-623d8514f787": {"doc_hash": "582a5dae704f885966b5a6deb5bd6995b8568e29c309aee091132abd0fec14ea"}, "677867c9-eea9-4bd3-86f5-ae5204525c90": {"doc_hash": "3aec0b5ef06b2be7e6e4ec8220c353619709b01ea9c6afdd50bfdbb470725daa"}, "a2b4a975-2720-45eb-b279-99a44d2230d3": {"doc_hash": "1a15acbda20af2ae32ba331a4a8f192460dbbf4b854effdadcd840187b6a0f75"}, "0c7b77a4-343e-4635-8037-7f2079b85bf0": {"doc_hash": "7832ed2ee111d05dde28c2db25cbb0fac371053adae1fb299a94ea5038b00530"}}, "docstore/data": {"4635f189-1158-49a1-a556-91c606c167e5": {"__data__": {"text": "# TinyViT: Fast Pretraining Distillation for Small Vision Transformers [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Tiny%20vision%20transformer%20models,%20SOTA%20performance!!&url=https://github.com/microsoft/Cream/tree/main/TinyViT&via=houwen_peng&hashtags=ViT,tiny,efficient)\n\n\n:pushpin: This is an official PyTorch implementation of **[ECCV 2022]** - [TinyViT: Fast Pretraining Distillation for Small Vision Transformers](https://arxiv.org/pdf/2207.10666.pdf).\n\nTinyViT is a new family of **tiny and efficient** vision transformers pretrained on **large-scale** datasets with our proposed **fast distillation framework**. The central idea is to **transfer knowledge** from **large pretrained models** to small ones. The logits of large teacher models are sparsified and stored in disk in advance to **save the memory cost and computation overheads**.\n\n:rocket: TinyViT with **only 21M parameters** achieves **84.8%** top-1 accuracy on ImageNet-1k, and **86.5%** accuracy under 512x512 resolutions.\n\n<div align=\"center\">\n    <img width=\"80%\" alt=\"TinyViT overview\" src=\".figure/framework.png\"/>\n</div>\n\n:sunny: Hiring research interns for neural architecture search, tiny transformer design, model compression projects: houwen.peng@microsoft.com.\n\n## Highlights\n\n<div align=\"center\">\n    <img width=\"80%\" src=\".figure/performance.png\"/>\n</div>\n\n* TinyViT-21M ![](./.figure/distill.png) on IN-22k achieves **84.8%** top-1 accuracy on IN-1k, and **86.5%** accuracy under 512x512 resolutions.\n* TinyViT-21M **trained from scratch on IN-1k** without distillation achieves **83.1** top-1 accuracy, under **4.3 GFLOPs** and **1,571 images/s** throughput on V100 GPU.\n* TinyViT-5M ![](./.figure/distill.png) reaches **80.7%** top-1 accuracy on IN-1k under 3,060 images/s throughput.\n* Save teacher logits **once**, and **reuse** the saved sparse logits to distill **arbitrary students without overhead** of teacher model. It takes **16 GB / 481 GB** storage space for IN-1k (300 epochs) and IN-22k (90 epochs), respectively.\n\n## Features\n1. **Efficient Distillation**. The teacher logits can be saved in parallel and reused for arbitrary student models, to avoid re-forwarding cost of", "doc_id": "4635f189-1158-49a1-a556-91c606c167e5", "embedding": null, "doc_hash": "05722304fac3bd967412a7bcb24767fd5189ac9d10b504606d04ed2bb7efb92c", "extra_info": {"file_path": "TinyViT/README.md", "file_name": "README.md"}, "node_info": {"start": 0, "end": 2253}, "relationships": {"1": "43ec6c26abfd881fbb1ce8e697bb7a78ae0963e3", "3": "f1d90cf4-57ff-4285-a17d-6a9df23be39c"}}, "__type__": "1"}, "f1d90cf4-57ff-4285-a17d-6a9df23be39c": {"__data__": {"text": "(300 epochs) and IN-22k (90 epochs), respectively.\n\n## Features\n1. **Efficient Distillation**. The teacher logits can be saved in parallel and reused for arbitrary student models, to avoid re-forwarding cost of the large teacher model.\n\n2. **Reproducibility**. We provide the hyper-parameters of [IN-1k training](./configs/1k), [IN-22k pre-training with distillation](./configs/22k_distill), [IN-22kto1k fine-tuning](./configs/22kto1k), and [higher resolution fine-tuning](./configs/higher_resolution). In addition, all training logs are public (in Model Zoo).\n\n3. **Ease of Use**. One file to build a TinyViT model.\nThe file [`models/tiny_vit.py`](./models/tiny_vit.py) defines TinyViT model family.\n    ```python\n    from tiny_vit import tiny_vit_21m_224\n    model = tiny_vit_21m_224(pretrained=True)\n    output = model(image)\n    ```\n\n4. **Extensibility**. Add custom dataset, student and teacher models with no need to modify your code.\nThe class [`DatasetWrapper`](./data/build.py#L74) wraps the general dataset to support saving and loading sparse logits. It only need the logits of models for knowledge distillation.\n\n5. **Public teacher model**. We provide CLIP-ViT-Large/16-22k, a powerful teacher model on pretraining distillation (Acc@1 85.894 Acc@5 97.566 on IN-1k). We finetuned CLIP-ViT-Large/16 released by OpenAI on IN-22k.\n\n6. **Online Logging**. Support [wandb](https://wandb.ai) for checking the results anytime anywhere.\n\n## Model Zoo\n\nModel                                      | Pretrain | Input | Acc@1 | Acc@5 | #Params | MACs | FPS  | 22k Model | 1k Model\n:-----------------------------------------:|:---------|:-----:|:-----:|:-----:|:-------:|:----:|:----:|:---------:|:--------:\nTinyViT-5M ![](./.figure/distill.png)       | IN-22k   |224x224| 80.7  | 95.6  | 5.4M    | 1.3G |", "doc_id": "f1d90cf4-57ff-4285-a17d-6a9df23be39c", "embedding": null, "doc_hash": "d0540d28095305e0b6f91636b912dbb0cabf63849e13c10346d875db013be47c", "extra_info": {"file_path": "TinyViT/README.md", "file_name": "README.md"}, "node_info": {"start": 2072, "end": 3876}, "relationships": {"1": "43ec6c26abfd881fbb1ce8e697bb7a78ae0963e3", "2": "4635f189-1158-49a1-a556-91c606c167e5", "3": "ed1617b5-66d6-4d3a-ba26-7be2b4ae5255"}}, "__type__": "1"}, "ed1617b5-66d6-4d3a-ba26-7be2b4ae5255": {"__data__": {"text": "![](./.figure/distill.png)       | IN-22k   |224x224| 80.7  | 95.6  | 5.4M    | 1.3G | 3,060|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22k_distill.pth)/[config](./configs/22k_distill/tiny_vit_5m_22k_distill.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22k_distill.log)|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22kto1k_distill.pth)/[config](./configs/22kto1k/tiny_vit_5m_22kto1k.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_22kto1k_distill.log)\nTinyViT-11M ![](./.figure/distill.png)      | IN-22k   |224x224| 83.2  | 96.5  | 11M     | 2.0G | 2,468|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22k_distill.pth)/[config](./configs/22k_distill/tiny_vit_11m_22k_distill.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22k_distill.log)|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22kto1k_distill.pth)/[config](./configs/22kto1k/tiny_vit_11m_22kto1k.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_22kto1k_distill.log)\nTinyViT-21M ![](./.figure/distill.png)      | IN-22k   |224x224| 84.8  | 97.3  | 21M     | 4.3G |", "doc_id": "ed1617b5-66d6-4d3a-ba26-7be2b4ae5255", "embedding": null, "doc_hash": "e176bef703f895f1217b6ea19fff15a35ac114799f747159dfba3b9697cddefe", "extra_info": {"file_path": "TinyViT/README.md", "file_name": "README.md"}, "node_info": {"start": 3997, "end": 5401}, "relationships": {"1": "43ec6c26abfd881fbb1ce8e697bb7a78ae0963e3", "2": "f1d90cf4-57ff-4285-a17d-6a9df23be39c", "3": "4284f696-ebad-47f2-b396-b2f747cf3a26"}}, "__type__": "1"}, "4284f696-ebad-47f2-b396-b2f747cf3a26": {"__data__": {"text": "![](./.figure/distill.png)      | IN-22k   |224x224| 84.8  | 97.3  | 21M     | 4.3G | 1,571|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22k_distill.pth)/[config](./configs/22k_distill/tiny_vit_21m_22k_distill.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22k_distill.log)|[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_distill.pth)/[config](./configs/22kto1k/tiny_vit_21m_22kto1k.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_distill.log)\nTinyViT-21M-384 ![](./.figure/distill.png)  | IN-22k   |384x384| 86.2  | 97.8  | 21M     | 13.8G| 394  | - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_384_distill.pth)/[config](./configs/higher_resolution/tiny_vit_21m_224to384.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_384_distill.log)\nTinyViT-21M-512 ![](./.figure/distill.png)  | IN-22k   |512x512| 86.5  | 97.9  | 21M     | 27.0G| 167  | - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_512_distill.pth)/[config](./configs/higher_resolution/tiny_vit_21m_384to512.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_22kto1k_512_distill.log)\nTinyViT-5M                                 | IN-1k    |224x224| 79.1", "doc_id": "4284f696-ebad-47f2-b396-b2f747cf3a26", "embedding": null, "doc_hash": "9e6f8ab686effa800497c1fcf50e630bb1e7d352fc4f0b52dc9c8d6d33a5fe9d", "extra_info": {"file_path": "TinyViT/README.md", "file_name": "README.md"}, "node_info": {"start": 5403, "end": 6933}, "relationships": {"1": "43ec6c26abfd881fbb1ce8e697bb7a78ae0963e3", "2": "ed1617b5-66d6-4d3a-ba26-7be2b4ae5255", "3": "c390d2d1-e9e8-4991-af7e-f828bdd04a2f"}}, "__type__": "1"}, "c390d2d1-e9e8-4991-af7e-f828bdd04a2f": {"__data__": {"text": "                                | IN-1k    |224x224| 79.1  | 94.8  | 5.4M    | 1.3G | 3,060| - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_1k.pth)/[config](./configs/1k/tiny_vit_5m.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_5m_1k.log)\nTinyViT-11M                                | IN-1k    |224x224| 81.5  | 95.8  | 11M     | 2.0G | 2,468| - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_1k.pth)/[config](./configs/1k/tiny_vit_11m.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_11m_1k.log)\nTinyViT-21M                                | IN-1k    |224x224| 83.1  | 96.5  | 21M     | 4.3G | 1,571| - |[link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_1k.pth)/[config](./configs/1k/tiny_vit_21m.yaml)/[log](https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/tiny_vit_21m_1k.log)\n\nImageNet-22k (IN-22k) is the same as ImageNet-21k (IN-21k), where the number of classes is 21,841.\n\nThe models with ![](./.figure/distill.png) are pretrained on ImageNet-22k with the distillation of CLIP-ViT-L/14-22k, then finetuned on ImageNet-1k.\n\nWe finetune the 1k models on IN-1k to higher resolution progressively (224 -> 384 -> 512) [[detail]](./docs/TRAINING.md), without any IN-1k knowledge distillation.\n\n## Getting Started\n:beginner: Here is the setup tutorial and evaluation scripts.\n\n### Install dependencies and prepare datasets\n-", "doc_id": "c390d2d1-e9e8-4991-af7e-f828bdd04a2f", "embedding": null, "doc_hash": "ee7d8fb4b288423c13b7301617c498054824a5ac58b0815f2624e6856db35bf2", "extra_info": {"file_path": "TinyViT/README.md", "file_name": "README.md"}, "node_info": {"start": 6976, "end": 8548}, "relationships": {"1": "43ec6c26abfd881fbb1ce8e697bb7a78ae0963e3", "2": "4284f696-ebad-47f2-b396-b2f747cf3a26", "3": "22f3ec44-1a3c-4a9e-a417-2162c119ee49"}}, "__type__": "1"}, "22f3ec44-1a3c-4a9e-a417-2162c119ee49": {"__data__": {"text": "384 -> 512) [[detail]](./docs/TRAINING.md), without any IN-1k knowledge distillation.\n\n## Getting Started\n:beginner: Here is the setup tutorial and evaluation scripts.\n\n### Install dependencies and prepare datasets\n- [Preparation](./docs/PREPARATION.md)\n\n### Evaluate it !\n- [Evaluation](./docs/EVALUATION.md)\n\n## Pretrain a TinyViT model on ImageNet\n:beginner: For the proposed fast pretraining distillation, we need to **save teacher sparse logits** firstly, then **pretrain a model**.\n\n- [How to save teacher sparse logits?](./docs/SAVE_TEACHER_LOGITS.md)\n- [Let's train a TinyViT model](./docs/TRAINING.md)\n\n## Citation\n\nIf this repo is helpful for you, please consider to cite it. :mega: Thank you! :)\n\n```bibtex\n@InProceedings{tiny_vit,\n  title={TinyViT: Fast Pretraining Distillation for Small Vision Transformers},\n  author={Wu, Kan and Zhang, Jinnian and Peng, Houwen and Liu, Mengchen and Xiao, Bin and Fu, Jianlong and Yuan, Lu},\n  booktitle={European conference on computer vision (ECCV)},\n  year={2022}\n}\n```\n\n## Acknowledge\n\nOur code is based on [Swin Transformer](https://github.com/microsoft/swin-transformer), [LeViT](https://github.com/facebookresearch/LeViT), [pytorch-image-models](https://github.com/rwightman/pytorch-image-models), [CLIP](https://github.com/openai/CLIP) and [PyTorch](https://github.com/pytorch/pytorch). Thank contributors for their awesome contribution!\n\n\n## License\n\n- [License](./LICENSE)\n", "doc_id": "22f3ec44-1a3c-4a9e-a417-2162c119ee49", "embedding": null, "doc_hash": "0a79629b7200db01135bde170eeb26de72b5933066e763c49acd4545926798d4", "extra_info": {"file_path": "TinyViT/README.md", "file_name": "README.md"}, "node_info": {"start": 8351, "end": 9783}, "relationships": {"1": "43ec6c26abfd881fbb1ce8e697bb7a78ae0963e3", "2": "c390d2d1-e9e8-4991-af7e-f828bdd04a2f"}}, "__type__": "1"}, "2e3bc64a-325f-4725-9f92-a4e46b56d60b": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT Config\n# Copyright (c) 2022 Microsoft\n# Based on the code: Swin Transformer\n#   (https://github.com/microsoft/swin-transformer)\n# Adapted for TinyViT\n# --------------------------------------------------------\n\nimport os\nimport yaml\nfrom yacs.config import CfgNode as CN\n\n_C = CN()\n\n# Base config files\n_C.BASE = ['']\n\n# -----------------------------------------------------------------------------\n# Data settings\n# -----------------------------------------------------------------------------\n_C.DATA = CN()\n# Batch size for a single GPU, could be overwritten by command line argument\n_C.DATA.BATCH_SIZE = 128\n# Path to dataset, could be overwritten by command line argument\n_C.DATA.DATA_PATH = ''\n# Dataset name\n_C.DATA.DATASET = 'imagenet'\n# Dataset mean/std type\n_C.DATA.MEAN_AND_STD_TYPE = \"default\"\n# Input image size\n_C.DATA.IMG_SIZE = 224\n# Interpolation to resize image (random, bilinear, bicubic)\n_C.DATA.INTERPOLATION = 'bicubic'\n# Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\n_C.DATA.PIN_MEMORY = True\n# Number of data loading threads\n_C.DATA.NUM_WORKERS = 8\n# Data image filename format\n_C.DATA.FNAME_FORMAT = '{}.jpeg'\n# Data debug, when debug is True, only use few images\n_C.DATA.DEBUG = False\n\n\n# -----------------------------------------------------------------------------\n# Model settings\n# -----------------------------------------------------------------------------\n_C.MODEL = CN()\n# Model type\n_C.MODEL.TYPE = 'tiny_vit'\n# Model name\n_C.MODEL.NAME = 'tiny_vit'\n# Pretrained weight from checkpoint, could be imagenet22k pretrained weight\n# could be overwritten by command line argument\n_C.MODEL.PRETRAINED = ''\n# Checkpoint to resume, could be overwritten by command line argument\n_C.MODEL.RESUME = ''\n# Number of classes, overwritten in data preparation\n_C.MODEL.NUM_CLASSES = 1000\n# Dropout rate\n_C.MODEL.DROP_RATE = 0.0\n# Drop path rate\n_C.MODEL.DROP_PATH_RATE = 0.1\n# Label Smoothing\n_C.MODEL.LABEL_SMOOTHING = 0.1\n\n# TinyViT Model\n_C.MODEL.TINY_VIT = CN()\n_C.MODEL.TINY_VIT.IN_CHANS = 3\n_C.MODEL.TINY_VIT.DEPTHS = [2, 2, 6, 2]\n_C.MODEL.TINY_VIT.NUM_HEADS = [3, 6, 12, 18]\n_C.MODEL.TINY_VIT.WINDOW_SIZES = [7, 7, 14,", "doc_id": "2e3bc64a-325f-4725-9f92-a4e46b56d60b", "embedding": null, "doc_hash": "44b4cbd335da28533f5242ba5ac89d2fd5cc88b9a9fb6be16d484042eb856873", "extra_info": {"file_path": "TinyViT/config.py", "file_name": "config.py"}, "node_info": {"start": 0, "end": 2243}, "relationships": {"1": "c852740c01eb74de22419b87d0b7fc94fc57a994", "3": "2a3cfe7c-439a-40da-b795-d986680faec8"}}, "__type__": "1"}, "2a3cfe7c-439a-40da-b795-d986680faec8": {"__data__": {"text": "2, 6, 2]\n_C.MODEL.TINY_VIT.NUM_HEADS = [3, 6, 12, 18]\n_C.MODEL.TINY_VIT.WINDOW_SIZES = [7, 7, 14, 7]\n_C.MODEL.TINY_VIT.EMBED_DIMS = [96, 192, 384, 576]\n_C.MODEL.TINY_VIT.MLP_RATIO = 4.\n_C.MODEL.TINY_VIT.MBCONV_EXPAND_RATIO = 4.0\n_C.MODEL.TINY_VIT.LOCAL_CONV_SIZE = 3\n\n# DISTILL\n_C.DISTILL = CN()\n_C.DISTILL.ENABLED = False\n_C.DISTILL.TEACHER_LOGITS_PATH = ''\n_C.DISTILL.SAVE_TEACHER_LOGITS = False\n_C.DISTILL.LOGITS_TOPK = 100\n\n# -----------------------------------------------------------------------------\n# Training settings\n# -----------------------------------------------------------------------------\n_C.TRAIN = CN()\n_C.TRAIN.START_EPOCH = 0\n_C.TRAIN.EPOCHS = 300\n_C.TRAIN.WARMUP_EPOCHS = 20\n_C.TRAIN.WEIGHT_DECAY = 0.05\n_C.TRAIN.BASE_LR = 5e-4\n_C.TRAIN.WARMUP_LR = 5e-7\n_C.TRAIN.MIN_LR = 5e-6\n# Clip gradient norm\n_C.TRAIN.CLIP_GRAD = 5.0\n# Auto resume from latest checkpoint\n_C.TRAIN.AUTO_RESUME = True\n# Gradient accumulation steps\n# could be overwritten by command line argument\n_C.TRAIN.ACCUMULATION_STEPS = 1\n# Whether to use gradient checkpointing to save memory\n# could be overwritten by command line argument\n_C.TRAIN.USE_CHECKPOINT = False\n# train learning rate decay\n_C.TRAIN.LAYER_LR_DECAY = 1.0\n# batch norm is in evaluation mode when training\n_C.TRAIN.EVAL_BN_WHEN_TRAINING = False\n\n# LR scheduler\n_C.TRAIN.LR_SCHEDULER = CN()\n_C.TRAIN.LR_SCHEDULER.NAME = 'cosine'\n# Epoch interval to decay LR, used in StepLRScheduler\n_C.TRAIN.LR_SCHEDULER.DECAY_EPOCHS = 30\n# LR decay rate, used in StepLRScheduler\n_C.TRAIN.LR_SCHEDULER.DECAY_RATE = 0.1\n\n# Optimizer\n_C.TRAIN.OPTIMIZER = CN()\n_C.TRAIN.OPTIMIZER.NAME = 'adamw'\n# Optimizer", "doc_id": "2a3cfe7c-439a-40da-b795-d986680faec8", "embedding": null, "doc_hash": "83716859bdfcdcadaf69613a72941165c41e7d0841f871b64a37fa79976eb0f9", "extra_info": {"file_path": "TinyViT/config.py", "file_name": "config.py"}, "node_info": {"start": 2158, "end": 3802}, "relationships": {"1": "c852740c01eb74de22419b87d0b7fc94fc57a994", "2": "2e3bc64a-325f-4725-9f92-a4e46b56d60b", "3": "d8a08c82-1bef-4e97-bbaa-cfb9dfe43ce9"}}, "__type__": "1"}, "d8a08c82-1bef-4e97-bbaa-cfb9dfe43ce9": {"__data__": {"text": "= 0.1\n\n# Optimizer\n_C.TRAIN.OPTIMIZER = CN()\n_C.TRAIN.OPTIMIZER.NAME = 'adamw'\n# Optimizer Epsilon\n_C.TRAIN.OPTIMIZER.EPS = 1e-8\n# Optimizer Betas\n_C.TRAIN.OPTIMIZER.BETAS = (0.9, 0.999)\n# SGD momentum\n_C.TRAIN.OPTIMIZER.MOMENTUM = 0.9\n\n# -----------------------------------------------------------------------------\n# Augmentation settings\n# -----------------------------------------------------------------------------\n_C.AUG = CN()\n# Color jitter factor\n_C.AUG.COLOR_JITTER = 0.4\n# Use AutoAugment policy. \"v0\" or \"original\"\n_C.AUG.AUTO_AUGMENT = 'rand-m9-mstd0.5-inc1'\n# Random erase prob\n_C.AUG.REPROB = 0.25\n# Random erase mode\n_C.AUG.REMODE = 'pixel'\n# Random erase count\n_C.AUG.RECOUNT = 1\n# Mixup alpha, mixup enabled if > 0\n_C.AUG.MIXUP = 0.8\n# Cutmix alpha, cutmix enabled if > 0\n_C.AUG.CUTMIX = 1.0\n# Cutmix min/max ratio, overrides alpha and enables cutmix if set\n_C.AUG.CUTMIX_MINMAX = None\n# Probability of performing mixup or cutmix when either/both is enabled\n_C.AUG.MIXUP_PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled\n_C.AUG.MIXUP_SWITCH_PROB = 0.5\n# How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"\n_C.AUG.MIXUP_MODE = 'batch'\n\n# -----------------------------------------------------------------------------\n# Testing settings\n# -----------------------------------------------------------------------------\n_C.TEST = CN()\n# Whether to use center crop when testing\n_C.TEST.CROP = True\n\n# -----------------------------------------------------------------------------\n# Misc\n# -----------------------------------------------------------------------------\n\n# Enable Pytorch automatic mixed precision (amp).\n_C.AMP_ENABLE = True\n# Path to output folder, overwritten by command line argument\n_C.OUTPUT = ''\n# Tag of experiment, overwritten by command line argument\n_C.TAG = 'default'\n# Frequency to save checkpoint\n_C.SAVE_FREQ = 1\n# Frequency to logging info\n_C.PRINT_FREQ = 10\n# Fixed random seed\n_C.SEED = 0\n# Perform evaluation only, overwritten by command line argument\n_C.EVAL_MODE = False\n# Test throughput only, overwritten by command line argument\n_C.THROUGHPUT_MODE = False\n# local rank for DistributedDataParallel, given by command line argument\n_C.LOCAL_RANK =", "doc_id": "d8a08c82-1bef-4e97-bbaa-cfb9dfe43ce9", "embedding": null, "doc_hash": "e322e193cb94a93e4c18437e8ba7d214cd824159ea6dd111434bca517777d4af", "extra_info": {"file_path": "TinyViT/config.py", "file_name": "config.py"}, "node_info": {"start": 3806, "end": 6044}, "relationships": {"1": "c852740c01eb74de22419b87d0b7fc94fc57a994", "2": "2a3cfe7c-439a-40da-b795-d986680faec8", "3": "2fbb5ed3-2e8c-49b7-9c85-3b2ff2976817"}}, "__type__": "1"}, "2fbb5ed3-2e8c-49b7-9c85-3b2ff2976817": {"__data__": {"text": "argument\n_C.EVAL_MODE = False\n# Test throughput only, overwritten by command line argument\n_C.THROUGHPUT_MODE = False\n# local rank for DistributedDataParallel, given by command line argument\n_C.LOCAL_RANK = 0\n\n\ndef _update_config_from_file(config, cfg_file):\n    config.defrost()\n    with open(cfg_file, 'r') as f:\n        yaml_cfg = yaml.load(f, Loader=yaml.FullLoader)\n\n    for cfg in yaml_cfg.setdefault('BASE', ['']):\n        if cfg:\n            _update_config_from_file(\n                config, os.path.join(os.path.dirname(cfg_file), cfg)\n            )\n    print('=> merge config from {}'.format(cfg_file))\n    config.merge_from_file(cfg_file)\n    config.freeze()\n\n\ndef update_config(config, args):\n    _update_config_from_file(config, args.cfg)\n\n    config.defrost()\n    if args.opts:\n        config.merge_from_list(args.opts)\n\n    # merge from specific arguments\n    if args.batch_size:\n        config.DATA.BATCH_SIZE = args.batch_size\n    if args.data_path:\n        config.DATA.DATA_PATH = args.data_path\n    if args.pretrained:\n        config.MODEL.PRETRAINED = args.pretrained\n    if args.resume:\n        config.MODEL.RESUME = args.resume\n    if args.accumulation_steps:\n        config.TRAIN.ACCUMULATION_STEPS = args.accumulation_steps\n    if args.use_checkpoint:\n        config.TRAIN.USE_CHECKPOINT = True\n    if args.disable_amp or args.only_cpu:\n        config.AMP_ENABLE = False\n    if args.output:\n        config.OUTPUT = args.output\n    if args.tag:\n        config.TAG = args.tag\n    if args.eval:\n        config.EVAL_MODE = True\n    if args.throughput:\n        config.THROUGHPUT_MODE = True\n\n    # set local rank for distributed training\n    if args.local_rank is None and 'LOCAL_RANK' in", "doc_id": "2fbb5ed3-2e8c-49b7-9c85-3b2ff2976817", "embedding": null, "doc_hash": "36568591ee35d1ee410178468d2938c592d99d5ff00df2ac049a331fec96ac01", "extra_info": {"file_path": "TinyViT/config.py", "file_name": "config.py"}, "node_info": {"start": 5944, "end": 7651}, "relationships": {"1": "c852740c01eb74de22419b87d0b7fc94fc57a994", "2": "d8a08c82-1bef-4e97-bbaa-cfb9dfe43ce9", "3": "e6da9cda-be5c-4047-86f4-3de8c466ffaa"}}, "__type__": "1"}, "e6da9cda-be5c-4047-86f4-3de8c466ffaa": {"__data__": {"text": "True\n    if args.throughput:\n        config.THROUGHPUT_MODE = True\n\n    # set local rank for distributed training\n    if args.local_rank is None and 'LOCAL_RANK' in os.environ:\n        args.local_rank = int(os.environ['LOCAL_RANK'])\n    # set local rank for distributed training\n    config.LOCAL_RANK = args.local_rank\n\n    # output folder\n    config.OUTPUT = os.path.join(config.OUTPUT, config.MODEL.NAME, config.TAG)\n\n    config.freeze()\n\n\ndef get_config(args):\n    \"\"\"Get a yacs CfgNode object with default values.\"\"\"\n    # Return a clone so that the defaults will not be altered\n    # This is for the \"local variable\" use pattern\n    config = _C.clone()\n    update_config(config, args)\n\n    return config\n", "doc_id": "e6da9cda-be5c-4047-86f4-3de8c466ffaa", "embedding": null, "doc_hash": "e816a09b2d09e320cb606e91229e57734acfe9565aeac62115db4dd6466e7b2c", "extra_info": {"file_path": "TinyViT/config.py", "file_name": "config.py"}, "node_info": {"start": 7671, "end": 8380}, "relationships": {"1": "c852740c01eb74de22419b87d0b7fc94fc57a994", "2": "2fbb5ed3-2e8c-49b7-9c85-3b2ff2976817"}}, "__type__": "1"}, "59d1b062-86c9-4a52-a8db-6c9065d042d8": {"__data__": {"text": "from .build import build_loader\n", "doc_id": "59d1b062-86c9-4a52-a8db-6c9065d042d8", "embedding": null, "doc_hash": "edd7d8eb303691e0b40704618f054500df6738b8563f036fd68d69b09bc846ae", "extra_info": {"file_path": "TinyViT/data/__init__.py", "file_name": "__init__.py"}, "node_info": {"start": 0, "end": 32}, "relationships": {"1": "dd06953cd65f7ba27366e9c04dd9ec710c99174c"}}, "__type__": "1"}, "4cecc9a2-81c0-4320-a46f-08ae413658c0": {"__data__": {"text": "# Image Augmentation for TinyViT\n\nThe code is based on [timm.data](https://github.com/rwightman/pytorch-image-models/tree/master/timm/data) of [pytorch-image-models](https://github.com/rwightman/pytorch-image-models) written by [Ross Wightman](https://github.com/rwightman) and the contributors. Thanks a lot!\n\nWe adapt it for TinyViT.\n\nApache License\n\n## Code Structure\n\nFile                                         | Description\n---------------------------------------------|--------------------------\n[`aug_random.py`](./aug_random.py)           | unify all random values of augmentation with a random generator\n[`dataset_wrapper.py`](./dataset_wrapper.py) | a dataset wrapper for saving logits\n[`manager.py`](./manager.py)                 | The writter and reader for logits files\n", "doc_id": "4cecc9a2-81c0-4320-a46f-08ae413658c0", "embedding": null, "doc_hash": "7ee972d01204cff6eb8a3c19890eded6764f70b3e714e5dcb366ac3d2d11bfed", "extra_info": {"file_path": "TinyViT/data/augmentation/README.md", "file_name": "README.md"}, "node_info": {"start": 0, "end": 785}, "relationships": {"1": "57766cb8a52b49ee6997575754b70edcfea2c920"}}, "__type__": "1"}, "9e5848cd-2ea5-41c3-b957-42858921f610": {"__data__": {"text": "from .auto_augment import RandAugment, AutoAugment, rand_augment_ops, auto_augment_policy,\\\n    rand_augment_transform, auto_augment_transform\nfrom .config import resolve_data_config\nfrom .constants import *\nfrom .dataset import ImageDataset, IterableImageDataset, AugMixDataset\nfrom .dataset_factory import create_dataset\nfrom .loader import create_loader\nfrom .mixup import Mixup, FastCollateMixup\nfrom .parsers import create_parser\nfrom .real_labels import RealLabelsImagenet\nfrom .transforms import *\nfrom .transforms_factory import create_transform", "doc_id": "9e5848cd-2ea5-41c3-b957-42858921f610", "embedding": null, "doc_hash": "bdadf076048694d832dc022c7e45caf52d4798c3979e9cdefb16d367155f6f6e", "extra_info": {"file_path": "TinyViT/data/augmentation/__init__.py", "file_name": "__init__.py"}, "node_info": {"start": 0, "end": 553}, "relationships": {"1": "7d3cb2b4d7e823aabb1d55781149579eeb94b024"}}, "__type__": "1"}, "7f9d39e6-2488-4db0-b644-4a22dfd0f0f6": {"__data__": {"text": "import numpy as np\nfrom numpy.random import Generator, PCG64\n\nRNG = None\n\n\nclass AugRandomContext:\n    def __init__(self, seed):\n        self.seed = seed\n\n    def __enter__(self):\n        global RNG\n        assert RNG is None\n        RNG = Generator(PCG64(seed=self.seed))\n\n    def __exit__(self, *_):\n        global RNG\n        RNG = None\n\n\nclass random:\n    # inline: random module\n    @staticmethod\n    def random():\n        return RNG.random()\n\n    @staticmethod\n    def uniform(a, b):\n        return random.random() * (b - a) + a\n\n    @staticmethod\n    def randint(a, b):\n        # [low, high]\n        return min(int(random.random() * (b - a + 1)) + a, b)\n\n    @staticmethod\n    def gauss(mu, sigma):\n        return RNG.normal(mu, sigma)\n\n\nclass np_random:\n    # numpy.random\n    @staticmethod\n    def choice(a, size, *args, **kwargs):\n        return RNG.choice(a, size, *args, **kwargs)\n\n    @staticmethod\n    def randint(low, high, size=None, dtype=int):\n        # [low, high)\n        if size is None:\n            return dtype(random.randint(low, high - 1))\n        out = [random.randint(low, high - 1) for _ in range(size)]\n        return np.array(out, dtype=dtype)\n\n    @staticmethod\n    def rand(*shape):\n        return RNG.random(shape)\n\n    @staticmethod\n    def beta(a, b, size=None):\n        return RNG.beta(a, b, size=size)\n\n\nif __name__ == '__main__':\n    for _ in range(2):\n        with AugRandomContext(seed=0):\n            print(np_random.randint(-100, 100, size=10))\n        with AugRandomContext(seed=1):\n            print(np_random.randint(-100, 100, size=10))\n", "doc_id": "7f9d39e6-2488-4db0-b644-4a22dfd0f0f6", "embedding": null, "doc_hash": "a3b5904d84b772312ba9c477e7c67b1ebb233d6f77886b7569dac982a830bca5", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_random.py", "file_name": "aug_random.py"}, "node_info": {"start": 0, "end": 1583}, "relationships": {"1": "056e3a9c4e6e31c8a3672ca02c5bbcea6c126ce3"}}, "__type__": "1"}, "16aede74-9972-434e-bc64-c6a537a14eac": {"__data__": {"text": "from __future__ import division\nimport torch\nimport math\nimport sys\nfrom .aug_random import random\nfrom PIL import Image\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\nimport numpy as np\nimport numbers\nimport types\nimport collections\nimport warnings\n\nfrom torchvision.transforms import functional as F\n\nif sys.version_info < (3, 3):\n    Sequence = collections.Sequence\n    Iterable = collections.Iterable\nelse:\n    Sequence = collections.abc.Sequence\n    Iterable = collections.abc.Iterable\n\n\n__all__ = [\"Compose\", \"ToTensor\", \"ToPILImage\", \"Normalize\", \"Resize\", \"Scale\", \"CenterCrop\", \"Pad\",\n           \"Lambda\", \"RandomApply\", \"RandomChoice\", \"RandomOrder\", \"RandomCrop\", \"RandomHorizontalFlip\",\n           \"RandomVerticalFlip\", \"RandomResizedCrop\", \"RandomSizedCrop\", \"FiveCrop\", \"TenCrop\", \"LinearTransformation\",\n           \"ColorJitter\", \"RandomRotation\", \"RandomAffine\", \"Grayscale\", \"RandomGrayscale\"]\n\n_pil_interpolation_to_str = {\n    Image.NEAREST: 'PIL.Image.NEAREST',\n    Image.BILINEAR: 'PIL.Image.BILINEAR',\n    Image.BICUBIC: 'PIL.Image.BICUBIC',\n    Image.LANCZOS: 'PIL.Image.LANCZOS',\n    Image.HAMMING: 'PIL.Image.HAMMING',\n    Image.BOX: 'PIL.Image.BOX',\n}\n\n\nclass Compose(object):\n    \"\"\"Composes several transforms together.\n\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n\n    Example:\n        >>> transforms.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    \"\"\"\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        for t in self.transforms:\n   ", "doc_id": "16aede74-9972-434e-bc64-c6a537a14eac", "embedding": null, "doc_hash": "4f00ee343ce5da1e830a1040b77e9e7f857b7b8e3b31cea6d941b42cd0f1de6a", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 0, "end": 1810}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "3": "ae9d0196-05dc-42d9-8fdd-d0e9b41504ba"}}, "__type__": "1"}, "ae9d0196-05dc-42d9-8fdd-d0e9b41504ba": {"__data__": {"text": "      return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        for t in self.transforms:\n            format_string += '\\n'\n            format_string += '    {0}'.format(t)\n        format_string += '\\n)'\n        return format_string\n\n\nclass ToTensor(object):\n    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n\n    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n    if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\n    or if the numpy.ndarray has dtype = np.uint8\n\n    In the other cases, tensors are returned without scaling.\n    \"\"\"\n\n    def __call__(self, pic):\n        \"\"\"\n        Args:\n            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n\n        Returns:\n            Tensor: Converted image.\n        \"\"\"\n        return F.to_tensor(pic)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n\nclass ToPILImage(object):\n    \"\"\"Convert a tensor or an ndarray to PIL Image.\n\n    Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape\n    H x W x C to a PIL Image while preserving the value range.\n\n    Args:\n        mode (`PIL.Image mode`_): color space and pixel depth of input data (optional).\n            If ``mode`` is ``None`` (default) there are some assumptions made about the input data:\n             - If the input has 4 channels, the ``mode`` is assumed to be ``RGBA``.\n             - If the input has 3 channels, the ``mode`` is assumed to be ``RGB``.\n             - If the input has 2 channels, the ``mode`` is assumed to be ``LA``.\n", "doc_id": "ae9d0196-05dc-42d9-8fdd-d0e9b41504ba", "embedding": null, "doc_hash": "ecf2ad95b232e9fdaac4988f63236fa1d4e5bcef054ad02949abb91a74ccc3a0", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 1716, "end": 3413}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "16aede74-9972-434e-bc64-c6a537a14eac", "3": "b8846b64-9555-497c-abfd-66f438a00b11"}}, "__type__": "1"}, "b8846b64-9555-497c-abfd-66f438a00b11": {"__data__": {"text": "      - If the input has 3 channels, the ``mode`` is assumed to be ``RGB``.\n             - If the input has 2 channels, the ``mode`` is assumed to be ``LA``.\n             - If the input has 1 channel, the ``mode`` is determined by the data type (i.e ``int``, ``float``,\n              ``short``).\n\n    .. _PIL.Image mode: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes\n    \"\"\"\n    def __init__(self, mode=None):\n        self.mode = mode\n\n    def __call__(self, pic):\n        \"\"\"\n        Args:\n            pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\n\n        Returns:\n            PIL Image: Image converted to PIL Image.\n\n        \"\"\"\n        return F.to_pil_image(pic, self.mode)\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        if self.mode is not None:\n            format_string += 'mode={0}'.format(self.mode)\n        format_string += ')'\n        return format_string\n\n\nclass Normalize(object):\n    \"\"\"Normalize a tensor image with mean and standard deviation.\n    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``torch.*Tensor`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n\n    .. note::\n        This transform acts out of place, i.e., it does not mutates the input tensor.\n\n    Args:\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    \"\"\"\n\n    def __init__(self, mean, std, inplace=False):\n        self.mean = mean\n        self.std = std\n        self.inplace = inplace\n\n    def __call__(self, tensor):\n        \"\"\"\n     ", "doc_id": "b8846b64-9555-497c-abfd-66f438a00b11", "embedding": null, "doc_hash": "0514c7946068c627da828f1fcd9d6ed15c022b64938f5869871a907aef160498", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 3396, "end": 5116}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "ae9d0196-05dc-42d9-8fdd-d0e9b41504ba", "3": "3d67f6a6-90a7-41fa-aaad-864df1de4ce7"}}, "__type__": "1"}, "3d67f6a6-90a7-41fa-aaad-864df1de4ce7": {"__data__": {"text": "  self.mean = mean\n        self.std = std\n        self.inplace = inplace\n\n    def __call__(self, tensor):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n\n        Returns:\n            Tensor: Normalized Tensor image.\n        \"\"\"\n        return F.normalize(tensor, self.mean, self.std, self.inplace)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\n\nclass Resize(object):\n    \"\"\"Resize the input PIL Image to the given size.\n\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    \"\"\"\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        assert isinstance(size, int) or (isinstance(size, Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be scaled.\n\n        Returns:\n            PIL Image: Rescaled image.\n        \"\"\"\n        return F.resize(img, self.size, self.interpolation)\n\n    def __repr__(self):\n        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size,", "doc_id": "3d67f6a6-90a7-41fa-aaad-864df1de4ce7", "embedding": null, "doc_hash": "8956f3540386c71f08c599e562ddd8934962089ce3a2f450d4ee3f83b88db2a3", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 5150, "end": 6798}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "b8846b64-9555-497c-abfd-66f438a00b11", "3": "e547455a-37f3-4146-904f-ea3596ea15a4"}}, "__type__": "1"}, "e547455a-37f3-4146-904f-ea3596ea15a4": {"__data__": {"text": "  interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)\n\n\nclass Scale(Resize):\n    \"\"\"\n    Note: This transform is deprecated in favor of Resize.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n                      \"please use transforms.Resize instead.\")\n        super(Scale, self).__init__(*args, **kwargs)\n\n\nclass CenterCrop(object):\n    \"\"\"Crops the given PIL Image at the center.\n\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    \"\"\"\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be cropped.\n\n        Returns:\n            PIL Image: Cropped image.\n        \"\"\"\n        return F.center_crop(img, self.size)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(size={0})'.format(self.size)\n\n\nclass Pad(object):\n    \"\"\"Pad the given PIL Image on all sides with the given \"pad\" value.\n\n    Args:\n        padding (int or tuple): Padding on each border. If a single int is provided this\n            is used to pad all borders. If tuple of length 2 is provided this is the padding\n            on left/right and top/bottom respectively. If a tuple of length 4 is provided\n            this is the padding for the left, top, right and bottom borders\n            respectively.\n ", "doc_id": "e547455a-37f3-4146-904f-ea3596ea15a4", "embedding": null, "doc_hash": "42514ae34e9f6f577caffd77b47948ec8b4b5b1c4ff74f662012f7449e41db30", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 6739, "end": 8484}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "3d67f6a6-90a7-41fa-aaad-864df1de4ce7", "3": "7168fdfa-4084-4a20-b02a-926086ab3ead"}}, "__type__": "1"}, "7168fdfa-4084-4a20-b02a-926086ab3ead": {"__data__": {"text": "on left/right and top/bottom respectively. If a tuple of length 4 is provided\n            this is the padding for the left, top, right and bottom borders\n            respectively.\n        fill (int or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of\n            length 3, it is used to fill R, G, B channels respectively.\n            This value is only used when the padding_mode is constant\n        padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric.\n            Default is constant.\n\n            - constant: pads with a constant value, this value is specified with fill\n\n            - edge: pads with the last value at the edge of the image\n\n            - reflect: pads with reflection of image without repeating the last value on the edge\n\n                For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n                will result in [3, 2, 1, 2, 3, 4, 3, 2]\n\n            - symmetric: pads with reflection of image repeating the last value on the edge\n\n                For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n                will result in [2, 1, 1, 2, 3, 4, 4, 3]\n    \"\"\"\n\n    def __init__(self, padding, fill=0, padding_mode='constant'):\n        assert isinstance(padding, (numbers.Number, tuple))\n        assert isinstance(fill, (numbers.Number, str, tuple))\n        assert padding_mode in ['constant', 'edge', 'reflect', 'symmetric']\n        if isinstance(padding, Sequence) and len(padding) not in [2, 4]:\n            raise ValueError(\"Padding must be an int or a 2, or 4 element tuple, not a \" +\n                             \"{} element tuple\".format(len(padding)))\n\n        self.padding = padding\n        self.fill = fill\n        self.padding_mode = padding_mode\n\n  ", "doc_id": "7168fdfa-4084-4a20-b02a-926086ab3ead", "embedding": null, "doc_hash": "aa5e63d451bdad9caeda65f059dcafa2d2b1bcbb8932ced83dfa99b35317d836", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 8492, "end": 10300}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "e547455a-37f3-4146-904f-ea3596ea15a4", "3": "58e4b1c9-61bc-40e4-bdf9-30fa95aa0c97"}}, "__type__": "1"}, "58e4b1c9-61bc-40e4-bdf9-30fa95aa0c97": {"__data__": {"text": "     \"{} element tuple\".format(len(padding)))\n\n        self.padding = padding\n        self.fill = fill\n        self.padding_mode = padding_mode\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be padded.\n\n        Returns:\n            PIL Image: Padded image.\n        \"\"\"\n        return F.pad(img, self.padding, self.fill, self.padding_mode)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(padding={0}, fill={1}, padding_mode={2})'.\\\n            format(self.padding, self.fill, self.padding_mode)\n\n\nclass Lambda(object):\n    \"\"\"Apply a user-defined lambda as a transform.\n\n    Args:\n        lambd (function): Lambda/function to be used for transform.\n    \"\"\"\n\n    def __init__(self, lambd):\n        assert callable(lambd), repr(type(lambd).__name__) + \" object is not callable\"\n        self.lambd = lambd\n\n    def __call__(self, img):\n        return self.lambd(img)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n\nclass RandomTransforms(object):\n    \"\"\"Base class for a list of transformations with randomness\n\n    Args:\n        transforms (list or tuple): list of transformations\n    \"\"\"\n\n    def __init__(self, transforms):\n        assert isinstance(transforms, (list, tuple))\n        self.transforms = transforms\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError()\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        for t in self.transforms:\n            format_string += '\\n'\n            format_string += '    {0}'.format(t)\n        format_string += '\\n)'\n        return format_string\n\n\nclass RandomApply(RandomTransforms):\n    \"\"\"Apply", "doc_id": "58e4b1c9-61bc-40e4-bdf9-30fa95aa0c97", "embedding": null, "doc_hash": "223ac5427ed9bb7d06b13ab6f633786393a71407f4a199dc8740068d2819bf4a", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 10326, "end": 12016}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "7168fdfa-4084-4a20-b02a-926086ab3ead", "3": "95cf1b1a-366f-4ace-bd97-8c09267b1422"}}, "__type__": "1"}, "95cf1b1a-366f-4ace-bd97-8c09267b1422": {"__data__": {"text": "    format_string += '    {0}'.format(t)\n        format_string += '\\n)'\n        return format_string\n\n\nclass RandomApply(RandomTransforms):\n    \"\"\"Apply randomly a list of transformations with a given probability\n\n    Args:\n        transforms (list or tuple): list of transformations\n        p (float): probability\n    \"\"\"\n\n    def __init__(self, transforms, p=0.5):\n        super(RandomApply, self).__init__(transforms)\n        self.p = p\n\n    def __call__(self, img):\n        if self.p < random.random():\n            return img\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        format_string += '\\n    p={}'.format(self.p)\n        for t in self.transforms:\n            format_string += '\\n'\n            format_string += '    {0}'.format(t)\n        format_string += '\\n)'\n        return format_string\n\n\nclass RandomOrder(RandomTransforms):\n    \"\"\"Apply a list of transformations in a random order\n    \"\"\"\n    def __call__(self, img):\n        order = list(range(len(self.transforms)))\n        random.shuffle(order)\n        for i in order:\n            img = self.transforms[i](img)\n        return img\n\n\nclass RandomChoice(RandomTransforms):\n    \"\"\"Apply single transformation randomly picked from a list\n    \"\"\"\n    def __call__(self, img):\n        t = random.choice(self.transforms)\n        return t(img)\n\n\nclass RandomCrop(object):\n    \"\"\"Crop the given PIL Image at a random location.\n\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n     ", "doc_id": "95cf1b1a-366f-4ace-bd97-8c09267b1422", "embedding": null, "doc_hash": "35dcb44bed90f9ea50b5ace6d91584e3c84d02703498c37bc43c560628942f84", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 12007, "end": 13701}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "58e4b1c9-61bc-40e4-bdf9-30fa95aa0c97", "3": "affa04fa-bfe3-4603-b8da-875f9740b27c"}}, "__type__": "1"}, "affa04fa-bfe3-4603-b8da-875f9740b27c": {"__data__": {"text": "Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n        padding (int or sequence, optional): Optional padding on each border\n            of the image. Default is None, i.e no padding. If a sequence of length\n            4 is provided, it is used to pad left, top, right, bottom borders\n            respectively. If a sequence of length 2 is provided, it is used to\n            pad left/right, top/bottom borders, respectively.\n        pad_if_needed (boolean): It will pad the image if smaller than the\n            desired size to avoid raising an exception.\n        fill: Pixel fill value for constant fill. Default is 0. If a tuple of\n            length 3, it is used to fill R, G, B channels respectively.\n            This value is only used when the padding_mode is constant\n        padding_mode: Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.\n\n             - constant: pads with a constant value, this value is specified with fill\n\n             - edge: pads with the last value on the edge of the image\n\n             - reflect: pads with reflection of image (without repeating the last value on the edge)\n\n                padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n                will result in [3, 2, 1, 2, 3, 4, 3, 2]\n\n             - symmetric: pads with reflection of image (repeating the last value on the edge)\n\n                padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n                will result in [2, 1, 1, 2, 3, 4, 4, 3]\n\n    \"\"\"\n\n    def __init__(self, size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant'):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n       ", "doc_id": "affa04fa-bfe3-4603-b8da-875f9740b27c", "embedding": null, "doc_hash": "fdf81cb75058c56bbed6d5d793c9eb5ee8b440b612d171ce030bef7e9699752e", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 13721, "end": 15551}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "95cf1b1a-366f-4ace-bd97-8c09267b1422", "3": "7061537c-6bb1-431e-85ba-5340b6d7c544"}}, "__type__": "1"}, "7061537c-6bb1-431e-85ba-5340b6d7c544": {"__data__": {"text": "fill=0, padding_mode='constant'):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n        self.pad_if_needed = pad_if_needed\n        self.fill = fill\n        self.padding_mode = padding_mode\n\n    @staticmethod\n    def get_params(img, output_size):\n        \"\"\"Get parameters for ``crop`` for a random crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n        \"\"\"\n        w, h = img.size\n        th, tw = output_size\n        if w == tw and h == th:\n            return 0, 0, h, w\n\n        i = random.randint(0, h - th)\n        j = random.randint(0, w - tw)\n        return i, j, th, tw\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be cropped.\n\n        Returns:\n            PIL Image: Cropped image.\n        \"\"\"\n        if self.padding is not None:\n            img = F.pad(img, self.padding, self.fill, self.padding_mode)\n\n        # pad the width if needed\n        if self.pad_if_needed and img.size[0] < self.size[1]:\n            img = F.pad(img, (self.size[1] - img.size[0], 0), self.fill, self.padding_mode)\n        # pad the height if needed\n        if self.pad_if_needed and img.size[1] < self.size[0]:\n            img = F.pad(img,", "doc_id": "7061537c-6bb1-431e-85ba-5340b6d7c544", "embedding": null, "doc_hash": "8a051c6b45c0fc4ba17888daf56e55483cb20382f96a00f097384c0e25c15c71", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 15551, "end": 17036}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "affa04fa-bfe3-4603-b8da-875f9740b27c", "3": "540a9298-44b6-452b-89f3-e1df09535535"}}, "__type__": "1"}, "540a9298-44b6-452b-89f3-e1df09535535": {"__data__": {"text": "       # pad the height if needed\n        if self.pad_if_needed and img.size[1] < self.size[0]:\n            img = F.pad(img, (0, self.size[0] - img.size[1]), self.fill, self.padding_mode)\n\n        i, j, h, w = self.get_params(img, self.size)\n\n        return F.crop(img, i, j, h, w)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(size={0}, padding={1})'.format(self.size, self.padding)\n\n\nclass RandomHorizontalFlip(object):\n    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n\n    Args:\n        p (float): probability of the image being flipped. Default value is 0.5\n    \"\"\"\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be flipped.\n\n        Returns:\n            PIL Image: Randomly flipped image.\n        \"\"\"\n        if random.random() < self.p:\n            return F.hflip(img)\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(p={})'.format(self.p)\n\n\nclass RandomVerticalFlip(object):\n    \"\"\"Vertically flip the given PIL Image randomly with a given probability.\n\n    Args:\n        p (float): probability of the image being flipped. Default value is 0.5\n    \"\"\"\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be flipped.\n\n        Returns:\n            PIL Image: Randomly flipped image.\n        \"\"\"\n        if random.random() < self.p:\n            return F.vflip(img)\n      ", "doc_id": "540a9298-44b6-452b-89f3-e1df09535535", "embedding": null, "doc_hash": "aa4d64821f20faa263d02d044d2289f538be76def980c7f7a3a74c8344e4c80f", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 17052, "end": 18609}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "7061537c-6bb1-431e-85ba-5340b6d7c544", "3": "80642d4a-8e8e-46d7-b681-15dbc2374ac5"}}, "__type__": "1"}, "80642d4a-8e8e-46d7-b681-15dbc2374ac5": {"__data__": {"text": "Image: Randomly flipped image.\n        \"\"\"\n        if random.random() < self.p:\n            return F.vflip(img)\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(p={})'.format(self.p)\n\n\nclass RandomResizedCrop(object):\n    \"\"\"Crop the given PIL Image to random size and aspect ratio.\n\n    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n    is finally resized to given size.\n    This is popularly used to train the Inception networks.\n\n    Args:\n        size: expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped\n        interpolation: Default: PIL.Image.BILINEAR\n    \"\"\"\n\n    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.), interpolation=Image.BILINEAR):\n        if isinstance(size, tuple):\n            self.size = size\n        else:\n            self.size = (size, size)\n        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n            warnings.warn(\"range should be of kind (min, max)\")\n\n        self.interpolation = interpolation\n        self.scale = scale\n        self.ratio = ratio\n\n    @staticmethod\n    def get_params(img, scale, ratio):\n        \"\"\"Get parameters for ``crop`` for a random sized crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n    ", "doc_id": "80642d4a-8e8e-46d7-b681-15dbc2374ac5", "embedding": null, "doc_hash": "3e499be502a4fbf277dbee822883ab27291a68818154e3a8dccb485d62c79678", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 18618, "end": 20341}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "540a9298-44b6-452b-89f3-e1df09535535", "3": "26fb4783-fe37-41cd-adb1-4d99ebd205a5"}}, "__type__": "1"}, "26fb4783-fe37-41cd-adb1-4d99ebd205a5": {"__data__": {"text": "aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        \"\"\"\n        area = img.size[0] * img.size[1]\n\n        for attempt in range(10):\n            target_area = random.uniform(*scale) * area\n            aspect_ratio = random.uniform(*ratio)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5 and min(ratio) <= (h / w) <= max(ratio):\n                w, h = h, w\n\n            if w <= img.size[0] and h <= img.size[1]:\n                i = random.randint(0, img.size[1] - h)\n                j = random.randint(0, img.size[0] - w)\n                return i, j, h, w\n\n        # Fallback\n        w = min(img.size[0], img.size[1])\n        i = (img.size[1] - w) // 2\n        j = (img.size[0] - w) // 2\n        return i, j, w, w\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be cropped and resized.\n\n        Returns:\n            PIL Image: Randomly cropped and resized image.\n        \"\"\"\n        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n        return F.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n\n    def __repr__(self):\n        interpolate_str =", "doc_id": "26fb4783-fe37-41cd-adb1-4d99ebd205a5", "embedding": null, "doc_hash": "2aa6fbfacb0114acfb65b9c8e28c3118a2d81c070b845924311adbb21b1186de", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 20316, "end": 21679}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "80642d4a-8e8e-46d7-b681-15dbc2374ac5", "3": "f501ca90-0c88-4bb2-83d0-e43e634ac8ec"}}, "__type__": "1"}, "f501ca90-0c88-4bb2-83d0-e43e634ac8ec": {"__data__": {"text": "       return F.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n\n    def __repr__(self):\n        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)\n        format_string += ', scale={0}'.format(tuple(round(s, 4) for s in self.scale))\n        format_string += ', ratio={0}'.format(tuple(round(r, 4) for r in self.ratio))\n        format_string += ', interpolation={0})'.format(interpolate_str)\n        return format_string\n\n\nclass RandomSizedCrop(RandomResizedCrop):\n    \"\"\"\n    Note: This transform is deprecated in favor of RandomResizedCrop.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\n                      \"please use transforms.RandomResizedCrop instead.\")\n        super(RandomSizedCrop, self).__init__(*args, **kwargs)\n\n\nclass FiveCrop(object):\n    \"\"\"Crop the given PIL Image into four corners and the central crop\n\n    .. Note::\n         This transform returns a tuple of images and there may be a mismatch in the number of\n         inputs and targets your Dataset returns. See below for an example of how to deal with\n         this.\n\n    Args:\n         size (sequence or int): Desired output size of the crop. If size is an ``int``\n            instead of sequence like (h, w), a square crop of size (size, size) is made.\n\n    Example:\n         >>> transform = Compose([\n         >>>    FiveCrop(size), # this is a list of PIL Images\n         >>>    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor\n         >>> ])\n         >>> #In your test loop you can do the following:\n         >>> input, target = batch # input is a 5d tensor, target is 2d\n      ", "doc_id": "f501ca90-0c88-4bb2-83d0-e43e634ac8ec", "embedding": null, "doc_hash": "1056d4921724c34b8871b58603f179013df777f1abad2420c06a887285ea4a4f", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 21684, "end": 23492}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "26fb4783-fe37-41cd-adb1-4d99ebd205a5", "3": "9adfaa02-f3ce-47c3-a6cf-4d194103593b"}}, "__type__": "1"}, "9adfaa02-f3ce-47c3-a6cf-4d194103593b": {"__data__": {"text": "     >>> ])\n         >>> #In your test loop you can do the following:\n         >>> input, target = batch # input is a 5d tensor, target is 2d\n         >>> bs, ncrops, c, h, w = input.size()\n         >>> result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops\n         >>> result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops\n    \"\"\"\n\n    def __init__(self, size):\n        self.size = size\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            assert len(size) == 2, \"Please provide only two dimensions (h, w) for size.\"\n            self.size = size\n\n    def __call__(self, img):\n        return F.five_crop(img, self.size)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(size={0})'.format(self.size)\n\n\nclass TenCrop(object):\n    \"\"\"Crop the given PIL Image into four corners and the central crop plus the flipped version of\n    these (horizontal flipping is used by default)\n\n    .. Note::\n         This transform returns a tuple of images and there may be a mismatch in the number of\n         inputs and targets your Dataset returns. See below for an example of how to deal with\n         this.\n\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n        vertical_flip(bool): Use vertical flipping instead of horizontal\n\n    Example:\n         >>> transform = Compose([\n         >>>    TenCrop(size), # this is a list of PIL Images\n         >>>    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor\n         >>> ])\n         >>> #In your test loop you can do the", "doc_id": "9adfaa02-f3ce-47c3-a6cf-4d194103593b", "embedding": null, "doc_hash": "8d149c13bed9e7dc0c01ff62716e265c216ac54ce1ea86fba59efa1c66325957", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 23496, "end": 25243}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "f501ca90-0c88-4bb2-83d0-e43e634ac8ec", "3": "2860cdec-aea3-4d39-8bbc-70474632b43f"}}, "__type__": "1"}, "2860cdec-aea3-4d39-8bbc-70474632b43f": {"__data__": {"text": "crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor\n         >>> ])\n         >>> #In your test loop you can do the following:\n         >>> input, target = batch # input is a 5d tensor, target is 2d\n         >>> bs, ncrops, c, h, w = input.size()\n         >>> result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops\n         >>> result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops\n    \"\"\"\n\n    def __init__(self, size, vertical_flip=False):\n        self.size = size\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            assert len(size) == 2, \"Please provide only two dimensions (h, w) for size.\"\n            self.size = size\n        self.vertical_flip = vertical_flip\n\n    def __call__(self, img):\n        return F.ten_crop(img, self.size, self.vertical_flip)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(size={0}, vertical_flip={1})'.format(self.size, self.vertical_flip)\n\n\nclass LinearTransformation(object):\n    \"\"\"Transform a tensor image with a square transformation matrix computed\n    offline.\n\n    Given transformation_matrix, will flatten the torch.*Tensor, compute the dot\n    product with the transformation matrix and reshape the tensor to its\n    original shape.\n\n    Applications:\n        - whitening: zero-center the data, compute the data covariance matrix\n                 [D x D] with np.dot(X.T, X), perform SVD on this matrix and\n                 pass it as transformation_matrix.\n\n    Args:\n        transformation_matrix (Tensor): tensor [D x D], D = C x H x W\n    \"\"\"\n\n    def __init__(self, transformation_matrix):\n        if transformation_matrix.size(0) !=", "doc_id": "2860cdec-aea3-4d39-8bbc-70474632b43f", "embedding": null, "doc_hash": "eee25ec340ee6f097c52a25c70e3f26e733250830c297d17f24f0c17e6c326bd", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 25235, "end": 26958}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "9adfaa02-f3ce-47c3-a6cf-4d194103593b", "3": "cd472d9f-d039-4f43-98c6-76bd2c1d7897"}}, "__type__": "1"}, "cd472d9f-d039-4f43-98c6-76bd2c1d7897": {"__data__": {"text": "(Tensor): tensor [D x D], D = C x H x W\n    \"\"\"\n\n    def __init__(self, transformation_matrix):\n        if transformation_matrix.size(0) != transformation_matrix.size(1):\n            raise ValueError(\"transformation_matrix should be square. Got \" +\n                             \"[{} x {}] rectangular matrix.\".format(*transformation_matrix.size()))\n        self.transformation_matrix = transformation_matrix\n\n    def __call__(self, tensor):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be whitened.\n\n        Returns:\n            Tensor: Transformed image.\n        \"\"\"\n        if tensor.size(0) * tensor.size(1) * tensor.size(2) != self.transformation_matrix.size(0):\n            raise ValueError(\"tensor and transformation matrix have incompatible shape.\" +\n                             \"[{} x {} x {}] != \".format(*tensor.size()) +\n                             \"{}\".format(self.transformation_matrix.size(0)))\n        flat_tensor = tensor.view(1, -1)\n        transformed_tensor = torch.mm(flat_tensor, self.transformation_matrix)\n        tensor = transformed_tensor.view(tensor.size())\n        return tensor\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        format_string += (str(self.transformation_matrix.numpy().tolist()) + ')')\n        return format_string\n\n\nclass ColorJitter(object):\n    \"\"\"Randomly change the brightness, contrast and saturation of an image.\n\n    Args:\n        brightness (float or tuple of float (min, max)): How much to jitter brightness.\n            brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n  ", "doc_id": "cd472d9f-d039-4f43-98c6-76bd2c1d7897", "embedding": null, "doc_hash": "4e187a68109e391573b2c6a69d892deb58d65ff23d0a95e2601d479d7041c05c", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 26956, "end": 28601}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "2860cdec-aea3-4d39-8bbc-70474632b43f", "3": "3089486b-5a50-4f76-bd8c-c3e64f07d38b"}}, "__type__": "1"}, "3089486b-5a50-4f76-bd8c-c3e64f07d38b": {"__data__": {"text": "brightness (float or tuple of float (min, max)): How much to jitter brightness.\n            brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n            or the given [min, max]. Should be non negative numbers.\n        contrast (float or tuple of float (min, max)): How much to jitter contrast.\n            contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]\n            or the given [min, max]. Should be non negative numbers.\n        saturation (float or tuple of float (min, max)): How much to jitter saturation.\n            saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]\n            or the given [min, max]. Should be non negative numbers.\n        hue (float or tuple of float (min, max)): How much to jitter hue.\n            hue_factor is chosen uniformly from [-hue, hue] or the given [min, max].\n            Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.\n    \"\"\"\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n        self.brightness = self._check_input(brightness, 'brightness')\n        self.contrast = self._check_input(contrast, 'contrast')\n        self.saturation = self._check_input(saturation, 'saturation')\n        self.hue = self._check_input(hue, 'hue', center=0, bound=(-0.5, 0.5),\n                                     clip_first_on_zero=False)\n\n    def _check_input(self, value, name, center=1, bound=(0, float('inf')), clip_first_on_zero=True):\n        if isinstance(value, numbers.Number):\n            if value < 0:\n                raise ValueError(\"If {} is a single number, it must be non negative.\".format(name))\n            value = [center - value, center + value]\n ", "doc_id": "3089486b-5a50-4f76-bd8c-c3e64f07d38b", "embedding": null, "doc_hash": "ba7e7f31f167d35a1a8070d1174f2bd3d54f9d2e66e4d113f9fe8b428f292223", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 28569, "end": 30290}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "cd472d9f-d039-4f43-98c6-76bd2c1d7897", "3": "453ea2ec-86e5-4578-8f7c-0a1730ccf858"}}, "__type__": "1"}, "453ea2ec-86e5-4578-8f7c-0a1730ccf858": {"__data__": {"text": "               raise ValueError(\"If {} is a single number, it must be non negative.\".format(name))\n            value = [center - value, center + value]\n            if clip_first_on_zero:\n                value[0] = max(value[0], 0)\n        elif isinstance(value, (tuple, list)) and len(value) == 2:\n            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n                raise ValueError(\"{} values should be between {}\".format(name, bound))\n        else:\n            raise TypeError(\"{} should be a single number or a list/tuple with lenght 2.\".format(name))\n\n        # if value is 0 or (1., 1.) for brightness/contrast/saturation\n        # or (0., 0.) for hue, do nothing\n        if value[0] == value[1] == center:\n            value = None\n        return value\n\n    @staticmethod\n    def get_params(brightness, contrast, saturation, hue):\n        \"\"\"Get a randomized transform to be applied on image.\n\n        Arguments are same as that of __init__.\n\n        Returns:\n            Transform which randomly adjusts brightness, contrast and\n            saturation in a random order.\n        \"\"\"\n        transforms = []\n\n        if brightness is not None:\n            brightness_factor = random.uniform(brightness[0], brightness[1])\n            transforms.append(Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n\n        if contrast is not None:\n            contrast_factor = random.uniform(contrast[0], contrast[1])\n            transforms.append(Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n           ", "doc_id": "453ea2ec-86e5-4578-8f7c-0a1730ccf858", "embedding": null, "doc_hash": "7e3c2f487d9fe772afdfdc999eb424189397896753ed950ee1a1ef8406cd688f", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 30325, "end": 31979}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "3089486b-5a50-4f76-bd8c-c3e64f07d38b", "3": "b04dd427-f9af-4d3e-9079-4f04aff00e95"}}, "__type__": "1"}, "b04dd427-f9af-4d3e-9079-4f04aff00e95": {"__data__": {"text": "contrast_factor)))\n\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n            transforms.append(Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n\n        if hue is not None:\n            hue_factor = random.uniform(hue[0], hue[1])\n            transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n\n        random.shuffle(transforms)\n        transform = Compose(transforms)\n\n        return transform\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Input image.\n\n        Returns:\n            PIL Image: Color jittered image.\n        \"\"\"\n        transform = self.get_params(self.brightness, self.contrast,\n                                    self.saturation, self.hue)\n        return transform(img)\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        format_string += 'brightness={0}'.format(self.brightness)\n        format_string += ', contrast={0}'.format(self.contrast)\n        format_string += ', saturation={0}'.format(self.saturation)\n        format_string += ', hue={0})'.format(self.hue)\n        return format_string\n\n\nclass RandomRotation(object):\n    \"\"\"Rotate the image by angle.\n\n    Args:\n        degrees (sequence or float or int): Range of degrees to select from.\n            If degrees is a number instead of sequence like (min, max), the range of degrees\n            will be (-degrees, +degrees).\n        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n            An optional resampling filter. See `filters`_ for more information.\n       ", "doc_id": "b04dd427-f9af-4d3e-9079-4f04aff00e95", "embedding": null, "doc_hash": "9d3276124e1b6aedcc93046a5684f5c4aef29b4bbb7c111142e77e15469919d4", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 31982, "end": 33637}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "453ea2ec-86e5-4578-8f7c-0a1730ccf858", "3": "8d1beeac-9d78-4a70-996b-3a28ddf18e63"}}, "__type__": "1"}, "8d1beeac-9d78-4a70-996b-3a28ddf18e63": {"__data__": {"text": "PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n            An optional resampling filter. See `filters`_ for more information.\n            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n        expand (bool, optional): Optional expansion flag.\n            If true, expands the output to make it large enough to hold the entire rotated image.\n            If false or omitted, make the output image the same size as the input image.\n            Note that the expand flag assumes rotation around the center and no translation.\n        center (2-tuple, optional): Optional center of rotation.\n            Origin is the upper left corner.\n            Default is the center of the image.\n\n    .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters\n\n    \"\"\"\n\n    def __init__(self, degrees, resample=False, expand=False, center=None):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError(\"If degrees is a single number, it must be positive.\")\n            self.degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n            self.degrees = degrees\n\n        self.resample = resample\n        self.expand = expand\n        self.center = center\n\n    @staticmethod\n    def get_params(degrees):\n        \"\"\"Get parameters for ``rotate`` for a random rotation.\n\n        Returns:\n            sequence: params to be passed to ``rotate`` for random rotation.\n        \"\"\"\n        angle = random.uniform(degrees[0], degrees[1])\n\n        return angle\n\n    def __call__(self, img):\n        \"\"\"\n            img (PIL Image): Image to be rotated.\n\n", "doc_id": "8d1beeac-9d78-4a70-996b-3a28ddf18e63", "embedding": null, "doc_hash": "58ccd4a9047696151bc658f6cf80bb3c2f833a5b4694bba5ead31a6122c03335", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 33634, "end": 35387}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "b04dd427-f9af-4d3e-9079-4f04aff00e95", "3": "a2b1b96d-455c-4033-8160-aa5a25f0e2b3"}}, "__type__": "1"}, "a2b1b96d-455c-4033-8160-aa5a25f0e2b3": {"__data__": {"text": "degrees[1])\n\n        return angle\n\n    def __call__(self, img):\n        \"\"\"\n            img (PIL Image): Image to be rotated.\n\n        Returns:\n            PIL Image: Rotated image.\n        \"\"\"\n\n        angle = self.get_params(self.degrees)\n\n        return F.rotate(img, angle, self.resample, self.expand, self.center)\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '(degrees={0}'.format(self.degrees)\n        format_string += ', resample={0}'.format(self.resample)\n        format_string += ', expand={0}'.format(self.expand)\n        if self.center is not None:\n            format_string += ', center={0}'.format(self.center)\n        format_string += ')'\n        return format_string\n\n\nclass RandomAffine(object):\n    \"\"\"Random affine transformation of the image keeping center invariant\n\n    Args:\n        degrees (sequence or float or int): Range of degrees to select from.\n            If degrees is a number instead of sequence like (min, max), the range of degrees\n            will be (-degrees, +degrees). Set to 0 to deactivate rotations.\n        translate (tuple, optional): tuple of maximum absolute fraction for horizontal\n            and vertical translations. For example translate=(a, b), then horizontal shift\n            is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is\n            randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default.\n        scale (tuple, optional): scaling factor interval, e.g (a, b), then scale is\n            randomly sampled from the range a <= scale <= b. Will keep original scale by default.\n        shear (sequence or float or int, optional): Range of degrees to select from.\n            If degrees is a number instead of sequence like (min, max), the range of degrees\n            will be (-degrees, +degrees). Will not", "doc_id": "a2b1b96d-455c-4033-8160-aa5a25f0e2b3", "embedding": null, "doc_hash": "743eadd83fca789cd631d6cc59c5d063bbc5b9174e10705b96e176fbbdbd414d", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 35411, "end": 37287}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "8d1beeac-9d78-4a70-996b-3a28ddf18e63", "3": "b2aba21d-fd17-40f3-a618-496cf2a7978a"}}, "__type__": "1"}, "b2aba21d-fd17-40f3-a618-496cf2a7978a": {"__data__": {"text": "Range of degrees to select from.\n            If degrees is a number instead of sequence like (min, max), the range of degrees\n            will be (-degrees, +degrees). Will not apply shear by default\n        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n            An optional resampling filter. See `filters`_ for more information.\n            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n        fillcolor (int): Optional fill color for the area outside the transform in the output image. (Pillow>=5.0.0)\n\n    .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters\n\n    \"\"\"\n\n    def __init__(self, degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError(\"If degrees is a single number, it must be positive.\")\n            self.degrees = (-degrees, degrees)\n        else:\n            assert isinstance(degrees, (tuple, list)) and len(degrees) == 2, \\\n                \"degrees should be a list or tuple and it must be of length 2.\"\n            self.degrees = degrees\n\n        if translate is not None:\n            assert isinstance(translate, (tuple, list)) and len(translate) == 2, \\\n                \"translate should be a list or tuple and it must be of length 2.\"\n            for t in translate:\n                if not (0.0 <= t <= 1.0):\n                    raise ValueError(\"translation values should be between 0 and 1\")\n        self.translate = translate\n\n        if scale is not None:\n            assert isinstance(scale, (tuple, list)) and len(scale) == 2, \\\n          ", "doc_id": "b2aba21d-fd17-40f3-a618-496cf2a7978a", "embedding": null, "doc_hash": "c9ac45f55d44c022afe25ff48ca1bbda3c51a960762a098cc0056a6fda2a68c8", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 37246, "end": 38955}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "a2b1b96d-455c-4033-8160-aa5a25f0e2b3", "3": "2b4d5910-9827-414f-8633-9fc80a1f7916"}}, "__type__": "1"}, "2b4d5910-9827-414f-8633-9fc80a1f7916": {"__data__": {"text": "= translate\n\n        if scale is not None:\n            assert isinstance(scale, (tuple, list)) and len(scale) == 2, \\\n                \"scale should be a list or tuple and it must be of length 2.\"\n            for s in scale:\n                if s <= 0:\n                    raise ValueError(\"scale values should be positive\")\n        self.scale = scale\n\n        if shear is not None:\n            if isinstance(shear, numbers.Number):\n                if shear < 0:\n                    raise ValueError(\"If shear is a single number, it must be positive.\")\n                self.shear = (-shear, shear)\n            else:\n                assert isinstance(shear, (tuple, list)) and len(shear) == 2, \\\n                    \"shear should be a list or tuple and it must be of length 2.\"\n                self.shear = shear\n        else:\n            self.shear = shear\n\n        self.resample = resample\n        self.fillcolor = fillcolor\n\n    @staticmethod\n    def get_params(degrees, translate, scale_ranges, shears, img_size):\n        \"\"\"Get parameters for affine transformation\n\n        Returns:\n            sequence: params to be passed to the affine transformation\n        \"\"\"\n        angle = random.uniform(degrees[0], degrees[1])\n        if translate is not None:\n            max_dx = translate[0] * img_size[0]\n            max_dy = translate[1] * img_size[1]\n            translations = (np.round(random.uniform(-max_dx, max_dx)),\n                            np.round(random.uniform(-max_dy,", "doc_id": "2b4d5910-9827-414f-8633-9fc80a1f7916", "embedding": null, "doc_hash": "9c0f502d6e25799b203b0a4ca5975805d3e2f973b1a8df2072841f32d9ac523c", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 38999, "end": 40483}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "b2aba21d-fd17-40f3-a618-496cf2a7978a", "3": "c2d5978e-0caf-4bd4-b711-12539f419076"}}, "__type__": "1"}, "c2d5978e-0caf-4bd4-b711-12539f419076": {"__data__": {"text": "  translations = (np.round(random.uniform(-max_dx, max_dx)),\n                            np.round(random.uniform(-max_dy, max_dy)))\n        else:\n            translations = (0, 0)\n\n        if scale_ranges is not None:\n            scale = random.uniform(scale_ranges[0], scale_ranges[1])\n        else:\n            scale = 1.0\n\n        if shears is not None:\n            shear = random.uniform(shears[0], shears[1])\n        else:\n            shear = 0.0\n\n        return angle, translations, scale, shear\n\n    def __call__(self, img):\n        \"\"\"\n            img (PIL Image): Image to be transformed.\n\n        Returns:\n            PIL Image: Affine transformed image.\n        \"\"\"\n        ret = self.get_params(self.degrees, self.translate, self.scale, self.shear, img.size)\n        return F.affine(img, *ret, resample=self.resample, fillcolor=self.fillcolor)\n\n    def __repr__(self):\n        s = '{name}(degrees={degrees}'\n        if self.translate is not None:\n            s += ', translate={translate}'\n        if self.scale is not None:\n            s += ', scale={scale}'\n        if self.shear is not None:\n            s += ', shear={shear}'\n        if self.resample > 0:\n            s += ', resample={resample}'\n        if self.fillcolor != 0:\n            s += ', fillcolor={fillcolor}'\n        s += ')'\n        d = dict(self.__dict__)\n        d['resample'] = _pil_interpolation_to_str[d['resample']]\n        return", "doc_id": "c2d5978e-0caf-4bd4-b711-12539f419076", "embedding": null, "doc_hash": "66285e2d9c71acd2c0d8e2040e0429e6911ea41aae1ee7d66bf9811c5b196cc4", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 40481, "end": 41897}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "2b4d5910-9827-414f-8633-9fc80a1f7916", "3": "19264a5a-5884-452c-972c-7dafe4289fb4"}}, "__type__": "1"}, "19264a5a-5884-452c-972c-7dafe4289fb4": {"__data__": {"text": " s += ')'\n        d = dict(self.__dict__)\n        d['resample'] = _pil_interpolation_to_str[d['resample']]\n        return s.format(name=self.__class__.__name__, **d)\n\n\nclass Grayscale(object):\n    \"\"\"Convert image to grayscale.\n\n    Args:\n        num_output_channels (int): (1 or 3) number of channels desired for output image\n\n    Returns:\n        PIL Image: Grayscale version of the input.\n        - If num_output_channels == 1 : returned image is single channel\n        - If num_output_channels == 3 : returned image is 3 channel with r == g == b\n\n    \"\"\"\n\n    def __init__(self, num_output_channels=1):\n        self.num_output_channels = num_output_channels\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be converted to grayscale.\n\n        Returns:\n            PIL Image: Randomly grayscaled image.\n        \"\"\"\n        return F.to_grayscale(img, num_output_channels=self.num_output_channels)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(num_output_channels={0})'.format(self.num_output_channels)\n\n\nclass RandomGrayscale(object):\n    \"\"\"Randomly convert image to grayscale with a probability of p (default 0.1).\n\n    Args:\n        p (float): probability that image should be converted to grayscale.\n\n    Returns:\n        PIL Image: Grayscale version of the input image with probability p and unchanged\n        with probability (1-p).\n        - If input image is 1 channel: grayscale version is 1 channel\n        - If input image is 3 channel: grayscale version is 3 channel with r == g == b\n\n    \"\"\"\n\n    def __init__(self, p=0.1):\n        self.p = p\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be converted to", "doc_id": "19264a5a-5884-452c-972c-7dafe4289fb4", "embedding": null, "doc_hash": "f87ebf9b983b58fe2b2721c12e6c18dcc9fbe17be321f3ba4fcc8bd0cb66f1b1", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 41896, "end": 43630}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "c2d5978e-0caf-4bd4-b711-12539f419076", "3": "2ca8223f-e33b-4003-bba1-0e251330e1ee"}}, "__type__": "1"}, "2ca8223f-e33b-4003-bba1-0e251330e1ee": {"__data__": {"text": "   self.p = p\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be converted to grayscale.\n\n        Returns:\n            PIL Image: Randomly grayscaled image.\n        \"\"\"\n        num_output_channels = 1 if img.mode == 'L' else 3\n        if random.random() < self.p:\n            return F.to_grayscale(img, num_output_channels=num_output_channels)\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(p={0})'.format(self.p)\n", "doc_id": "2ca8223f-e33b-4003-bba1-0e251330e1ee", "embedding": null, "doc_hash": "9486ec6b85483d722685be637b4ca58e27b1dd9895525f06e0acdbaa9113068b", "extra_info": {"file_path": "TinyViT/data/augmentation/aug_tv_transforms.py", "file_name": "aug_tv_transforms.py"}, "node_info": {"start": 43597, "end": 44097}, "relationships": {"1": "c1ac61cd34dd2a4b24ad8897b60cbe948e47198c", "2": "19264a5a-5884-452c-972c-7dafe4289fb4"}}, "__type__": "1"}, "f4e01d7e-2f1c-4053-b334-f0d904c11ee8": {"__data__": {"text": "\"\"\" AutoAugment, RandAugment, and AugMix for PyTorch\n\nThis code implements the searched ImageNet policies with various tweaks and improvements and\ndoes not include any of the search code.\n\nAA and RA Implementation adapted from:\n    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py\n\nAugMix adapted from:\n    https://github.com/google-research/augmix\n\nPapers:\n    AutoAugment: Learning Augmentation Policies from Data - https://arxiv.org/abs/1805.09501\n    Learning Data Augmentation Strategies for Object Detection - https://arxiv.org/abs/1906.11172\n    RandAugment: Practical automated data augmentation... - https://arxiv.org/abs/1909.13719\n    AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty - https://arxiv.org/abs/1912.02781\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom .aug_random import random, np_random\nimport math\nimport re\nfrom PIL import Image, ImageOps, ImageEnhance, ImageChops\nimport PIL\nimport numpy as np\n\n\n_PIL_VER = tuple([int(x) for x in PIL.__version__.split('.')[:2]])\n\n_FILL = (128, 128, 128)\n\n_LEVEL_DENOM = 10.  # denominator for conversion from 'Mx' magnitude scale to fractional aug level for op arguments\n\n_HPARAMS_DEFAULT = dict(\n    translate_const=250,\n    img_mean=_FILL,\n)\n\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\n\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop('resample', Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:\n        return interpolation\n\n\ndef _check_args_tf(kwargs):\n    if 'fillcolor' in kwargs and _PIL_VER < (5, 0):\n        kwargs.pop('fillcolor')\n    kwargs['resample'] = _interpolation(kwargs)\n\n\ndef shear_x(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs)\n\n\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size,", "doc_id": "f4e01d7e-2f1c-4053-b334-f0d904c11ee8", "embedding": null, "doc_hash": "6c4a0ffdb41f729c35811d9779e543b16ef9d47c0a31ec6b4f06af48a209735f", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 0, "end": 1979}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "3": "82c4ea39-60d9-4867-987d-6457f2f528cd"}}, "__type__": "1"}, "82c4ea39-60d9-4867-987d-6457f2f528cd": {"__data__": {"text": "(1, factor, 0, 0, 1, 0), **kwargs)\n\n\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs)\n\n\ndef translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):\n        w, h = img.size\n        post_trans = (0, 0)\n        rotn_center = (w / 2.0, h / 2.0)\n        angle = -math.radians(degrees)\n        matrix = [\n            round(math.cos(angle), 15),\n            round(math.sin(angle), 15),\n            0.0,\n            round(-math.sin(angle), 15),\n            round(math.cos(angle), 15),\n            0.0,\n        ]\n\n        def transform(x, y, matrix):\n            (a, b, c,", "doc_id": "82c4ea39-60d9-4867-987d-6457f2f528cd", "embedding": null, "doc_hash": "0a86c2d136560d85cab88a8d8120bcc81339c1e268fef303d744faf19fcbcd8d", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 1864, "end": 3313}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "f4e01d7e-2f1c-4053-b334-f0d904c11ee8", "3": "8768b9ac-ce46-4c2f-9aa7-b1f8bb3d6822"}}, "__type__": "1"}, "8768b9ac-ce46-4c2f-9aa7-b1f8bb3d6822": {"__data__": {"text": "15),\n            0.0,\n        ]\n\n        def transform(x, y, matrix):\n            (a, b, c, d, e, f) = matrix\n            return a * x + b * y + c, d * x + e * y + f\n\n        matrix[2], matrix[5] = transform(\n            -rotn_center[0] - post_trans[0], -rotn_center[1] - post_trans[1], matrix\n        )\n        matrix[2] += rotn_center[0]\n        matrix[5] += rotn_center[1]\n        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)\n    else:\n        return img.rotate(degrees, resample=kwargs['resample'])\n\n\ndef auto_contrast(img, **__):\n    return ImageOps.autocontrast(img)\n\n\ndef invert(img, **__):\n    return ImageOps.invert(img)\n\n\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\n\n\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\n\n\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n\ndef posterize(img, bits_to_keep, **__):\n    if bits_to_keep >= 8:\n        return img\n    return ImageOps.posterize(img, bits_to_keep)\n\n\ndef contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\n\n\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\n\n\ndef brightness(img, factor, **__):\n ", "doc_id": "8768b9ac-ce46-4c2f-9aa7-b1f8bb3d6822", "embedding": null, "doc_hash": "ede7d62e3cb64a0346e4b4eae31aca6747796af31a5ab57832898832953a7fd2", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 3384, "end": 4883}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "82c4ea39-60d9-4867-987d-6457f2f528cd", "3": "5254fa30-8598-4d3c-b041-5760105955bd"}}, "__type__": "1"}, "5254fa30-8598-4d3c-b041-5760105955bd": {"__data__": {"text": "   return ImageEnhance.Contrast(img).enhance(factor)\n\n\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\n\n\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\n\n\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\n\n\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\n\n\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _LEVEL_DENOM) * 30.\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _enhance_level_to_arg(level, _hparams):\n    # range [0.1, 1.9]\n    return (level / _LEVEL_DENOM) * 1.8 + 0.1,\n\n\ndef _enhance_increasing_level_to_arg(level, _hparams):\n    # the 'no change' level is 1.0, moving away from that towards 0. or 2.0 increases the enhancement blend\n    # range [0.1, 1.9] if level <= _LEVEL_DENOM\n    level = (level / _LEVEL_DENOM) * .9\n    level = max(0.1, 1.0 + _randomly_negate(level))  # keep it >= 0.1\n    return level,\n\n\ndef _shear_level_to_arg(level, _hparams):\n    # range [-0.3, 0.3]\n    level = (level / _LEVEL_DENOM) * 0.3\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _translate_abs_level_to_arg(level, hparams):\n    translate_const = hparams['translate_const']\n    level = (level / _LEVEL_DENOM) * float(translate_const)\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _translate_rel_level_to_arg(level, hparams):\n    # default range [-0.45, 0.45]\n    translate_pct = hparams.get('translate_pct', 0.45)\n    level = (level / _LEVEL_DENOM) * translate_pct\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _posterize_level_to_arg(level, _hparams):\n    # As per Tensorflow TPU EfficientNet impl\n ", "doc_id": "5254fa30-8598-4d3c-b041-5760105955bd", "embedding": null, "doc_hash": "3953ee04d1354839de4f600484322ec778aeddd24c7a0782887f73ab790af6f2", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 4772, "end": 6543}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "8768b9ac-ce46-4c2f-9aa7-b1f8bb3d6822", "3": "dae0f160-8532-4c16-87d5-2a0bd6f9de00"}}, "__type__": "1"}, "dae0f160-8532-4c16-87d5-2a0bd6f9de00": {"__data__": {"text": "   level = _randomly_negate(level)\n    return level,\n\n\ndef _posterize_level_to_arg(level, _hparams):\n    # As per Tensorflow TPU EfficientNet impl\n    # range [0, 4], 'keep 0 up to 4 MSB of original image'\n    # intensity/severity of augmentation decreases with level\n    return int((level / _LEVEL_DENOM) * 4),\n\n\ndef _posterize_increasing_level_to_arg(level, hparams):\n    # As per Tensorflow models research and UDA impl\n    # range [4, 0], 'keep 4 down to 0 MSB of original image',\n    # intensity/severity of augmentation increases with level\n    return 4 - _posterize_level_to_arg(level, hparams)[0],\n\n\ndef _posterize_original_level_to_arg(level, _hparams):\n    # As per original AutoAugment paper description\n    # range [4, 8], 'keep 4 up to 8 MSB of image'\n    # intensity/severity of augmentation decreases with level\n    return int((level / _LEVEL_DENOM) * 4) + 4,\n\n\ndef _solarize_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation decreases with level\n    return int((level / _LEVEL_DENOM) * 256),\n\n\ndef _solarize_increasing_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation increases with level\n    return 256 - _solarize_level_to_arg(level, _hparams)[0],\n\n\ndef _solarize_add_level_to_arg(level, _hparams):\n    # range [0, 110]\n    return int((level / _LEVEL_DENOM) * 110),\n\n\nLEVEL_TO_ARG = {\n    'AutoContrast': None,\n    'Equalize': None,\n    'Invert': None,\n    'Rotate': _rotate_level_to_arg,\n    # There are several variations of the posterize level scaling in various Tensorflow/Google repositories/papers\n    'Posterize': _posterize_level_to_arg,\n    'PosterizeIncreasing': _posterize_increasing_level_to_arg,\n    'PosterizeOriginal': _posterize_original_level_to_arg,\n    'Solarize': _solarize_level_to_arg,\n    'SolarizeIncreasing': _solarize_increasing_level_to_arg,\n    'SolarizeAdd':", "doc_id": "dae0f160-8532-4c16-87d5-2a0bd6f9de00", "embedding": null, "doc_hash": "e62e0775b3c60873a235db51e05a97c993b66d6b7c4c80342a8bc63cf43b1a67", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 6577, "end": 8469}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "5254fa30-8598-4d3c-b041-5760105955bd", "3": "8faf17ab-c765-4460-896d-8fbe9a486e6f"}}, "__type__": "1"}, "8faf17ab-c765-4460-896d-8fbe9a486e6f": {"__data__": {"text": "_posterize_original_level_to_arg,\n    'Solarize': _solarize_level_to_arg,\n    'SolarizeIncreasing': _solarize_increasing_level_to_arg,\n    'SolarizeAdd': _solarize_add_level_to_arg,\n    'Color': _enhance_level_to_arg,\n    'ColorIncreasing': _enhance_increasing_level_to_arg,\n    'Contrast': _enhance_level_to_arg,\n    'ContrastIncreasing': _enhance_increasing_level_to_arg,\n    'Brightness': _enhance_level_to_arg,\n    'BrightnessIncreasing': _enhance_increasing_level_to_arg,\n    'Sharpness': _enhance_level_to_arg,\n    'SharpnessIncreasing': _enhance_increasing_level_to_arg,\n    'ShearX': _shear_level_to_arg,\n    'ShearY': _shear_level_to_arg,\n    'TranslateX': _translate_abs_level_to_arg,\n    'TranslateY': _translate_abs_level_to_arg,\n    'TranslateXRel': _translate_rel_level_to_arg,\n    'TranslateYRel': _translate_rel_level_to_arg,\n}\n\n\nNAME_TO_OP = {\n    'AutoContrast': auto_contrast,\n    'Equalize': equalize,\n    'Invert': invert,\n    'Rotate': rotate,\n    'Posterize': posterize,\n    'PosterizeIncreasing': posterize,\n    'PosterizeOriginal': posterize,\n    'Solarize': solarize,\n    'SolarizeIncreasing': solarize,\n    'SolarizeAdd': solarize_add,\n    'Color': color,\n    'ColorIncreasing': color,\n    'Contrast': contrast,\n    'ContrastIncreasing': contrast,\n    'Brightness': brightness,\n    'BrightnessIncreasing': brightness,\n    'Sharpness': sharpness,\n    'SharpnessIncreasing': sharpness,\n    'ShearX': shear_x,\n    'ShearY': shear_y,\n    'TranslateX': translate_x_abs,\n    'TranslateY': translate_y_abs,\n    'TranslateXRel': translate_x_rel,\n    'TranslateYRel': translate_y_rel,\n}\n\n\nclass AugmentOp:\n\n    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n       ", "doc_id": "8faf17ab-c765-4460-896d-8fbe9a486e6f", "embedding": null, "doc_hash": "bcf0acd411ce8400c5996c103a79e3b81ecf3475e64d7efe4a6ffef9cbea3478", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 8456, "end": 10156}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "dae0f160-8532-4c16-87d5-2a0bd6f9de00", "3": "dda4c406-8745-4eb8-b6e8-991d4bb6a30c"}}, "__type__": "1"}, "dda4c406-8745-4eb8-b6e8-991d4bb6a30c": {"__data__": {"text": "   'TranslateYRel': translate_y_rel,\n}\n\n\nclass AugmentOp:\n\n    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n        hparams = hparams or _HPARAMS_DEFAULT\n        self.name = name\n        self.aug_fn = NAME_TO_OP[name]\n        self.level_fn = LEVEL_TO_ARG[name]\n        self.prob = prob\n        self.magnitude = magnitude\n        self.hparams = hparams.copy()\n        self.kwargs = dict(\n            fillcolor=hparams['img_mean'] if 'img_mean' in hparams else _FILL,\n            resample=hparams['interpolation'] if 'interpolation' in hparams else _RANDOM_INTERPOLATION,\n        )\n\n        # If magnitude_std is > 0, we introduce some randomness\n        # in the usually fixed policy and sample magnitude from a normal distribution\n        # with mean `magnitude` and std-dev of `magnitude_std`.\n        # NOTE This is my own hack, being tested, not in papers or reference impls.\n        # If magnitude_std is inf, we sample magnitude from a uniform distribution\n        self.magnitude_std = self.hparams.get('magnitude_std', 0)\n        self.magnitude_max = self.hparams.get('magnitude_max', None)\n\n    def __call__(self, img):\n        if self.prob < 1.0 and random.random() > self.prob:\n            return img\n        magnitude = self.magnitude\n        if self.magnitude_std > 0:\n            # magnitude randomization enabled\n            if self.magnitude_std == float('inf'):\n                magnitude = random.uniform(0, magnitude)\n            elif self.magnitude_std > 0:\n                magnitude = random.gauss(magnitude, self.magnitude_std)\n        # default upper_bound for the timm RA impl is", "doc_id": "dda4c406-8745-4eb8-b6e8-991d4bb6a30c", "embedding": null, "doc_hash": "30f2a9497758ce4591f76d7b8f95d5bb4ccaa5d878574bd5af0d6a7a0d488a54", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 10183, "end": 11802}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "8faf17ab-c765-4460-896d-8fbe9a486e6f", "3": "47eebb8d-91ef-4aa9-990e-5718c03e843d"}}, "__type__": "1"}, "47eebb8d-91ef-4aa9-990e-5718c03e843d": {"__data__": {"text": "> 0:\n                magnitude = random.gauss(magnitude, self.magnitude_std)\n        # default upper_bound for the timm RA impl is _LEVEL_DENOM (10)\n        # setting magnitude_max overrides this to allow M > 10 (behaviour closer to Google TF RA impl)\n        upper_bound = self.magnitude_max or _LEVEL_DENOM\n        magnitude = max(0., min(magnitude, upper_bound))\n        level_args = self.level_fn(magnitude, self.hparams) if self.level_fn is not None else tuple()\n        return self.aug_fn(img, *level_args, **self.kwargs)\n\n    def __repr__(self):\n        fs = self.__class__.__name__ + f'(name={self.name}, p={self.prob}'\n        fs += f', m={self.magnitude}, mstd={self.magnitude_std}'\n        if self.magnitude_max is not None:\n            fs += f', mmax={self.magnitude_max}'\n        fs += ')'\n        return fs\n\n\ndef auto_augment_policy_v0(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, cannot find a paper reference.\n    policy = [\n        [('Equalize', 0.8, 1), ('ShearY', 0.8, 4)],\n        [('Color', 0.4, 9), ('Equalize', 0.6, 3)],\n        [('Color', 0.4, 1), ('Rotate', 0.6, 8)],\n        [('Solarize', 0.8, 3), ('Equalize', 0.4, 7)],\n        [('Solarize', 0.4, 2), ('Solarize', 0.6, 2)],\n        [('Color', 0.2, 0), ('Equalize', 0.8, 8)],\n        [('Equalize', 0.4, 8), ('SolarizeAdd', 0.8, 3)],\n        [('ShearX', 0.2, 9), ('Rotate', 0.6, 8)],\n        [('Color', 0.6, 1), ('Equalize', 1.0, 2)],\n        [('Invert', 0.4, 9), ('Rotate', 0.6, 0)],\n      ", "doc_id": "47eebb8d-91ef-4aa9-990e-5718c03e843d", "embedding": null, "doc_hash": "ffc17d1e2b61b040be828daef5d3ace4af460b36389049f26487e7882d443737", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 11822, "end": 13306}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "dda4c406-8745-4eb8-b6e8-991d4bb6a30c", "3": "b0ac782c-40a8-4bfd-aaf6-09a205e5ffc8"}}, "__type__": "1"}, "b0ac782c-40a8-4bfd-aaf6-09a205e5ffc8": {"__data__": {"text": "     [('Color', 0.6, 1), ('Equalize', 1.0, 2)],\n        [('Invert', 0.4, 9), ('Rotate', 0.6, 0)],\n        [('Equalize', 1.0, 9), ('ShearY', 0.6, 3)],\n        [('Color', 0.4, 7), ('Equalize', 0.6, 0)],\n        [('Posterize', 0.4, 6), ('AutoContrast', 0.4, 7)],\n        [('Solarize', 0.6, 8), ('Color', 0.6, 9)],\n        [('Solarize', 0.2, 4), ('Rotate', 0.8, 9)],\n        [('Rotate', 1.0, 7), ('TranslateYRel', 0.8, 9)],\n        [('ShearX', 0.0, 0), ('Solarize', 0.8, 4)],\n        [('ShearY', 0.8, 0), ('Color', 0.6, 4)],\n        [('Color', 1.0, 0), ('Rotate', 0.6, 2)],\n        [('Equalize', 0.8, 4), ('Equalize', 0.0, 8)],\n        [('Equalize', 1.0, 4), ('AutoContrast', 0.6, 2)],\n        [('ShearY', 0.4, 7), ('SolarizeAdd', 0.6, 7)],\n        [('Posterize', 0.8, 2), ('Solarize', 0.6, 10)],  # This results in black image with Tpu posterize\n        [('Solarize', 0.6, 8), ('Equalize', 0.6, 1)],\n        [('Color', 0.8, 6), ('Rotate', 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_v0r(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, with variation of Posterize used\n    # in Google research implementation (number of bits discarded increases with magnitude)\n    policy = [\n        [('Equalize', 0.8, 1), ('ShearY', 0.8, 4)],\n        [('Color', 0.4, 9), ('Equalize', 0.6, 3)],\n        [('Color', 0.4, 1), ('Rotate', 0.6,", "doc_id": "b0ac782c-40a8-4bfd-aaf6-09a205e5ffc8", "embedding": null, "doc_hash": "a91359614a8f4dd8fdf4f19fa66a272a49649e9884ce6fdcbf571d988677d038", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 13326, "end": 14744}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "47eebb8d-91ef-4aa9-990e-5718c03e843d", "3": "bbf88635-cd9f-462b-acaf-3aed4838e1ad"}}, "__type__": "1"}, "bbf88635-cd9f-462b-acaf-3aed4838e1ad": {"__data__": {"text": "0.8, 4)],\n        [('Color', 0.4, 9), ('Equalize', 0.6, 3)],\n        [('Color', 0.4, 1), ('Rotate', 0.6, 8)],\n        [('Solarize', 0.8, 3), ('Equalize', 0.4, 7)],\n        [('Solarize', 0.4, 2), ('Solarize', 0.6, 2)],\n        [('Color', 0.2, 0), ('Equalize', 0.8, 8)],\n        [('Equalize', 0.4, 8), ('SolarizeAdd', 0.8, 3)],\n        [('ShearX', 0.2, 9), ('Rotate', 0.6, 8)],\n        [('Color', 0.6, 1), ('Equalize', 1.0, 2)],\n        [('Invert', 0.4, 9), ('Rotate', 0.6, 0)],\n        [('Equalize', 1.0, 9), ('ShearY', 0.6, 3)],\n        [('Color', 0.4, 7), ('Equalize', 0.6, 0)],\n        [('PosterizeIncreasing', 0.4, 6), ('AutoContrast', 0.4, 7)],\n        [('Solarize', 0.6, 8), ('Color', 0.6, 9)],\n        [('Solarize', 0.2, 4), ('Rotate', 0.8, 9)],\n        [('Rotate', 1.0, 7), ('TranslateYRel', 0.8, 9)],\n        [('ShearX', 0.0, 0), ('Solarize', 0.8, 4)],\n        [('ShearY', 0.8, 0), ('Color', 0.6, 4)],\n        [('Color', 1.0, 0), ('Rotate', 0.6, 2)],\n        [('Equalize', 0.8, 4), ('Equalize', 0.0, 8)],\n        [('Equalize', 1.0, 4), ('AutoContrast', 0.6, 2)],\n        [('ShearY', 0.4, 7), ('SolarizeAdd', 0.6, 7)],\n        [('PosterizeIncreasing', 0.8, 2), ('Solarize', 0.6, 10)],\n        [('Solarize', 0.6, 8), ('Equalize', 0.6, 1)],\n        [('Color', 0.8, 6),", "doc_id": "bbf88635-cd9f-462b-acaf-3aed4838e1ad", "embedding": null, "doc_hash": "da6baab307a13942e5216b0244aeceafee73f1d859329aa78663dfeca2b171db", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 14742, "end": 16015}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "b0ac782c-40a8-4bfd-aaf6-09a205e5ffc8", "3": "e870ec1c-89c7-4bf9-95d2-476717b255fa"}}, "__type__": "1"}, "e870ec1c-89c7-4bf9-95d2-476717b255fa": {"__data__": {"text": "2), ('Solarize', 0.6, 10)],\n        [('Solarize', 0.6, 8), ('Equalize', 0.6, 1)],\n        [('Color', 0.8, 6), ('Rotate', 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_original(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501\n    policy = [\n        [('PosterizeOriginal', 0.4, 8), ('Rotate', 0.6, 9)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n        [('PosterizeOriginal', 0.6, 7), ('PosterizeOriginal', 0.6, 6)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Equalize', 0.4, 4), ('Rotate', 0.8, 8)],\n        [('Solarize', 0.6, 3), ('Equalize', 0.6, 7)],\n        [('PosterizeOriginal', 0.8, 5), ('Equalize', 1.0, 2)],\n        [('Rotate', 0.2, 3), ('Solarize', 0.6, 8)],\n        [('Equalize', 0.6, 8), ('PosterizeOriginal', 0.4, 6)],\n        [('Rotate', 0.8, 8), ('Color', 0.4, 0)],\n        [('Rotate', 0.4, 9), ('Equalize', 0.6, 2)],\n        [('Equalize', 0.0, 7), ('Equalize', 0.8, 8)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Rotate', 0.8, 8), ('Color', 1.0, 2)],\n        [('Color', 0.8, 8), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6,", "doc_id": "e870ec1c-89c7-4bf9-95d2-476717b255fa", "embedding": null, "doc_hash": "099e020b070e08df14667b913e767c701ed6e7124a08843e176c2bfd8a673d09", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 16011, "end": 17373}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "bbf88635-cd9f-462b-acaf-3aed4838e1ad", "3": "fa2fc4ed-c7a7-4739-9ca6-fe49ed3172b0"}}, "__type__": "1"}, "fa2fc4ed-c7a7-4739-9ca6-fe49ed3172b0": {"__data__": {"text": "8), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_originalr(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501 with research posterize variation\n    policy = [\n        [('PosterizeIncreasing', 0.4, 8), ('Rotate', 0.6, 9)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n        [('PosterizeIncreasing', 0.6, 7), ('PosterizeIncreasing', 0.6, 6)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Equalize', 0.4, 4), ('Rotate', 0.8, 8)],\n        [('Solarize', 0.6, 3), ('Equalize', 0.6, 7)],\n        [('PosterizeIncreasing', 0.8, 5), ('Equalize', 1.0, 2)],\n        [('Rotate', 0.2, 3), ('Solarize', 0.6, 8)],\n        [('Equalize', 0.6, 8), ('PosterizeIncreasing', 0.4, 6)],\n        [('Rotate', 0.8, 8), ('Color', 0.4, 0)],\n        [('Rotate', 0.4, 9), ('Equalize', 0.6,", "doc_id": "fa2fc4ed-c7a7-4739-9ca6-fe49ed3172b0", "embedding": null, "doc_hash": "12671c6b2065b4096f164a6954f01955e32638c941e578957693e6a5141f09d7", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 17378, "end": 18766}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "e870ec1c-89c7-4bf9-95d2-476717b255fa", "3": "e14d3638-fd21-433d-85ad-ac529d57e347"}}, "__type__": "1"}, "e14d3638-fd21-433d-85ad-ac529d57e347": {"__data__": {"text": "0.4, 6)],\n        [('Rotate', 0.8, 8), ('Color', 0.4, 0)],\n        [('Rotate', 0.4, 9), ('Equalize', 0.6, 2)],\n        [('Equalize', 0.0, 7), ('Equalize', 0.8, 8)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Rotate', 0.8, 8), ('Color', 1.0, 2)],\n        [('Color', 0.8, 8), ('Solarize', 0.8, 7)],\n        [('Sharpness', 0.4, 7), ('Invert', 0.6, 8)],\n        [('ShearX', 0.6, 5), ('Equalize', 1.0, 9)],\n        [('Color', 0.4, 0), ('Equalize', 0.6, 3)],\n        [('Equalize', 0.4, 7), ('Solarize', 0.2, 4)],\n        [('Solarize', 0.6, 5), ('AutoContrast', 0.6, 5)],\n        [('Invert', 0.6, 4), ('Equalize', 1.0, 8)],\n        [('Color', 0.6, 4), ('Contrast', 1.0, 8)],\n        [('Equalize', 0.8, 8), ('Equalize', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy(name='v0', hparams=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    if name == 'original':\n        return auto_augment_policy_original(hparams)\n    elif name == 'originalr':\n        return auto_augment_policy_originalr(hparams)\n    elif name == 'v0':\n        return auto_augment_policy_v0(hparams)\n    elif name == 'v0r':\n        return auto_augment_policy_v0r(hparams)\n    else:\n        assert False, 'Unknown AA policy (%s)' % name\n\n\nclass AutoAugment:\n\n    def __init__(self, policy):\n        self.policy =", "doc_id": "e14d3638-fd21-433d-85ad-ac529d57e347", "embedding": null, "doc_hash": "2a63cf1ae01ed17bf3388ccbf91cc22f278fcefb8324841d6bacf6f7cb55389a", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 18767, "end": 20192}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "fa2fc4ed-c7a7-4739-9ca6-fe49ed3172b0", "3": "814dbf86-b67c-4d1b-ad63-ade3f972fd15"}}, "__type__": "1"}, "814dbf86-b67c-4d1b-ad63-ade3f972fd15": {"__data__": {"text": "   else:\n        assert False, 'Unknown AA policy (%s)' % name\n\n\nclass AutoAugment:\n\n    def __init__(self, policy):\n        self.policy = policy\n\n    def __call__(self, img):\n        sub_policy = random.choice(self.policy)\n        for op in sub_policy:\n            img = op(img)\n        return img\n\n    def __repr__(self):\n        fs = self.__class__.__name__ + f'(policy='\n        for p in self.policy:\n            fs += '\\n\\t['\n            fs += ', '.join([str(op) for op in p])\n            fs += ']'\n        fs += ')'\n        return fs\n\n\ndef auto_augment_transform(config_str, hparams):\n    \"\"\"\n    Create a AutoAugment transform\n\n    :param config_str: String defining configuration of auto augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the AutoAugment policy (one of 'v0', 'v0r', 'original', 'originalr').\n    The remaining sections, not order sepecific determine\n        'mstd' -  float std deviation of magnitude noise applied\n    Ex 'original-mstd0.5' results in AutoAugment with original policy, magnitude_std 0.5\n\n    :param hparams: Other hparams (kwargs) for the AutoAugmentation scheme\n\n    :return: A PyTorch compatible Transform\n    \"\"\"\n    config = config_str.split('-')\n    policy_name = config[0]\n    config = config[1:]\n    for c in config:\n        cs = re.split(r'(\\d.*)', c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == 'mstd':\n            # noise param injected via hparams for now\n            hparams.setdefault('magnitude_std', float(val))\n        else:\n            assert False, 'Unknown AutoAugment config section'\n    aa_policy = auto_augment_policy(policy_name, hparams=hparams)\n    return", "doc_id": "814dbf86-b67c-4d1b-ad63-ade3f972fd15", "embedding": null, "doc_hash": "4329b31b8b07b9c72faa398bc0350c71685a676216b3f2796b648579946bf7e2", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 20168, "end": 21885}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "e14d3638-fd21-433d-85ad-ac529d57e347", "3": "a220f99a-22ca-49a3-876d-c14327f109a8"}}, "__type__": "1"}, "a220f99a-22ca-49a3-876d-c14327f109a8": {"__data__": {"text": "       else:\n            assert False, 'Unknown AutoAugment config section'\n    aa_policy = auto_augment_policy(policy_name, hparams=hparams)\n    return AutoAugment(aa_policy)\n\n\n_RAND_TRANSFORMS = [\n    'AutoContrast',\n    'Equalize',\n    'Invert',\n    'Rotate',\n    'Posterize',\n    'Solarize',\n    'SolarizeAdd',\n    'Color',\n    'Contrast',\n    'Brightness',\n    'Sharpness',\n    'ShearX',\n    'ShearY',\n    'TranslateXRel',\n    'TranslateYRel',\n    #'Cutout'  # NOTE I've implement this as random erasing separately\n]\n\n\n_RAND_INCREASING_TRANSFORMS = [\n    'AutoContrast',\n    'Equalize',\n    'Invert',\n    'Rotate',\n    'PosterizeIncreasing',\n    'SolarizeIncreasing',\n    'SolarizeAdd',\n    'ColorIncreasing',\n    'ContrastIncreasing',\n    'BrightnessIncreasing',\n    'SharpnessIncreasing',\n    'ShearX',\n    'ShearY',\n    'TranslateXRel',\n    'TranslateYRel',\n    #'Cutout'  # NOTE I've implement this as random erasing separately\n]\n\n\n\n# These experimental weights are based loosely on the relative improvements mentioned in paper.\n# They may not result in increased performance, but could likely be tuned to so.\n_RAND_CHOICE_WEIGHTS_0 = {\n    'Rotate': 0.3,\n    'ShearX': 0.2,\n    'ShearY': 0.2,\n    'TranslateXRel': 0.1,\n    'TranslateYRel': 0.1,\n    'Color': .025,\n    'Sharpness': 0.025,\n    'AutoContrast': 0.025,\n    'Solarize': .005,\n    'SolarizeAdd': .005,\n    'Contrast': .005,\n    'Brightness': .005,\n    'Equalize': .005,\n    'Posterize': 0,\n    'Invert': 0,\n}\n\n\ndef _select_rand_weights(weight_idx=0, transforms=None):\n    transforms = transforms or _RAND_TRANSFORMS\n    assert weight_idx == 0  # only one set of weights currently\n    rand_weights = _RAND_CHOICE_WEIGHTS_0\n", "doc_id": "a220f99a-22ca-49a3-876d-c14327f109a8", "embedding": null, "doc_hash": "b55b1d0ca9bd782cdf26bbca5da7b65fcae71d188d58dff42262f4eee6c31f6b", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 21873, "end": 23565}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "814dbf86-b67c-4d1b-ad63-ade3f972fd15", "3": "5fe951b4-00ce-4ede-a3d5-fd278561704d"}}, "__type__": "1"}, "5fe951b4-00ce-4ede-a3d5-fd278561704d": {"__data__": {"text": "transforms=None):\n    transforms = transforms or _RAND_TRANSFORMS\n    assert weight_idx == 0  # only one set of weights currently\n    rand_weights = _RAND_CHOICE_WEIGHTS_0\n    probs = [rand_weights[k] for k in transforms]\n    probs /= np.sum(probs)\n    return probs\n\n\ndef rand_augment_ops(magnitude=10, hparams=None, transforms=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _RAND_TRANSFORMS\n    return [AugmentOp(\n        name, prob=0.5, magnitude=magnitude, hparams=hparams) for name in transforms]\n\n\nclass RandAugment:\n    def __init__(self, ops, num_layers=2, choice_weights=None):\n        self.ops = ops\n        self.num_layers = num_layers\n        self.choice_weights = choice_weights\n\n    def __call__(self, img):\n        # no replacement when using weighted choice\n        ops = np_random.choice(\n            self.ops, self.num_layers, replace=self.choice_weights is None, p=self.choice_weights)\n        for op in ops:\n            img = op(img)\n        return img\n\n    def __repr__(self):\n        fs = self.__class__.__name__ + f'(n={self.num_layers}, ops='\n        for op in self.ops:\n            fs += f'\\n\\t{op}'\n        fs += ')'\n        return fs\n\n\ndef rand_augment_transform(config_str, hparams):\n    \"\"\"\n    Create a RandAugment transform\n\n    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand'). The remaining\n    sections, not order sepecific determine\n        'm' - integer magnitude of rand augment\n        'n' - integer num layers (number of transform ops selected per image)\n        'w' - integer probabiliy weight index (index of a set of weights to influence choice of op)\n        'mstd' -  float std deviation of magnitude noise", "doc_id": "5fe951b4-00ce-4ede-a3d5-fd278561704d", "embedding": null, "doc_hash": "e426db9e685a4ace00b9574677fd9effaeb7ab991db7d7991ea475d850a3d4b5", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 23540, "end": 25394}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "a220f99a-22ca-49a3-876d-c14327f109a8", "3": "6b572289-a953-4bbd-b667-9c82623f4dc8"}}, "__type__": "1"}, "6b572289-a953-4bbd-b667-9c82623f4dc8": {"__data__": {"text": "of transform ops selected per image)\n        'w' - integer probabiliy weight index (index of a set of weights to influence choice of op)\n        'mstd' -  float std deviation of magnitude noise applied, or uniform sampling if infinity (or > 100)\n        'mmax' - set upper bound for magnitude to something other than default of  _LEVEL_DENOM (10)\n        'inc' - integer (bool), use augmentations that increase in severity with magnitude (default: 0)\n    Ex 'rand-m9-n3-mstd0.5' results in RandAugment with magnitude 9, num_layers 3, magnitude_std 0.5\n    'rand-mstd1-w0' results in magnitude_std 1.0, weights 0, default magnitude of 10 and num_layers 2\n\n    :param hparams: Other hparams (kwargs) for the RandAugmentation scheme\n\n    :return: A PyTorch compatible Transform\n    \"\"\"\n    magnitude = _LEVEL_DENOM  # default to _LEVEL_DENOM for magnitude (currently 10)\n    num_layers = 2  # default to 2 ops per image\n    weight_idx = None  # default to no probability weights for op choice\n    transforms = _RAND_TRANSFORMS\n    config = config_str.split('-')\n    assert config[0] == 'rand'\n    config = config[1:]\n    for c in config:\n        cs = re.split(r'(\\d.*)', c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == 'mstd':\n            # noise param / randomization of magnitude values\n            mstd = float(val)\n            if mstd > 100:\n                # use uniform sampling in 0 to magnitude if mstd is > 100\n                mstd = float('inf')\n            hparams.setdefault('magnitude_std', mstd)\n        elif key == 'mmax':\n            # clip magnitude between [0, mmax] instead of default [0, _LEVEL_DENOM]\n            hparams.setdefault('magnitude_max', int(val))\n        elif key == 'inc':\n  ", "doc_id": "6b572289-a953-4bbd-b667-9c82623f4dc8", "embedding": null, "doc_hash": "0e8d7ea71173f3ad76dd90f6bc1f97739377dc7494d48e0b52cdbd301e79f6fb", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 25390, "end": 27140}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "5fe951b4-00ce-4ede-a3d5-fd278561704d", "3": "ced66210-a50f-4d41-ba13-aa02b5ac0c9f"}}, "__type__": "1"}, "ced66210-a50f-4d41-ba13-aa02b5ac0c9f": {"__data__": {"text": "mmax] instead of default [0, _LEVEL_DENOM]\n            hparams.setdefault('magnitude_max', int(val))\n        elif key == 'inc':\n            if bool(val):\n                transforms = _RAND_INCREASING_TRANSFORMS\n        elif key == 'm':\n            magnitude = int(val)\n        elif key == 'n':\n            num_layers = int(val)\n        elif key == 'w':\n            weight_idx = int(val)\n        else:\n            assert False, 'Unknown RandAugment config section'\n    ra_ops = rand_augment_ops(magnitude=magnitude, hparams=hparams, transforms=transforms)\n    choice_weights = None if weight_idx is None else _select_rand_weights(weight_idx)\n    return RandAugment(ra_ops, num_layers, choice_weights=choice_weights)\n\n\n_AUGMIX_TRANSFORMS = [\n    'AutoContrast',\n    'ColorIncreasing',  # not in paper\n    'ContrastIncreasing',  # not in paper\n    'BrightnessIncreasing',  # not in paper\n    'SharpnessIncreasing',  # not in paper\n    'Equalize',\n    'Rotate',\n    'PosterizeIncreasing',\n    'SolarizeIncreasing',\n    'ShearX',\n    'ShearY',\n    'TranslateXRel',\n    'TranslateYRel',\n]\n\n\ndef augmix_ops(magnitude=10, hparams=None, transforms=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _AUGMIX_TRANSFORMS\n    return [AugmentOp(\n        name, prob=1.0, magnitude=magnitude, hparams=hparams) for name in transforms]\n\n\nclass AugMixAugment:\n    \"\"\" AugMix Transform\n    Adapted and improved from impl here: https://github.com/google-research/augmix/blob/master/imagenet.py\n    From paper: 'AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty -\n    https://arxiv.org/abs/1912.02781\n    \"\"\"\n    def __init__(self, ops, alpha=1., width=3, depth=-1,", "doc_id": "ced66210-a50f-4d41-ba13-aa02b5ac0c9f", "embedding": null, "doc_hash": "44d751fad7bb69a8817c3fd3b879cfa0e07dd88bd9c2008e04bdd8d3e607cd33", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 27190, "end": 28890}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "6b572289-a953-4bbd-b667-9c82623f4dc8", "3": "5b1d9ee3-8e67-4b44-888b-e2ebcb791882"}}, "__type__": "1"}, "5b1d9ee3-8e67-4b44-888b-e2ebcb791882": {"__data__": {"text": "Simple Data Processing Method to Improve Robustness and Uncertainty -\n    https://arxiv.org/abs/1912.02781\n    \"\"\"\n    def __init__(self, ops, alpha=1., width=3, depth=-1, blended=False):\n        self.ops = ops\n        self.alpha = alpha\n        self.width = width\n        self.depth = depth\n        self.blended = blended  # blended mode is faster but not well tested\n\n    def _calc_blended_weights(self, ws, m):\n        ws = ws * m\n        cump = 1.\n        rws = []\n        for w in ws[::-1]:\n            alpha = w / cump\n            cump *= (1 - alpha)\n            rws.append(alpha)\n        return np.array(rws[::-1], dtype=np.float32)\n\n    def _apply_blended(self, img, mixing_weights, m):\n        # This is my first crack and implementing a slightly faster mixed augmentation. Instead\n        # of accumulating the mix for each chain in a Numpy array and then blending with original,\n        # it recomputes the blending coefficients and applies one PIL image blend per chain.\n        # TODO the results appear in the right ballpark but they differ by more than rounding.\n        img_orig = img.copy()\n        ws = self._calc_blended_weights(mixing_weights, m)\n        for w in ws:\n            depth = self.depth if self.depth > 0 else np_random.randint(1, 4)\n            ops = np_random.choice(self.ops, depth, replace=True)\n            img_aug = img_orig  # no ops are in-place, deep copy not necessary\n            for op in ops:\n                img_aug = op(img_aug)\n            img = Image.blend(img, img_aug, w)\n        return img\n\n    def _apply_basic(self, img, mixing_weights, m):\n        # This is a literal adaptation of the paper/official implementation without normalizations and\n", "doc_id": "5b1d9ee3-8e67-4b44-888b-e2ebcb791882", "embedding": null, "doc_hash": "0f1e271c9918dd87a74a3ec6aa37c9f40f795fb16f4545a9d600d7409fd5e205", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 28845, "end": 30543}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "ced66210-a50f-4d41-ba13-aa02b5ac0c9f", "3": "305a13de-a0cf-4aa7-9b42-e37d8872190c"}}, "__type__": "1"}, "305a13de-a0cf-4aa7-9b42-e37d8872190c": {"__data__": {"text": "w)\n        return img\n\n    def _apply_basic(self, img, mixing_weights, m):\n        # This is a literal adaptation of the paper/official implementation without normalizations and\n        # PIL <-> Numpy conversions between every op. It is still quite CPU compute heavy compared to the\n        # typical augmentation transforms, could use a GPU / Kornia implementation.\n        img_shape = img.size[0], img.size[1], len(img.getbands())\n        mixed = np.zeros(img_shape, dtype=np.float32)\n        for mw in mixing_weights:\n            depth = self.depth if self.depth > 0 else np_random.randint(1, 4)\n            ops = np_random.choice(self.ops, depth, replace=True)\n            img_aug = img  # no ops are in-place, deep copy not necessary\n            for op in ops:\n                img_aug = op(img_aug)\n            mixed += mw * np.asarray(img_aug, dtype=np.float32)\n        np.clip(mixed, 0, 255., out=mixed)\n        mixed = Image.fromarray(mixed.astype(np.uint8))\n        return Image.blend(img, mixed, m)\n\n    def __call__(self, img):\n        mixing_weights = np.float32(np_random.dirichlet([self.alpha] * self.width))\n        m = np.float32(np_random.beta(self.alpha, self.alpha))\n        if self.blended:\n            mixed = self._apply_blended(img, mixing_weights, m)\n        else:\n            mixed = self._apply_basic(img, mixing_weights, m)\n        return mixed\n\n    def __repr__(self):\n        fs = self.__class__.__name__ + f'(alpha={self.alpha}, width={self.width}, depth={self.depth}, ops='\n        for op in self.ops:\n            fs += f'\\n\\t{op}'\n        fs += ')'\n        return fs\n\n\ndef", "doc_id": "305a13de-a0cf-4aa7-9b42-e37d8872190c", "embedding": null, "doc_hash": "22a8e2124408b8cb1798601828235cb6b0fc723736f23018cf92a1ca7cae3f88", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 30548, "end": 32153}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "5b1d9ee3-8e67-4b44-888b-e2ebcb791882", "3": "641e91e0-ca24-4a61-972e-4e43c7b79b17"}}, "__type__": "1"}, "641e91e0-ca24-4a61-972e-4e43c7b79b17": {"__data__": {"text": "       for op in self.ops:\n            fs += f'\\n\\t{op}'\n        fs += ')'\n        return fs\n\n\ndef augment_and_mix_transform(config_str, hparams):\n    \"\"\" Create AugMix PyTorch transform\n\n    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand'). The remaining\n    sections, not order sepecific determine\n        'm' - integer magnitude (severity) of augmentation mix (default: 3)\n        'w' - integer width of augmentation chain (default: 3)\n        'd' - integer depth of augmentation chain (-1 is random [1, 3], default: -1)\n        'b' - integer (bool), blend each branch of chain into end result without a final blend, less CPU (default: 0)\n        'mstd' -  float std deviation of magnitude noise applied (default: 0)\n    Ex 'augmix-m5-w4-d2' results in AugMix with severity 5, chain width 4, chain depth 2\n\n    :param hparams: Other hparams (kwargs) for the Augmentation transforms\n\n    :return: A PyTorch compatible Transform\n    \"\"\"\n    magnitude = 3\n    width = 3\n    depth = -1\n    alpha = 1.\n    blended = False\n    config = config_str.split('-')\n    assert config[0] == 'augmix'\n    config = config[1:]\n    for c in config:\n        cs = re.split(r'(\\d.*)', c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == 'mstd':\n            # noise param injected via hparams for now\n            hparams.setdefault('magnitude_std', float(val))\n        elif key == 'm':\n            magnitude = int(val)\n        elif key == 'w':\n            width = int(val)\n        elif key == 'd':\n            depth = int(val)\n        elif key == 'a':\n      ", "doc_id": "641e91e0-ca24-4a61-972e-4e43c7b79b17", "embedding": null, "doc_hash": "e808ccb7a93317d47fb8fe3d5e86bbf4c77cd9463f7201f7b753d422fe5c8a39", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 32240, "end": 33990}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "305a13de-a0cf-4aa7-9b42-e37d8872190c", "3": "648760d1-9933-44a9-a969-81c4a28f1be5"}}, "__type__": "1"}, "648760d1-9933-44a9-a969-81c4a28f1be5": {"__data__": {"text": "    width = int(val)\n        elif key == 'd':\n            depth = int(val)\n        elif key == 'a':\n            alpha = float(val)\n        elif key == 'b':\n            blended = bool(val)\n        else:\n            assert False, 'Unknown AugMix config section'\n    hparams.setdefault('magnitude_std', float('inf'))  # default to uniform sampling (if not set via mstd arg)\n    ops = augmix_ops(magnitude=magnitude, hparams=hparams)\n    return AugMixAugment(ops, alpha=alpha, width=width, depth=depth, blended=blended)\n", "doc_id": "648760d1-9933-44a9-a969-81c4a28f1be5", "embedding": null, "doc_hash": "5441253a6517456495691e4b4609ce009e7f4832eb2e030e6121eb0c0d7b28e3", "extra_info": {"file_path": "TinyViT/data/augmentation/auto_augment.py", "file_name": "auto_augment.py"}, "node_info": {"start": 33939, "end": 34455}, "relationships": {"1": "8e2cd24b26ca26e258c0a6467618f9276d76d7ea", "2": "641e91e0-ca24-4a61-972e-4e43c7b79b17"}}, "__type__": "1"}, "539bfd56-684c-4635-b1b7-15a16f5c6ed5": {"__data__": {"text": "import logging\nfrom .constants import *\n\n\n_logger = logging.getLogger(__name__)\n\n\ndef resolve_data_config(args, default_cfg={}, model=None, use_test_size=False, verbose=False):\n    new_config = {}\n    default_cfg = default_cfg\n    if not default_cfg and model is not None and hasattr(model, 'default_cfg'):\n        default_cfg = model.default_cfg\n\n    # Resolve input/image size\n    in_chans = 3\n    if 'chans' in args and args['chans'] is not None:\n        in_chans = args['chans']\n\n    input_size = (in_chans, 224, 224)\n    if 'input_size' in args and args['input_size'] is not None:\n        assert isinstance(args['input_size'], (tuple, list))\n        assert len(args['input_size']) == 3\n        input_size = tuple(args['input_size'])\n        in_chans = input_size[0]  # input_size overrides in_chans\n    elif 'img_size' in args and args['img_size'] is not None:\n        assert isinstance(args['img_size'], int)\n        input_size = (in_chans, args['img_size'], args['img_size'])\n    else:\n        if use_test_size and 'test_input_size' in default_cfg:\n            input_size = default_cfg['test_input_size']\n        elif 'input_size' in default_cfg:\n            input_size = default_cfg['input_size']\n    new_config['input_size'] = input_size\n\n    # resolve interpolation method\n    new_config['interpolation'] = 'bicubic'\n    if 'interpolation' in args and args['interpolation']:\n        new_config['interpolation'] = args['interpolation']\n    elif 'interpolation' in default_cfg:\n        new_config['interpolation'] = default_cfg['interpolation']\n\n    # resolve dataset + model mean for normalization\n    new_config['mean'] = IMAGENET_DEFAULT_MEAN\n    if 'mean' in args and args['mean'] is not None:\n        mean = tuple(args['mean'])\n        if len(mean) == 1:\n            mean =", "doc_id": "539bfd56-684c-4635-b1b7-15a16f5c6ed5", "embedding": null, "doc_hash": "ede7d9588ee5637f99f2d2e9b5e4a74e6f31402d8658414a0398ce1231f1c523", "extra_info": {"file_path": "TinyViT/data/augmentation/config.py", "file_name": "config.py"}, "node_info": {"start": 0, "end": 1786}, "relationships": {"1": "38f5689a707f5602e38cb717ed6115f26a0d7ea2", "3": "30ac438d-5302-4574-9475-3c5fc7aeb259"}}, "__type__": "1"}, "30ac438d-5302-4574-9475-3c5fc7aeb259": {"__data__": {"text": " if 'mean' in args and args['mean'] is not None:\n        mean = tuple(args['mean'])\n        if len(mean) == 1:\n            mean = tuple(list(mean) * in_chans)\n        else:\n            assert len(mean) == in_chans\n        new_config['mean'] = mean\n    elif 'mean' in default_cfg:\n        new_config['mean'] = default_cfg['mean']\n\n    # resolve dataset + model std deviation for normalization\n    new_config['std'] = IMAGENET_DEFAULT_STD\n    if 'std' in args and args['std'] is not None:\n        std = tuple(args['std'])\n        if len(std) == 1:\n            std = tuple(list(std) * in_chans)\n        else:\n            assert len(std) == in_chans\n        new_config['std'] = std\n    elif 'std' in default_cfg:\n        new_config['std'] = default_cfg['std']\n\n    # resolve default crop percentage\n    new_config['crop_pct'] = DEFAULT_CROP_PCT\n    if 'crop_pct' in args and args['crop_pct'] is not None:\n        new_config['crop_pct'] = args['crop_pct']\n    elif 'crop_pct' in default_cfg:\n        new_config['crop_pct'] = default_cfg['crop_pct']\n\n    if verbose:\n        _logger.info('Data processing configuration for current model + dataset:')\n        for n, v in new_config.items():\n            _logger.info('\\t%s: %s' % (n, str(v)))\n\n    return new_config\n", "doc_id": "30ac438d-5302-4574-9475-3c5fc7aeb259", "embedding": null, "doc_hash": "bd33a52a6fb771b49954a91b6c49a1b8c6dbc63487d0e4b09a3c800679db2981", "extra_info": {"file_path": "TinyViT/data/augmentation/config.py", "file_name": "config.py"}, "node_info": {"start": 1657, "end": 2915}, "relationships": {"1": "38f5689a707f5602e38cb717ed6115f26a0d7ea2", "2": "539bfd56-684c-4635-b1b7-15a16f5c6ed5"}}, "__type__": "1"}, "221304b4-db53-49fa-b8dd-fd51f5bc3da1": {"__data__": {"text": "DEFAULT_CROP_PCT = 0.875\nIMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\nIMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\nIMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\nIMAGENET_DPN_MEAN = (124 / 255, 117 / 255, 104 / 255)\nIMAGENET_DPN_STD = tuple([1 / (.0167 * 255)] * 3)\n", "doc_id": "221304b4-db53-49fa-b8dd-fd51f5bc3da1", "embedding": null, "doc_hash": "ed35710b20551fb2a689386e49f6e5ab1cb1679c6252bd516a7b5a1fb5842235", "extra_info": {"file_path": "TinyViT/data/augmentation/constants.py", "file_name": "constants.py"}, "node_info": {"start": 0, "end": 303}, "relationships": {"1": "d6d4a01b0316989a3f5142167f1e384b098bc930"}}, "__type__": "1"}, "04b17466-e501-4e79-b28d-ceaa306a8d42": {"__data__": {"text": "\"\"\" Quick n Simple Image Folder, Tarfile based DataSet\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport torch.utils.data as data\nimport os\nimport torch\nimport logging\n\nfrom PIL import Image\n\nfrom .parsers import create_parser\n\n_logger = logging.getLogger(__name__)\n\n\n_ERROR_RETRY = 50\n\n\nclass ImageDataset(data.Dataset):\n\n    def __init__(\n            self,\n            root,\n            parser=None,\n            class_map=None,\n            load_bytes=False,\n            transform=None,\n            target_transform=None,\n    ):\n        if parser is None or isinstance(parser, str):\n            parser = create_parser(parser or '', root=root, class_map=class_map)\n        self.parser = parser\n        self.load_bytes = load_bytes\n        self.transform = transform\n        self.target_transform = target_transform\n        self._consecutive_errors = 0\n\n    def __getitem__(self, index):\n        img, target = self.parser[index]\n        try:\n            img = img.read() if self.load_bytes else Image.open(img).convert('RGB')\n        except Exception as e:\n            _logger.warning(f'Skipped sample (index {index}, file {self.parser.filename(index)}). {str(e)}')\n            self._consecutive_errors += 1\n            if self._consecutive_errors < _ERROR_RETRY:\n                return self.__getitem__((index + 1) % len(self.parser))\n            else:\n                raise e\n        self._consecutive_errors = 0\n        if self.transform is not None:\n            img = self.transform(img)\n        if target is None:\n            target = -1\n        elif self.target_transform is not None:\n            target =", "doc_id": "04b17466-e501-4e79-b28d-ceaa306a8d42", "embedding": null, "doc_hash": "815e74673653e1dff03588d9e5a50196cec0802b9295d5d2c8a23e1b691602d3", "extra_info": {"file_path": "TinyViT/data/augmentation/dataset.py", "file_name": "dataset.py"}, "node_info": {"start": 0, "end": 1622}, "relationships": {"1": "d3603a233d98d3e390dfa6aaaa13ef40099fe4ab", "3": "d324bfe8-bfb6-49aa-a867-d975022178d9"}}, "__type__": "1"}, "d324bfe8-bfb6-49aa-a867-d975022178d9": {"__data__": {"text": "       if target is None:\n            target = -1\n        elif self.target_transform is not None:\n            target = self.target_transform(target)\n        return img, target\n\n    def __len__(self):\n        return len(self.parser)\n\n    def filename(self, index, basename=False, absolute=False):\n        return self.parser.filename(index, basename, absolute)\n\n    def filenames(self, basename=False, absolute=False):\n        return self.parser.filenames(basename, absolute)\n\n\nclass IterableImageDataset(data.IterableDataset):\n\n    def __init__(\n            self,\n            root,\n            parser=None,\n            split='train',\n            is_training=False,\n            batch_size=None,\n            repeats=0,\n            download=False,\n            transform=None,\n            target_transform=None,\n    ):\n        assert parser is not None\n        if isinstance(parser, str):\n            self.parser = create_parser(\n                parser, root=root, split=split, is_training=is_training,\n                batch_size=batch_size, repeats=repeats, download=download)\n        else:\n            self.parser = parser\n        self.transform = transform\n        self.target_transform = target_transform\n        self._consecutive_errors = 0\n\n    def __iter__(self):\n        for img, target in self.parser:\n            if self.transform is not None:\n                img = self.transform(img)\n            if self.target_transform is not None:\n                target = self.target_transform(target)\n            yield img, target\n\n    def __len__(self):\n        if", "doc_id": "d324bfe8-bfb6-49aa-a867-d975022178d9", "embedding": null, "doc_hash": "ca9872e8185a18547e5a90bf53f98700d26c0561b6b82b99a8a635155c6c2606", "extra_info": {"file_path": "TinyViT/data/augmentation/dataset.py", "file_name": "dataset.py"}, "node_info": {"start": 1554, "end": 3114}, "relationships": {"1": "d3603a233d98d3e390dfa6aaaa13ef40099fe4ab", "2": "04b17466-e501-4e79-b28d-ceaa306a8d42", "3": "62cf7729-e7bc-45dd-a70f-375233356f91"}}, "__type__": "1"}, "62cf7729-e7bc-45dd-a70f-375233356f91": {"__data__": {"text": "               target = self.target_transform(target)\n            yield img, target\n\n    def __len__(self):\n        if hasattr(self.parser, '__len__'):\n            return len(self.parser)\n        else:\n            return 0\n\n    def filename(self, index, basename=False, absolute=False):\n        assert False, 'Filename lookup by index not supported, use filenames().'\n\n    def filenames(self, basename=False, absolute=False):\n        return self.parser.filenames(basename, absolute)\n\n\nclass AugMixDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset wrapper to perform AugMix or other clean/augmentation mixes\"\"\"\n\n    def __init__(self, dataset, num_splits=2):\n        self.augmentation = None\n        self.normalize = None\n        self.dataset = dataset\n        if self.dataset.transform is not None:\n            self._set_transforms(self.dataset.transform)\n        self.num_splits = num_splits\n\n    def _set_transforms(self, x):\n        assert isinstance(x, (list, tuple)) and len(x) == 3, 'Expecting a tuple/list of 3 transforms'\n        self.dataset.transform = x[0]\n        self.augmentation = x[1]\n        self.normalize = x[2]\n\n    @property\n    def transform(self):\n        return self.dataset.transform\n\n    @transform.setter\n    def transform(self, x):\n        self._set_transforms(x)\n\n    def _normalize(self, x):\n        return x if self.normalize is None else self.normalize(x)\n\n    def __getitem__(self, i):\n        x, y = self.dataset[i]  # all splits share the same dataset base transform\n        x_list = [self._normalize(x)]  # first split only normalizes (this is the 'clean' split)\n        # run the full augmentation on the remaining splits\n        for _ in range(self.num_splits - 1):\n           ", "doc_id": "62cf7729-e7bc-45dd-a70f-375233356f91", "embedding": null, "doc_hash": "d74519ba549237a8760ba7b9faf999821d19521c6d6cfc4be4f13e8087e85496", "extra_info": {"file_path": "TinyViT/data/augmentation/dataset.py", "file_name": "dataset.py"}, "node_info": {"start": 3110, "end": 4825}, "relationships": {"1": "d3603a233d98d3e390dfa6aaaa13ef40099fe4ab", "2": "d324bfe8-bfb6-49aa-a867-d975022178d9", "3": "98ecca9c-0fd6-4349-8500-4885a80ffbda"}}, "__type__": "1"}, "98ecca9c-0fd6-4349-8500-4885a80ffbda": {"__data__": {"text": "(this is the 'clean' split)\n        # run the full augmentation on the remaining splits\n        for _ in range(self.num_splits - 1):\n            x_list.append(self._normalize(self.augmentation(x)))\n        return tuple(x_list), y\n\n    def __len__(self):\n        return len(self.dataset)\n", "doc_id": "98ecca9c-0fd6-4349-8500-4885a80ffbda", "embedding": null, "doc_hash": "13d84650c82042d7e96c877ee03174abd6ec6e2c240fbb263783bc129ab827d4", "extra_info": {"file_path": "TinyViT/data/augmentation/dataset.py", "file_name": "dataset.py"}, "node_info": {"start": 4755, "end": 5042}, "relationships": {"1": "d3603a233d98d3e390dfa6aaaa13ef40099fe4ab", "2": "62cf7729-e7bc-45dd-a70f-375233356f91"}}, "__type__": "1"}, "73a540a6-48cb-414f-b781-b9a1a8560143": {"__data__": {"text": "import os\n\n# from torchvision.datasets import CIFAR100, CIFAR10, MNIST, QMNIST, KMNIST, FashionMNIST, ImageNet, ImageFolder\nfrom torchvision.datasets import CIFAR100, CIFAR10, MNIST, KMNIST, FashionMNIST, ImageFolder\ntry:\n    from torchvision.datasets import Places365\n    has_places365 = True\nexcept ImportError:\n    has_places365 = False\ntry:\n    from torchvision.datasets import INaturalist\n    has_inaturalist = True\nexcept ImportError:\n    has_inaturalist = False\n\nfrom .dataset import IterableImageDataset, ImageDataset\n\n_TORCH_BASIC_DS = dict(\n    cifar10=CIFAR10,\n    cifar100=CIFAR100,\n    mnist=MNIST,\n    #qmist=QMNIST,\n    kmnist=KMNIST,\n    fashion_mnist=FashionMNIST,\n)\n_TRAIN_SYNONYM = {'train', 'training'}\n_EVAL_SYNONYM = {'val', 'valid', 'validation', 'eval', 'evaluation'}\n\n\ndef _search_split(root, split):\n    # look for sub-folder with name of split in root and use that if it exists\n    split_name = split.split('[')[0]\n    try_root = os.path.join(root, split_name)\n    if os.path.exists(try_root):\n        return try_root\n\n    def _try(syn):\n        for s in syn:\n            try_root = os.path.join(root, s)\n            if os.path.exists(try_root):\n                return try_root\n        return root\n    if split_name in _TRAIN_SYNONYM:\n        root = _try(_TRAIN_SYNONYM)\n    elif split_name in _EVAL_SYNONYM:\n        root = _try(_EVAL_SYNONYM)\n    return root\n\n\ndef create_dataset(\n        name,\n        root,\n        split='validation',\n        search_split=True,\n        class_map=None,\n        load_bytes=False,\n        is_training=False,\n        download=False,\n    ", "doc_id": "73a540a6-48cb-414f-b781-b9a1a8560143", "embedding": null, "doc_hash": "fc7acc3c067e98b7bdd9f15cfdfda2415aa7c22487d41993da32c83e6f717f70", "extra_info": {"file_path": "TinyViT/data/augmentation/dataset_factory.py", "file_name": "dataset_factory.py"}, "node_info": {"start": 0, "end": 1597}, "relationships": {"1": "9cf00d6c245d6b46fc5d4361e291f58af9b9c0c3", "3": "9d4bdd0b-320b-4b4b-941d-c912ae2573e8"}}, "__type__": "1"}, "9d4bdd0b-320b-4b4b-941d-c912ae2573e8": {"__data__": {"text": "       class_map=None,\n        load_bytes=False,\n        is_training=False,\n        download=False,\n        batch_size=None,\n        repeats=0,\n        **kwargs\n):\n    \"\"\" Dataset factory method\n\n    In parenthesis after each arg are the type of dataset supported for each arg, one of:\n      * folder - default, timm folder (or tar) based ImageDataset\n      * torch - torchvision based datasets\n      * TFDS - Tensorflow-datasets wrapper in IterabeDataset interface via IterableImageDataset\n      * all - any of the above\n\n    Args:\n        name: dataset name, empty is okay for folder based datasets\n        root: root folder of dataset (all)\n        split: dataset split (all)\n        search_split: search for split specific child fold from root so one can specify\n            `imagenet/` instead of `/imagenet/val`, etc on cmd line / config. (folder, torch/folder)\n        class_map: specify class -> index mapping via text file or dict (folder)\n        load_bytes: load data, return images as undecoded bytes (folder)\n        download: download dataset if not present and supported (TFDS, torch)\n        is_training: create dataset in train mode, this is different from the split.\n            For Iterable / TDFS it enables shuffle, ignored for other datasets. (TFDS)\n        batch_size: batch size hint for (TFDS)\n        repeats: dataset repeats per iteration i.e. epoch (TFDS)\n        **kwargs: other args to pass to dataset\n\n    Returns:\n        Dataset object\n    \"\"\"\n    name = name.lower()\n    if name.startswith('torch/'):\n        name = name.split('/', 2)[-1]\n        torch_kwargs = dict(root=root, download=download, **kwargs)\n        if name in _TORCH_BASIC_DS:\n            ds_class = _TORCH_BASIC_DS[name]\n            use_train = split in", "doc_id": "9d4bdd0b-320b-4b4b-941d-c912ae2573e8", "embedding": null, "doc_hash": "704d654b9dc7d159b87fcf2197b5d7f645b967104c0138559e6820f1c16ed582", "extra_info": {"file_path": "TinyViT/data/augmentation/dataset_factory.py", "file_name": "dataset_factory.py"}, "node_info": {"start": 1529, "end": 3283}, "relationships": {"1": "9cf00d6c245d6b46fc5d4361e291f58af9b9c0c3", "2": "73a540a6-48cb-414f-b781-b9a1a8560143", "3": "1d723cd4-5557-4cda-9117-9416abfe3162"}}, "__type__": "1"}, "1d723cd4-5557-4cda-9117-9416abfe3162": {"__data__": {"text": "     if name in _TORCH_BASIC_DS:\n            ds_class = _TORCH_BASIC_DS[name]\n            use_train = split in _TRAIN_SYNONYM\n            ds = ds_class(train=use_train, **torch_kwargs)\n        elif name == 'inaturalist' or name == 'inat':\n            assert has_inaturalist, 'Please update to PyTorch 1.10, torchvision 0.11+ for Inaturalist'\n            target_type = 'full'\n            split_split = split.split('/')\n            if len(split_split) > 1:\n                target_type = split_split[0].split('_')\n                if len(target_type) == 1:\n                    target_type = target_type[0]\n                split = split_split[-1]\n            if split in _TRAIN_SYNONYM:\n                split = '2021_train'\n            elif split in _EVAL_SYNONYM:\n                split = '2021_valid'\n            ds = INaturalist(version=split, target_type=target_type, **torch_kwargs)\n        elif name == 'places365':\n            assert has_places365, 'Please update to a newer PyTorch and torchvision for Places365 dataset.'\n            if split in _TRAIN_SYNONYM:\n                split = 'train-standard'\n            elif split in _EVAL_SYNONYM:\n                split = 'val'\n            ds = Places365(split=split, **torch_kwargs)\n        elif name == 'imagenet':\n            if split in _EVAL_SYNONYM:\n                split = 'val'\n            ds", "doc_id": "1d723cd4-5557-4cda-9117-9416abfe3162", "embedding": null, "doc_hash": "6e6d9e0a4f72791e8cb1acde72d7056a2bb8464a93ce20bfa2664b60e78accc3", "extra_info": {"file_path": "TinyViT/data/augmentation/dataset_factory.py", "file_name": "dataset_factory.py"}, "node_info": {"start": 3280, "end": 4628}, "relationships": {"1": "9cf00d6c245d6b46fc5d4361e291f58af9b9c0c3", "2": "9d4bdd0b-320b-4b4b-941d-c912ae2573e8", "3": "95ff0e9f-afb1-4ab3-b476-19ab30cfa108"}}, "__type__": "1"}, "95ff0e9f-afb1-4ab3-b476-19ab30cfa108": {"__data__": {"text": "== 'imagenet':\n            if split in _EVAL_SYNONYM:\n                split = 'val'\n            ds = ImageNet(split=split, **torch_kwargs)\n        elif name == 'image_folder' or name == 'folder':\n            # in case torchvision ImageFolder is preferred over timm ImageDataset for some reason\n            if search_split and os.path.isdir(root):\n                # look for split specific sub-folder in root\n                root = _search_split(root, split)\n            ds = ImageFolder(root, **kwargs)\n        else:\n            assert False, f\"Unknown torchvision dataset {name}\"\n    elif name.startswith('tfds/'):\n        ds = IterableImageDataset(\n            root, parser=name, split=split, is_training=is_training,\n            download=download, batch_size=batch_size, repeats=repeats, **kwargs)\n    else:\n        # FIXME support more advance split cfg for ImageFolder/Tar datasets in the future\n        if search_split and os.path.isdir(root):\n            # look for split specific sub-folder in root\n            root = _search_split(root, split)\n        ds = ImageDataset(root, parser=name, class_map=class_map, load_bytes=load_bytes, **kwargs)\n    return ds\n", "doc_id": "95ff0e9f-afb1-4ab3-b476-19ab30cfa108", "embedding": null, "doc_hash": "347e06950e53bca927248ac2afb8bcf79ecb6131e8eb8acd3990be8fb66efa78", "extra_info": {"file_path": "TinyViT/data/augmentation/dataset_factory.py", "file_name": "dataset_factory.py"}, "node_info": {"start": 4603, "end": 5769}, "relationships": {"1": "9cf00d6c245d6b46fc5d4361e291f58af9b9c0c3", "2": "1d723cd4-5557-4cda-9117-9416abfe3162"}}, "__type__": "1"}, "0983c8fd-230f-4142-846a-29abf68ccc3c": {"__data__": {"text": "import os\nimport multiprocessing\nimport torch\nimport torch.distributed as dist\nimport numpy as np\nfrom .aug_random import AugRandomContext\nfrom .manager import TxtManager\n\n\ndef get_rank():\n    if not dist.is_available() or not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\n\nclass DatasetWrapper(torch.utils.data.Dataset):\n    def __init__(self, dataset, logits_path, topk, write):\n        super().__init__()\n        self.dataset = dataset\n        self.logits_path = logits_path\n        self.epoch = multiprocessing.Value('i', 0)\n        self.topk = topk\n        self.write_mode = write\n        self.keys = self._get_keys()\n        self._manager = (None, None)\n\n    def __getitem__(self, index: int):\n        if self.write_mode:\n            return self.__getitem_for_write(index)\n        return self.__getitem_for_read(index)\n\n    def __getitem_for_write(self, index: int):\n        # get an augmentation seed\n        key = self.keys[index]\n        seed = np.int32(np.random.randint(0, 1 << 31))\n        with AugRandomContext(seed=int(seed)):\n            item = self.dataset[index]\n        return (item, (key, seed))\n\n    def __getitem_for_read(self, index: int):\n        key = self.keys[index]\n        seed, logits_index, logits_value = self._get_saved_logits(key)\n        with AugRandomContext(seed=seed):\n            item = self.dataset[index]\n        return (item, (logits_index, logits_value, np.int32(seed)))\n\n    def _get_saved_logits(self, key: str):\n        manager = self.get_manager()\n        bstr: bytes = manager.read(key)\n        # parse the augmentation seed\n        seed = int(np.frombuffer(bstr[:4], dtype=np.int32))\n    ", "doc_id": "0983c8fd-230f-4142-846a-29abf68ccc3c", "embedding": null, "doc_hash": "add5f7c13729db51531adf8611de2839e0e8b17c3fcb7833b0d37e7ac37f4260", "extra_info": {"file_path": "TinyViT/data/augmentation/dataset_wrapper.py", "file_name": "dataset_wrapper.py"}, "node_info": {"start": 0, "end": 1658}, "relationships": {"1": "8822c47d450662f967e1d57ce97d484e26fe5e51", "3": "996157d5-7d73-4df1-ae0e-f2e84c4063ac"}}, "__type__": "1"}, "996157d5-7d73-4df1-ae0e-f2e84c4063ac": {"__data__": {"text": "  bstr: bytes = manager.read(key)\n        # parse the augmentation seed\n        seed = int(np.frombuffer(bstr[:4], dtype=np.int32))\n        # parse the logits index and value\n        # copy logits_index and logits_value to avoid warning of written flag from PyTorch\n        bstr = bstr[4:]\n        logits_index = np.frombuffer(\n            bstr[:self.topk * 2], dtype=np.int16).copy()\n        bstr = bstr[self.topk * 2:]\n        logits_value = np.frombuffer(\n            bstr[:self.topk * 2], dtype=np.float16).copy()\n        return seed, logits_index, logits_value\n\n    def _build_manager(self, logits_path: str):\n        # topk * [idx, value] * 2 bytes  for logits + 4 bytes for seed\n        item_size = self.topk * 2 * 2 + 4\n        rank = get_rank()\n        return TxtManager(logits_path, item_size, rank)\n\n    def set_epoch(self, epoch: int):\n        self.epoch.value = epoch\n        self._manager = (None, None)\n\n    def get_manager(self):\n        epoch = self.epoch.value\n        if epoch != self._manager[0]:\n            logits_path = os.path.join(\n                self.logits_path, f'logits_top{self.topk}_epoch{self.epoch.value}')\n            self._manager = (epoch, self._build_manager(logits_path))\n        return self._manager[1]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def _get_keys(self):\n        if hasattr(self.dataset, 'get_keys'):\n            keys = self.dataset.get_keys()\n            if self.write_mode:\n                # we only check key unique in the write mode\n               ", "doc_id": "996157d5-7d73-4df1-ae0e-f2e84c4063ac", "embedding": null, "doc_hash": "c1e539c8ed68db34094dd7625e42bc2036ce20a4d3540b6fa5932ff78d8b1e34", "extra_info": {"file_path": "TinyViT/data/augmentation/dataset_wrapper.py", "file_name": "dataset_wrapper.py"}, "node_info": {"start": 1555, "end": 3078}, "relationships": {"1": "8822c47d450662f967e1d57ce97d484e26fe5e51", "2": "0983c8fd-230f-4142-846a-29abf68ccc3c", "3": "811fcd0d-e4c7-49fa-b9fb-5ca0b11a6a50"}}, "__type__": "1"}, "811fcd0d-e4c7-49fa-b9fb-5ca0b11a6a50": {"__data__": {"text": "           if self.write_mode:\n                # we only check key unique in the write mode\n                assert len(keys) == len(set(keys)), 'keys must be unique'\n            return keys\n        return [str(i) for i in range(len(self))]\n", "doc_id": "811fcd0d-e4c7-49fa-b9fb-5ca0b11a6a50", "embedding": null, "doc_hash": "295742afaf2bdd27e1c4956fbc9f5976fcc88ee91c7325dc2a0a05f2b20cd9a3", "extra_info": {"file_path": "TinyViT/data/augmentation/dataset_wrapper.py", "file_name": "dataset_wrapper.py"}, "node_info": {"start": 3075, "end": 3315}, "relationships": {"1": "8822c47d450662f967e1d57ce97d484e26fe5e51", "2": "996157d5-7d73-4df1-ae0e-f2e84c4063ac"}}, "__type__": "1"}, "e42f73ff-1ddd-42f0-b40b-af7d93b2e5fd": {"__data__": {"text": "import math\nimport torch\nfrom torch.utils.data import Sampler\nimport torch.distributed as dist\n\n\nclass OrderedDistributedSampler(Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    \"\"\"\n\n    def __init__(self, dataset, num_replicas=None, rank=None):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        indices = list(range(len(self.dataset)))\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert", "doc_id": "e42f73ff-1ddd-42f0-b40b-af7d93b2e5fd", "embedding": null, "doc_hash": "eae84c7ed998231c2bbfee88e963a0a78fa655f3d8cdf3a18f6ae85e0616a92b", "extra_info": {"file_path": "TinyViT/data/augmentation/distributed_sampler.py", "file_name": "distributed_sampler.py"}, "node_info": {"start": 0, "end": 1835}, "relationships": {"1": "1cefc31d6f034e7e4e60c0a8de11087691b5734f", "3": "17e8b352-cf63-410b-ae27-4a493abdaaeb"}}, "__type__": "1"}, "17e8b352-cf63-410b-ae27-4a493abdaaeb": {"__data__": {"text": " assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass RepeatAugSampler(Sampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset for distributed,\n    with repeated augmentation.\n    It ensures that different each augmented version of a sample will be visible to a\n    different process (GPU). Heavily based on torch.utils.data.DistributedSampler\n\n    This sampler was taken from https://github.com/facebookresearch/deit/blob/0c4b8f60/samplers.py\n    Used in\n    Copyright (c) 2015-present, Facebook, Inc.\n    \"\"\"\n\n    def __init__(\n            self,\n            dataset,\n            num_replicas=None,\n            rank=None,\n            shuffle=True,\n            num_repeats=3,\n            selected_round=256,\n            selected_ratio=0,\n    ):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.shuffle = shuffle\n        self.num_repeats = num_repeats\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * num_repeats /", "doc_id": "17e8b352-cf63-410b-ae27-4a493abdaaeb", "embedding": null, "doc_hash": "ac1bf100a94bf1d55327713eb7111be934518c4df96f7a1ba7d14437a412e6b8", "extra_info": {"file_path": "TinyViT/data/augmentation/distributed_sampler.py", "file_name": "distributed_sampler.py"}, "node_info": {"start": 1721, "end": 3362}, "relationships": {"1": "1cefc31d6f034e7e4e60c0a8de11087691b5734f", "2": "e42f73ff-1ddd-42f0-b40b-af7d93b2e5fd", "3": "eb960038-85a8-4e26-a44b-b110986ab674"}}, "__type__": "1"}, "eb960038-85a8-4e26-a44b-b110986ab674": {"__data__": {"text": "   self.num_repeats = num_repeats\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * num_repeats / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        # Determine the number of samples to select per epoch for each rank.\n        # num_selected logic defaults to be the same as original RASampler impl, but this one can be tweaked\n        # via selected_ratio and selected_round args.\n        selected_ratio = selected_ratio or num_replicas  # ratio to reduce selected samples by, num_replicas if 0\n        if selected_round:\n            self.num_selected_samples = int(math.floor(\n                 len(self.dataset) // selected_round * selected_round / selected_ratio))\n        else:\n            self.num_selected_samples = int(math.ceil(len(self.dataset) / selected_ratio))\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n        if self.shuffle:\n            indices = torch.randperm(len(self.dataset), generator=g)\n        else:\n            indices = torch.arange(start=0, end=len(self.dataset))\n\n        # produce repeats e.g. [0, 0, 0, 1, 1, 1, 2, 2, 2....]\n        indices = torch.repeat_interleave(indices, repeats=self.num_repeats, dim=0).tolist()\n        # add extra samples to make it evenly divisible\n        padding_size = self.total_size - len(indices)\n        if padding_size > 0:\n            indices += indices[:padding_size]\n        assert len(indices) == self.total_size\n\n        # subsample per rank\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n ", "doc_id": "eb960038-85a8-4e26-a44b-b110986ab674", "embedding": null, "doc_hash": "a49619071730c4ada516368343abcefc49b13653d1b948719e5dbf3bf44c7f91", "extra_info": {"file_path": "TinyViT/data/augmentation/distributed_sampler.py", "file_name": "distributed_sampler.py"}, "node_info": {"start": 3375, "end": 5027}, "relationships": {"1": "1cefc31d6f034e7e4e60c0a8de11087691b5734f", "2": "17e8b352-cf63-410b-ae27-4a493abdaaeb", "3": "ae984af3-aa50-4ec2-84ee-cd2c36b99dd2"}}, "__type__": "1"}, "ae984af3-aa50-4ec2-84ee-cd2c36b99dd2": {"__data__": {"text": "      assert len(indices) == self.total_size\n\n        # subsample per rank\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        # return up to num selected samples\n        return iter(indices[:self.num_selected_samples])\n\n    def __len__(self):\n        return self.num_selected_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n", "doc_id": "ae984af3-aa50-4ec2-84ee-cd2c36b99dd2", "embedding": null, "doc_hash": "c5a2ce3cd71c9f2af3d4c2a189785d0f0be535953ca30481085bd76b36c5c864", "extra_info": {"file_path": "TinyViT/data/augmentation/distributed_sampler.py", "file_name": "distributed_sampler.py"}, "node_info": {"start": 4983, "end": 5404}, "relationships": {"1": "1cefc31d6f034e7e4e60c0a8de11087691b5734f", "2": "eb960038-85a8-4e26-a44b-b110986ab674"}}, "__type__": "1"}, "832ea36b-97c8-4bb6-a6d8-042b718f2665": {"__data__": {"text": "\"\"\" Loader Factory, Fast Collate, CUDA Prefetcher\n\nPrefetcher and Fast Collate inspired by NVIDIA APEX example at\nhttps://github.com/NVIDIA/apex/commit/d5e2bb4bdeedd27b1dfaf5bb2b24d6c000dee9be#diff-cf86c282ff7fba81fad27a559379d5bf\n\nHacked together by / Copyright 2021 Ross Wightman\n\"\"\"\nfrom .aug_random import random, np_random\nfrom functools import partial\nfrom typing import Callable\n\nimport torch.utils.data\nimport numpy as np\n\nfrom .transforms_factory import create_transform\nfrom .constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom .distributed_sampler import OrderedDistributedSampler, RepeatAugSampler\nfrom .random_erasing import RandomErasing\nfrom .mixup import FastCollateMixup\n\n\ndef fast_collate(batch):\n    \"\"\" A fast collation function optimized for uint8 images (np array or torch) and int64 targets (labels)\"\"\"\n    assert isinstance(batch[0], tuple)\n    batch_size = len(batch)\n    if isinstance(batch[0][0], tuple):\n        # This branch 'deinterleaves' and flattens tuples of input tensors into one tensor ordered by position\n        # such that all tuple of position n will end up in a torch.split(tensor, batch_size) in nth position\n        inner_tuple_size = len(batch[0][0])\n        flattened_batch_size = batch_size * inner_tuple_size\n        targets = torch.zeros(flattened_batch_size, dtype=torch.int64)\n        tensor = torch.zeros((flattened_batch_size, *batch[0][0][0].shape), dtype=torch.uint8)\n        for i in range(batch_size):\n            assert len(batch[i][0]) == inner_tuple_size  # all input tensor tuples must be same length\n            for j in range(inner_tuple_size):\n                targets[i + j * batch_size] = batch[i][1]\n                tensor[i + j * batch_size] += torch.from_numpy(batch[i][0][j])\n        return tensor, targets\n    elif isinstance(batch[0][0], np.ndarray):\n        targets = torch.tensor([b[1] for b in", "doc_id": "832ea36b-97c8-4bb6-a6d8-042b718f2665", "embedding": null, "doc_hash": "676e3903c6d61efe1c12777b5c056c225311619e74c9738487a10cca4055f5ac", "extra_info": {"file_path": "TinyViT/data/augmentation/loader.py", "file_name": "loader.py"}, "node_info": {"start": 0, "end": 1881}, "relationships": {"1": "876ae5f4623f99c9aa9e6aa33913cbe43bf4ca53", "3": "cca49118-4bf6-4e34-9dd1-19af8b5fc58a"}}, "__type__": "1"}, "cca49118-4bf6-4e34-9dd1-19af8b5fc58a": {"__data__": {"text": "       return tensor, targets\n    elif isinstance(batch[0][0], np.ndarray):\n        targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n        assert len(targets) == batch_size\n        tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n        for i in range(batch_size):\n            tensor[i] += torch.from_numpy(batch[i][0])\n        return tensor, targets\n    elif isinstance(batch[0][0], torch.Tensor):\n        targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n        assert len(targets) == batch_size\n        tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n        for i in range(batch_size):\n            tensor[i].copy_(batch[i][0])\n        return tensor, targets\n    else:\n        assert False\n\n\nclass PrefetchLoader:\n\n    def __init__(self,\n                 loader,\n                 mean=IMAGENET_DEFAULT_MEAN,\n                 std=IMAGENET_DEFAULT_STD,\n                 fp16=False,\n                 re_prob=0.,\n                 re_mode='const',\n                 re_count=1,\n                 re_num_splits=0):\n        self.loader = loader\n        self.mean = torch.tensor([x * 255 for x in mean]).cuda().view(1, 3, 1, 1)\n        self.std = torch.tensor([x * 255 for x in std]).cuda().view(1, 3, 1, 1)\n        self.fp16 = fp16\n        if fp16:\n            self.mean = self.mean.half()\n      ", "doc_id": "cca49118-4bf6-4e34-9dd1-19af8b5fc58a", "embedding": null, "doc_hash": "9c9b6856974c6fa1e1e98fb5c24d8cc8f6996f715b344802d87e2b1cc190a0be", "extra_info": {"file_path": "TinyViT/data/augmentation/loader.py", "file_name": "loader.py"}, "node_info": {"start": 1789, "end": 3165}, "relationships": {"1": "876ae5f4623f99c9aa9e6aa33913cbe43bf4ca53", "2": "832ea36b-97c8-4bb6-a6d8-042b718f2665", "3": "4a45bd57-03a9-49b4-9f80-3f948e0d65b4"}}, "__type__": "1"}, "4a45bd57-03a9-49b4-9f80-3f948e0d65b4": {"__data__": {"text": "3, 1, 1)\n        self.fp16 = fp16\n        if fp16:\n            self.mean = self.mean.half()\n            self.std = self.std.half()\n        if re_prob > 0.:\n            self.random_erasing = RandomErasing(\n                probability=re_prob, mode=re_mode, max_count=re_count, num_splits=re_num_splits)\n        else:\n            self.random_erasing = None\n\n    def __iter__(self):\n        stream = torch.cuda.Stream()\n        first = True\n\n        for next_input, next_target in self.loader:\n            with torch.cuda.stream(stream):\n                next_input = next_input.cuda(non_blocking=True)\n                next_target = next_target.cuda(non_blocking=True)\n                if self.fp16:\n                    next_input = next_input.half().sub_(self.mean).div_(self.std)\n                else:\n                    next_input = next_input.float().sub_(self.mean).div_(self.std)\n                if self.random_erasing is not None:\n                    next_input = self.random_erasing(next_input)\n\n            if not first:\n                yield input, target\n            else:\n                first = False\n\n            torch.cuda.current_stream().wait_stream(stream)\n            input = next_input\n            target = next_target\n\n        yield input, target\n\n    def __len__(self):\n        return len(self.loader)\n\n    @property\n    def sampler(self):\n        return self.loader.sampler\n\n   ", "doc_id": "4a45bd57-03a9-49b4-9f80-3f948e0d65b4", "embedding": null, "doc_hash": "8d6470d55669c73b5dd181f3065f9b5d9d5be9b34a7e19976b15c1b2ba4cb267", "extra_info": {"file_path": "TinyViT/data/augmentation/loader.py", "file_name": "loader.py"}, "node_info": {"start": 3202, "end": 4599}, "relationships": {"1": "876ae5f4623f99c9aa9e6aa33913cbe43bf4ca53", "2": "cca49118-4bf6-4e34-9dd1-19af8b5fc58a", "3": "4887be70-54e6-4457-911c-d67376492e07"}}, "__type__": "1"}, "4887be70-54e6-4457-911c-d67376492e07": {"__data__": {"text": "input, target\n\n    def __len__(self):\n        return len(self.loader)\n\n    @property\n    def sampler(self):\n        return self.loader.sampler\n\n    @property\n    def dataset(self):\n        return self.loader.dataset\n\n    @property\n    def mixup_enabled(self):\n        if isinstance(self.loader.collate_fn, FastCollateMixup):\n            return self.loader.collate_fn.mixup_enabled\n        else:\n            return False\n\n    @mixup_enabled.setter\n    def mixup_enabled(self, x):\n        if isinstance(self.loader.collate_fn, FastCollateMixup):\n            self.loader.collate_fn.mixup_enabled = x\n\n\ndef _worker_init(worker_id, worker_seeding='all'):\n    worker_info = torch.utils.data.get_worker_info()\n    assert worker_info.id == worker_id\n    if isinstance(worker_seeding, Callable):\n        seed = worker_seeding(worker_info)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        np_random.seed(seed % (2 ** 32 - 1))\n    else:\n        assert worker_seeding in ('all', 'part')\n        # random / torch seed already called in dataloader iter class w/ worker_info.seed\n        # to reproduce some old results (same seed + hparam combo), partial seeding is required (skip numpy re-seed)\n        if worker_seeding == 'all':\n            np_random.seed(worker_info.seed % (2 ** 32 - 1))\n\n\ndef create_loader(\n        dataset,\n        input_size,\n        batch_size,\n        is_training=False,\n        use_prefetcher=True,\n        no_aug=False,\n        re_prob=0.,\n        re_mode='const',\n        re_count=1,\n        re_split=False,\n        scale=None,\n        ratio=None,\n       ", "doc_id": "4887be70-54e6-4457-911c-d67376492e07", "embedding": null, "doc_hash": "60fdbcfcfd83a025946461cee2a1afdbe67a638731ae04cdfbcb29a0a1994e98", "extra_info": {"file_path": "TinyViT/data/augmentation/loader.py", "file_name": "loader.py"}, "node_info": {"start": 4546, "end": 6134}, "relationships": {"1": "876ae5f4623f99c9aa9e6aa33913cbe43bf4ca53", "2": "4a45bd57-03a9-49b4-9f80-3f948e0d65b4", "3": "883de9a3-ad84-4860-a698-fa863f008dce"}}, "__type__": "1"}, "883de9a3-ad84-4860-a698-fa863f008dce": {"__data__": {"text": "       re_count=1,\n        re_split=False,\n        scale=None,\n        ratio=None,\n        hflip=0.5,\n        vflip=0.,\n        color_jitter=0.4,\n        auto_augment=None,\n        num_aug_repeats=0,\n        num_aug_splits=0,\n        interpolation='bilinear',\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n        num_workers=1,\n        distributed=False,\n        crop_pct=None,\n        collate_fn=None,\n        pin_memory=False,\n        fp16=False,\n        tf_preprocessing=False,\n        use_multi_epochs_loader=False,\n        persistent_workers=True,\n        worker_seeding='all',\n):\n    re_num_splits = 0\n    if re_split:\n        # apply RE to second half of batch if no aug split otherwise line up with aug split\n        re_num_splits = num_aug_splits or 2\n    dataset.transform = create_transform(\n        input_size,\n        is_training=is_training,\n        use_prefetcher=use_prefetcher,\n        no_aug=no_aug,\n        scale=scale,\n        ratio=ratio,\n        hflip=hflip,\n        vflip=vflip,\n        color_jitter=color_jitter,\n        auto_augment=auto_augment,\n        interpolation=interpolation,\n        mean=mean,\n        std=std,\n        crop_pct=crop_pct,\n        tf_preprocessing=tf_preprocessing,\n        re_prob=re_prob,\n        re_mode=re_mode,\n        re_count=re_count,\n       ", "doc_id": "883de9a3-ad84-4860-a698-fa863f008dce", "embedding": null, "doc_hash": "504805d9bfa2c51e7840e1c71ebb6ba969f794d5bde7e36412d11027ace06338", "extra_info": {"file_path": "TinyViT/data/augmentation/loader.py", "file_name": "loader.py"}, "node_info": {"start": 6194, "end": 7518}, "relationships": {"1": "876ae5f4623f99c9aa9e6aa33913cbe43bf4ca53", "2": "4887be70-54e6-4457-911c-d67376492e07", "3": "5e789ec1-6363-4684-9f5c-caa44a7d1ff3"}}, "__type__": "1"}, "5e789ec1-6363-4684-9f5c-caa44a7d1ff3": {"__data__": {"text": "       re_prob=re_prob,\n        re_mode=re_mode,\n        re_count=re_count,\n        re_num_splits=re_num_splits,\n        separate=num_aug_splits > 0,\n    )\n\n    sampler = None\n    if distributed and not isinstance(dataset, torch.utils.data.IterableDataset):\n        if is_training:\n            if num_aug_repeats:\n                sampler = RepeatAugSampler(dataset, num_repeats=num_aug_repeats)\n            else:\n                sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n        else:\n            # This will add extra duplicate entries to result in equal num\n            # of samples per-process, will slightly alter validation results\n            sampler = OrderedDistributedSampler(dataset)\n    else:\n        assert num_aug_repeats == 0, \"RepeatAugment not currently supported in non-distributed or IterableDataset use\"\n\n    if collate_fn is None:\n        collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate\n\n    loader_class = torch.utils.data.DataLoader\n    if use_multi_epochs_loader:\n        loader_class = MultiEpochsDataLoader\n\n    loader_args = dict(\n        batch_size=batch_size,\n        shuffle=not isinstance(dataset, torch.utils.data.IterableDataset) and sampler is None and is_training,\n        num_workers=num_workers,\n        sampler=sampler,\n        collate_fn=collate_fn,\n        pin_memory=pin_memory,\n        drop_last=is_training,\n        worker_init_fn=partial(_worker_init, worker_seeding=worker_seeding),\n        persistent_workers=persistent_workers\n    )\n    try:\n        loader = loader_class(dataset, **loader_args)\n    except TypeError as e:\n       ", "doc_id": "5e789ec1-6363-4684-9f5c-caa44a7d1ff3", "embedding": null, "doc_hash": "297f872503eb20da76bd90b0a56e8646b1c17042d3aa75d7268108bc3e8a4b3f", "extra_info": {"file_path": "TinyViT/data/augmentation/loader.py", "file_name": "loader.py"}, "node_info": {"start": 7518, "end": 9164}, "relationships": {"1": "876ae5f4623f99c9aa9e6aa33913cbe43bf4ca53", "2": "883de9a3-ad84-4860-a698-fa863f008dce", "3": "48a2df4b-b862-4625-89bd-519c23b62d97"}}, "__type__": "1"}, "48a2df4b-b862-4625-89bd-519c23b62d97": {"__data__": {"text": "   persistent_workers=persistent_workers\n    )\n    try:\n        loader = loader_class(dataset, **loader_args)\n    except TypeError as e:\n        loader_args.pop('persistent_workers')  # only in Pytorch 1.7+\n        loader = loader_class(dataset, **loader_args)\n    if use_prefetcher:\n        prefetch_re_prob = re_prob if is_training and not no_aug else 0.\n        loader = PrefetchLoader(\n            loader,\n            mean=mean,\n            std=std,\n            fp16=fp16,\n            re_prob=prefetch_re_prob,\n            re_mode=re_mode,\n            re_count=re_count,\n            re_num_splits=re_num_splits\n        )\n\n    return loader\n\n\nclass MultiEpochsDataLoader(torch.utils.data.DataLoader):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._DataLoader__initialized = False\n        self.batch_sampler = _RepeatSampler(self.batch_sampler)\n        self._DataLoader__initialized = True\n        self.iterator = super().__iter__()\n\n    def __len__(self):\n        return len(self.batch_sampler.sampler)\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield next(self.iterator)\n\n\nclass _RepeatSampler(object):\n    \"\"\" Sampler that repeats forever.\n\n    Args:\n        sampler (Sampler)\n    \"\"\"\n\n    def __init__(self, sampler):\n        self.sampler = sampler\n\n    def __iter__(self):\n        while True:\n            yield from iter(self.sampler)\n", "doc_id": "48a2df4b-b862-4625-89bd-519c23b62d97", "embedding": null, "doc_hash": "4a41cc71a9a98977c5b4cc9577daff141ad3f153a91134aeaeb587f20e97e79d", "extra_info": {"file_path": "TinyViT/data/augmentation/loader.py", "file_name": "loader.py"}, "node_info": {"start": 9073, "end": 10495}, "relationships": {"1": "876ae5f4623f99c9aa9e6aa33913cbe43bf4ca53", "2": "5e789ec1-6363-4684-9f5c-caa44a7d1ff3"}}, "__type__": "1"}, "9f8f5602-1a3e-480e-8f45-ea15cd143c9e": {"__data__": {"text": "import os\nimport multiprocessing\nimport tempfile\n\n\nclass _Writer:\n    def __init__(self, path, rank):\n        self.msg_queue = multiprocessing.Queue()\n        self.worker = multiprocessing.Process(\n            target=self._async_manager_worker_fn,\n            args=(self.msg_queue, path, rank),\n        )\n        self.worker.start()\n\n    def write(self, key: str, value: bytes) -> bool:\n        self.msg_queue.put((key, value))\n        return True\n\n    class _WORKER_MSG:\n        KILL = 4\n\n    def _async_manager_worker_fn(self, msg_queue, path, rank):\n        # path: xxx/logits_top100_epoch0\n        rank_name = f'rank{rank}'\n        # logits_top100_epoch0_rank0\n        basename = os.path.basename(path) + f'_{rank_name}'\n        tmp_handle = tempfile.TemporaryDirectory(prefix='tinyvit_' + basename)\n\n        # tmp_dir/tinyvit_logits_top100_epoch0_rank0\n        temp_dirname = tmp_handle.name\n\n        tmp_filename = os.path.join(temp_dirname, rank_name)\n        # tmp_dir/tinyvit_logits_top100_epoch0_rank0/rank0-keys.txt\n        keys_fname = tmp_filename + '-keys.txt'\n        values_fname = tmp_filename + '-values.bin'\n\n        keys_file = open(keys_fname, 'w')\n        values_file = open(values_fname, 'wb')\n        keys = dict()\n\n        while 1:\n            item = msg_queue.get()\n            if item == _Writer._WORKER_MSG.KILL:\n                break\n            key, value = item\n            if key in keys:\n                continue\n            idx = len(keys)\n            keys[key] =", "doc_id": "9f8f5602-1a3e-480e-8f45-ea15cd143c9e", "embedding": null, "doc_hash": "d1cecaa2d051d9c2b9bdff3eaa7091bbc6e6552dc0a5c423bd198fa1101aaba6", "extra_info": {"file_path": "TinyViT/data/augmentation/manager.py", "file_name": "manager.py"}, "node_info": {"start": 0, "end": 1497}, "relationships": {"1": "c9d44c50bf5f555c9d52e9e036b0bb282677c276", "3": "e54b12eb-bdb0-4384-8828-236fbbad53ea"}}, "__type__": "1"}, "e54b12eb-bdb0-4384-8828-236fbbad53ea": {"__data__": {"text": "      if key in keys:\n                continue\n            idx = len(keys)\n            keys[key] = idx\n            keys_file.write(key + '\\n')\n            values_file.write(value)\n\n        keys_file.close()\n        values_file.close()\n\n        os.makedirs(path, exist_ok=True)\n        os.system(f'mv {temp_dirname}/* {path}/')\n        print(f\"Save logits over: {path}\")\n\n    def __del__(self):\n        if self.worker is not None:\n            self.msg_queue.put(_Writer._WORKER_MSG.KILL)\n            self.worker.join()\n\n\nclass _Reader:\n    def __init__(self, path: str, item_size: int, rank: int):\n        self.rank = rank\n        self.item_size = item_size\n        self.packages = self.search_packages(path)\n        self.packages_visited = [False] * len(self.packages)\n        # key -> package idx\n        self.keys = dict()\n\n    def read(self, key: str) -> bytes:\n        pkg_idx, value_idx = self.keys.get(key, (None, None))\n        if pkg_idx is None:\n            pkg_idx, value_idx = self.find_item_in_packages(key)\n        return self.packages[pkg_idx][value_idx]\n\n    def find_item_in_packages(self, key: str) -> (int, int):\n        for pkg_idx, pkg in enumerate(self.packages):\n            if not self.packages_visited[pkg_idx]:\n                self.packages_visited[pkg_idx] = True\n                # load keys\n                keys_fname = pkg.name + '-keys.txt'\n                with open(keys_fname, 'r') as keys_file:\n                ", "doc_id": "e54b12eb-bdb0-4384-8828-236fbbad53ea", "embedding": null, "doc_hash": "046ab305d723da9a23ca01d10ce3d1788461de1718abe523e03bfdd2068bc90e", "extra_info": {"file_path": "TinyViT/data/augmentation/manager.py", "file_name": "manager.py"}, "node_info": {"start": 1452, "end": 2895}, "relationships": {"1": "c9d44c50bf5f555c9d52e9e036b0bb282677c276", "2": "9f8f5602-1a3e-480e-8f45-ea15cd143c9e", "3": "2042c87f-6d15-4225-b93c-ca0dbf9653c5"}}, "__type__": "1"}, "2042c87f-6d15-4225-b93c-ca0dbf9653c5": {"__data__": {"text": " keys_fname = pkg.name + '-keys.txt'\n                with open(keys_fname, 'r') as keys_file:\n                    for i, k in enumerate(keys_file.readlines()):\n                        k = k.strip()\n                        self.keys[k] = (pkg_idx, i)\n                if key in self.keys:\n                    return self.keys[key]\n        raise KeyError(key)\n\n    def search_packages(self, path):\n        assert os.path.isdir(path), f'[Error] Reading logits fails. Path {path} not found.'\n        names = self.search_packages_names(path)\n        return [_Reader._PackageReader(name, self.item_size) for name in names]\n\n    def search_packages_names(self, path):\n        names = []\n        VALUES_POSTFIX = '-values.bin'\n        for name in os.listdir(path):\n            if name.endswith(VALUES_POSTFIX):\n                names.append(name[:-len(VALUES_POSTFIX)])\n\n        num_packages = len(names)\n\n        def rank_key_fn(name):\n            r = int(name[4:])\n            return (r - self.rank) % num_packages\n\n        # move the rankx-keys.txt to the front\n        names.sort(key=rank_key_fn)\n        names = list(map(lambda x: os.path.join(path, x), names))\n        return names\n\n    class _PackageReader:\n        def __init__(self, name, item_size):\n            self.name = name\n            self.item_size = item_size\n\n            # delay to create handle\n            self.values_file = None\n\n        def __getitem__(self, idx: int):\n            self._ensure_handle_created()\n   ", "doc_id": "2042c87f-6d15-4225-b93c-ca0dbf9653c5", "embedding": null, "doc_hash": "6baf9e20a8dace76391e6a0ec62185faf1adf5ad533d2bc4b28410c221e9c93a", "extra_info": {"file_path": "TinyViT/data/augmentation/manager.py", "file_name": "manager.py"}, "node_info": {"start": 2873, "end": 4352}, "relationships": {"1": "c9d44c50bf5f555c9d52e9e036b0bb282677c276", "2": "e54b12eb-bdb0-4384-8828-236fbbad53ea", "3": "2bc9688f-1bea-49b5-851b-b5188e603994"}}, "__type__": "1"}, "2bc9688f-1bea-49b5-851b-b5188e603994": {"__data__": {"text": "           self.values_file = None\n\n        def __getitem__(self, idx: int):\n            self._ensure_handle_created()\n            self.values_file.seek(self.item_size * idx)\n            return self.values_file.read(self.item_size)\n\n        def _ensure_handle_created(self):\n            if self.values_file is None:\n                values_fname = self.name + '-values.bin'\n                self.values_file = open(values_fname, 'rb')\n\n\nclass TxtManager:\n    def __init__(self, path: str, item_size: int, rank: int):\n        self.path = path\n        self.writer = None\n        self.reader = None\n        self.item_size = item_size\n        self.rank = rank\n\n    def write(self, key: str, value: bytes) -> bool:\n        if self.writer is None:\n            self.writer = _Writer(self.path, self.rank)\n        return self.writer.write(key, value)\n\n    def read(self, key: str) -> bytes:\n        if self.reader is None:\n            self.reader = _Reader(self.path, self.item_size, self.rank)\n        return self.reader.read(key)\n", "doc_id": "2bc9688f-1bea-49b5-851b-b5188e603994", "embedding": null, "doc_hash": "9226f14845ba36f32c185936885bd43eb955401d4b025fd6f6b6d2e0f580a5f6", "extra_info": {"file_path": "TinyViT/data/augmentation/manager.py", "file_name": "manager.py"}, "node_info": {"start": 4299, "end": 5321}, "relationships": {"1": "c9d44c50bf5f555c9d52e9e036b0bb282677c276", "2": "2042c87f-6d15-4225-b93c-ca0dbf9653c5"}}, "__type__": "1"}, "13fd19eb-f1d9-4598-ba45-dc9bf4ac578b": {"__data__": {"text": "\"\"\" Mixup and Cutmix\n\nPapers:\nmixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\n\nCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (https://arxiv.org/abs/1905.04899)\n\nCode Reference:\nCutMix: https://github.com/clovaai/CutMix-PyTorch\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport numpy as np\nimport torch\nfrom .aug_random import AugRandomContext, random, np_random\n\n\ndef one_hot(x, num_classes, on_value=1., off_value=0., device='cuda'):\n    x = x.long().view(-1, 1)\n    return torch.full((x.size()[0], num_classes), off_value, device=device).scatter_(1, x, on_value)\n\n\ndef mixup_target(target, num_classes, lam=1., smoothing=0.0, device='cuda'):\n    off_value = smoothing / num_classes\n    on_value = 1. - smoothing + off_value\n    y1 = one_hot(target, num_classes, on_value=on_value, off_value=off_value, device=device)\n    y2 = one_hot(target.flip(0), num_classes, on_value=on_value, off_value=off_value, device=device)\n    return y1 * lam + y2 * (1. - lam)\n\n\ndef rand_bbox(img_shape, lam, margin=0., count=None):\n    \"\"\" Standard CutMix bounding-box\n    Generates a random square bbox based on lambda value. This impl includes\n    support for enforcing a border margin as percent of bbox dimensions.\n\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"\n    ratio = np.sqrt(1 - lam)\n    img_h, img_w = img_shape[-2:]\n    cut_h, cut_w = int(img_h * ratio), int(img_w * ratio)\n    margin_y, margin_x = int(margin * cut_h), int(margin * cut_w)\n    cy = np_random.randint(0 + margin_y, img_h - margin_y, size=count)\n    cx = np_random.randint(0 + margin_x, img_w - margin_x, size=count)\n    yl = np.clip(cy - cut_h // 2, 0,", "doc_id": "13fd19eb-f1d9-4598-ba45-dc9bf4ac578b", "embedding": null, "doc_hash": "fb21646063ba9c008025a111ee6f5b3f98a3590996dbe836b2f95edabec43abf", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 0, "end": 1914}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "3": "007d169c-8b7e-4234-b68b-7ceb7fc88f1f"}}, "__type__": "1"}, "007d169c-8b7e-4234-b68b-7ceb7fc88f1f": {"__data__": {"text": "img_h - margin_y, size=count)\n    cx = np_random.randint(0 + margin_x, img_w - margin_x, size=count)\n    yl = np.clip(cy - cut_h // 2, 0, img_h)\n    yh = np.clip(cy + cut_h // 2, 0, img_h)\n    xl = np.clip(cx - cut_w // 2, 0, img_w)\n    xh = np.clip(cx + cut_w // 2, 0, img_w)\n    return yl, yh, xl, xh\n\n\ndef rand_bbox_minmax(img_shape, minmax, count=None):\n    \"\"\" Min-Max CutMix bounding-box\n    Inspired by Darknet cutmix impl, generates a random rectangular bbox\n    based on min/max percent values applied to each dimension of the input image.\n\n    Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9 range for max.\n\n    Args:\n        img_shape (tuple): Image shape as tuple\n        minmax (tuple or list): Min and max bbox ratios (as percent of image size)\n        count (int): Number of bbox to generate\n    \"\"\"\n    assert len(minmax) == 2\n    img_h, img_w = img_shape[-2:]\n    cut_h = np_random.randint(int(img_h * minmax[0]), int(img_h * minmax[1]), size=count)\n    cut_w = np_random.randint(int(img_w * minmax[0]), int(img_w * minmax[1]), size=count)\n    yl = np_random.randint(0, img_h - cut_h, size=count)\n    xl = np_random.randint(0, img_w - cut_w, size=count)\n    yu = yl + cut_h\n    xu = xl + cut_w\n    return yl, yu, xl, xu\n\n\ndef cutmix_bbox_and_lam(img_shape, lam, ratio_minmax=None, correct_lam=True, count=None):\n    \"\"\" Generate bbox and apply lambda correction.\n    \"\"\"\n    if ratio_minmax is not None:\n        yl, yu, xl, xu = rand_bbox_minmax(img_shape, ratio_minmax, count=count)\n    else:\n        yl, yu, xl, xu = rand_bbox(img_shape, lam, count=count)\n    if correct_lam or ratio_minmax is not None:\n        bbox_area = (yu -", "doc_id": "007d169c-8b7e-4234-b68b-7ceb7fc88f1f", "embedding": null, "doc_hash": "5ea46e5b8c14532882a06206fc47bccb3325a1826de6cf16b8a2074b568a7454", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 1804, "end": 3476}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "2": "13fd19eb-f1d9-4598-ba45-dc9bf4ac578b", "3": "2fe5fbaf-5b21-48e6-9436-fce92887f881"}}, "__type__": "1"}, "2fe5fbaf-5b21-48e6-9436-fce92887f881": {"__data__": {"text": "     yl, yu, xl, xu = rand_bbox(img_shape, lam, count=count)\n    if correct_lam or ratio_minmax is not None:\n        bbox_area = (yu - yl) * (xu - xl)\n        lam = 1. - bbox_area / float(img_shape[-2] * img_shape[-1])\n    return (yl, yu, xl, xu), lam\n\n\nclass Mixup:\n    \"\"\" Mixup/Cutmix that applies different params to each element or whole batch\n\n    Args:\n        mixup_alpha (float): mixup alpha value, mixup is active if > 0.\n        cutmix_alpha (float): cutmix alpha value, cutmix is active if > 0.\n        cutmix_minmax (List[float]): cutmix min/max image ratio, cutmix is active and uses this vs alpha if not None.\n        prob (float): probability of applying mixup or cutmix per batch or element\n        switch_prob (float): probability of switching to cutmix instead of mixup when both are active\n        mode (str): how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)\n        correct_lam (bool): apply lambda correction when cutmix bbox clipped by image borders\n        label_smoothing (float): apply label smoothing to the mixed target tensor\n        num_classes (int): number of classes for target\n    \"\"\"\n\n    def __init__(self, mixup_alpha=1., cutmix_alpha=0., cutmix_minmax=None, prob=1.0, switch_prob=0.5,\n                 mode='batch', correct_lam=True, label_smoothing=0.1, num_classes=1000):\n        self.mixup_alpha = mixup_alpha\n        self.cutmix_alpha = cutmix_alpha\n        self.cutmix_minmax = cutmix_minmax\n        if self.cutmix_minmax is not None:\n            assert len(self.cutmix_minmax) == 2\n            # force cutmix alpha == 1.0 when minmax active to keep logic simple & safe\n            self.cutmix_alpha = 1.0\n        self.mix_prob = prob\n        self.switch_prob = switch_prob\n       ", "doc_id": "2fe5fbaf-5b21-48e6-9436-fce92887f881", "embedding": null, "doc_hash": "2c72063991007273d06786876d5e59e76cb2057050b245a93a776faec69b71bb", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 3487, "end": 5253}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "2": "007d169c-8b7e-4234-b68b-7ceb7fc88f1f", "3": "963572fc-7d7f-48d8-a5da-6f986f8c22ea"}}, "__type__": "1"}, "963572fc-7d7f-48d8-a5da-6f986f8c22ea": {"__data__": {"text": "          self.cutmix_alpha = 1.0\n        self.mix_prob = prob\n        self.switch_prob = switch_prob\n        self.label_smoothing = label_smoothing\n        self.num_classes = num_classes\n        self.mode = mode\n        assert self.mode in ['batch', 'pair', 'elem', 'pair2'], 'Invalid mode: {}'.format(self.mode)\n        assert self.mode in ['pair2'], 'The mode of mixup should be `pair2` when saving logits'\n        self.correct_lam = correct_lam  # correct lambda based on clipped area for cutmix\n        self.mixup_enabled = True  # set to false to disable mixing (intended tp be set by train loop)\n\n    def _params_per_elem(self, batch_size):\n        lam = np.ones(batch_size, dtype=np.float32)\n        use_cutmix = np.zeros(batch_size, dtype=np.bool)\n        if self.mixup_enabled:\n            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:\n                use_cutmix = np_random.rand(batch_size) < self.switch_prob\n                lam_mix = np.where(\n                    use_cutmix,\n                    np_random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size),\n                    np_random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size))\n            elif self.mixup_alpha > 0.:\n                lam_mix = np_random.beta(self.mixup_alpha, self.mixup_alpha, size=batch_size)\n            elif self.cutmix_alpha > 0.:\n                use_cutmix = np.ones(batch_size, dtype=np.bool)\n                lam_mix = np_random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size)\n            else:\n  ", "doc_id": "963572fc-7d7f-48d8-a5da-6f986f8c22ea", "embedding": null, "doc_hash": "72cdf2b44dc3fbf483e711d7bd74676ac1167de2d1d3c4cbd2329c6a72a09544", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 5285, "end": 6816}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "2": "2fe5fbaf-5b21-48e6-9436-fce92887f881", "3": "735376d5-d5eb-4764-9d7c-c458c0319668"}}, "__type__": "1"}, "735376d5-d5eb-4764-9d7c-c458c0319668": {"__data__": {"text": "               lam_mix = np_random.beta(self.cutmix_alpha, self.cutmix_alpha, size=batch_size)\n            else:\n                assert False, \"One of mixup_alpha > 0., cutmix_alpha > 0., cutmix_minmax not None should be true.\"\n            lam = np.where(np_random.rand(batch_size) < self.mix_prob, lam_mix.astype(np.float32), lam)\n        return lam, use_cutmix\n\n    def _params_per_batch(self):\n        lam = 1.\n        use_cutmix = False\n        if self.mixup_enabled and np_random.rand() < self.mix_prob:\n            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:\n                use_cutmix = np_random.rand() < self.switch_prob\n                lam_mix = np_random.beta(self.cutmix_alpha, self.cutmix_alpha) if use_cutmix else \\\n                    np_random.beta(self.mixup_alpha, self.mixup_alpha)\n            elif self.mixup_alpha > 0.:\n                lam_mix = np_random.beta(self.mixup_alpha, self.mixup_alpha)\n            elif self.cutmix_alpha > 0.:\n                use_cutmix = True\n                lam_mix = np_random.beta(self.cutmix_alpha, self.cutmix_alpha)\n            else:\n                assert False, \"One of mixup_alpha > 0., cutmix_alpha > 0., cutmix_minmax not None should be true.\"\n            lam = float(lam_mix)\n        return lam, use_cutmix\n\n    def _mix_elem(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in", "doc_id": "735376d5-d5eb-4764-9d7c-c458c0319668", "embedding": null, "doc_hash": "0b77a3133c7224e6dc5b5834b876932a9369d413d4abf20067184c4ea49d44d1", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 6805, "end": 8306}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "2": "963572fc-7d7f-48d8-a5da-6f986f8c22ea", "3": "acd65ee3-f199-497f-ad7e-0c835e6da534"}}, "__type__": "1"}, "acd65ee3-f199-497f-ad7e-0c835e6da534": {"__data__": {"text": "  lam_batch, use_cutmix = self._params_per_elem(batch_size)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_pair(self, x):\n        batch_size = len(x)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n        x_orig = x.clone()  # need to keep an unmodified original for mixing source\n        for i in range(batch_size // 2):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            if lam != 1.:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n       ", "doc_id": "acd65ee3-f199-497f-ad7e-0c835e6da534", "embedding": null, "doc_hash": "d44c2f80b70419eb6029d8aa3589e981f616b977e99e6e4b2fc647d738f3a355", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 8265, "end": 9573}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "2": "735376d5-d5eb-4764-9d7c-c458c0319668", "3": "fd1c2d4d-bd8f-4afd-8532-9727b9cfa6db"}}, "__type__": "1"}, "fd1c2d4d-bd8f-4afd-8532-9727b9cfa6db": {"__data__": {"text": "                       x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n                    x[i][:, yl:yh, xl:xh] = x_orig[j][:, yl:yh, xl:xh]\n                    x[j][:, yl:yh, xl:xh] = x_orig[i][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    x[i] = x[i] * lam + x_orig[j] * (1 - lam)\n                    x[j] = x[j] * lam + x_orig[i] * (1 - lam)\n        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def _mix_batch(self, x):\n        lam, use_cutmix = self._params_per_batch()\n        if lam == 1.:\n            return 1.\n        if use_cutmix:\n            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                x.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n            x[:, :, yl:yh, xl:xh] = x.flip(0)[:, :, yl:yh, xl:xh]\n        else:\n            x_flipped = x.flip(0).mul_(1. - lam)\n            x.mul_(lam).add_(x_flipped)\n        return lam\n\n    def _mix_pair2(self, x, seeds):\n        assert seeds is not None, \"seeds must be provided when mode is `pair2` in mixup\"\n        batch_size = len(x)\n        lam_batch =", "doc_id": "fd1c2d4d-bd8f-4afd-8532-9727b9cfa6db", "embedding": null, "doc_hash": "fdb19e0eb4a054724de5175029c8363fa792fc063e0428c1a7801f2255f66b52", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 9621, "end": 10859}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "2": "acd65ee3-f199-497f-ad7e-0c835e6da534", "3": "6cc6666b-4ce7-4a72-8939-5dec06e880b9"}}, "__type__": "1"}, "6cc6666b-4ce7-4a72-8939-5dec06e880b9": {"__data__": {"text": "seeds):\n        assert seeds is not None, \"seeds must be provided when mode is `pair2` in mixup\"\n        batch_size = len(x)\n        lam_batch = np.ones(batch_size, dtype=np.float32)\n\n        for i in range(0, batch_size, 2):\n            # for each pair x[i] and x[i + 1]\n            seed = int(seeds[i] ^ seeds[i + 1])\n            with AugRandomContext(seed=seed):\n                lam, use_cutmix = self._params_per_batch()\n                lam_batch[i:i+2] = lam\n                if lam == 1.:\n                    continue\n                if use_cutmix:\n                    # cutmix\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        x[i].shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n                    x[i:i+2, :, yl:yh, xl:xh] = x[i:i+2].flip(0)[:, :, yl:yh, xl:xh]\n                else:\n                    # mixup\n                    x_flipped = x[i:i+2].flip(0).mul_(1. - lam)\n                    x[i:i+2].mul_(lam).add_(x_flipped)\n        return torch.tensor(lam_batch, device=x.device, dtype=x.dtype).unsqueeze(1)\n\n    def __call__(self, x, target, seeds=None):\n        assert len(x) % 2 == 0, 'Batch size should be even when using this'\n        if self.mode == 'elem':\n            lam =", "doc_id": "6cc6666b-4ce7-4a72-8939-5dec06e880b9", "embedding": null, "doc_hash": "6b6420cf8345834d779d69805857eafc7b613ae53b37fc399eee5cab9e65d2b0", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 10834, "end": 12097}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "2": "fd1c2d4d-bd8f-4afd-8532-9727b9cfa6db", "3": "4849a526-4b72-4c38-a00c-3d19ce8b0083"}}, "__type__": "1"}, "4849a526-4b72-4c38-a00c-3d19ce8b0083": {"__data__": {"text": "       assert len(x) % 2 == 0, 'Batch size should be even when using this'\n        if self.mode == 'elem':\n            lam = self._mix_elem(x)\n        elif self.mode == 'pair':\n            lam = self._mix_pair(x)\n        elif self.mode == 'pair2':\n            lam = self._mix_pair2(x, seeds)\n        else:\n            lam = self._mix_batch(x)\n        if target is not None:\n            target = mixup_target(target, self.num_classes, lam, self.label_smoothing, x.device)\n        return x, target\n\n\nclass FastCollateMixup(Mixup):\n    \"\"\" Fast Collate w/ Mixup/Cutmix that applies different params to each element or whole batch\n\n    A Mixup impl that's performed while collating the batches.\n    \"\"\"\n\n    def _mix_elem_collate(self, output, batch, half=False):\n        batch_size = len(batch)\n        num_elem = batch_size // 2 if half else batch_size\n        assert len(output) == num_elem\n        lam_batch, use_cutmix = self._params_per_elem(num_elem)\n        for i in range(num_elem):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            mixed = batch[i][0]\n            if lam != 1.:\n                if use_cutmix[i]:\n                    if not half:\n                        mixed = mixed.copy()\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        output.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n                    mixed[:, yl:yh, xl:xh] =", "doc_id": "4849a526-4b72-4c38-a00c-3d19ce8b0083", "embedding": null, "doc_hash": "032f04619b90b6ef76bc4f89f04d009b59160e93c0534d82db572fe8dcdc4b6d", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 12121, "end": 13560}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "2": "6cc6666b-4ce7-4a72-8939-5dec06e880b9", "3": "67ab8580-4e23-40ce-be15-b3eebe8165cf"}}, "__type__": "1"}, "67ab8580-4e23-40ce-be15-b3eebe8165cf": {"__data__": {"text": "lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n                    mixed[:, yl:yh, xl:xh] = batch[j][0][:, yl:yh, xl:xh]\n                    lam_batch[i] = lam\n                else:\n                    mixed = mixed.astype(np.float32) * lam + batch[j][0].astype(np.float32) * (1 - lam)\n                    np.rint(mixed, out=mixed)\n            output[i] += torch.from_numpy(mixed.astype(np.uint8))\n        if half:\n            lam_batch = np.concatenate((lam_batch, np.ones(num_elem)))\n        return torch.tensor(lam_batch).unsqueeze(1)\n\n    def _mix_pair_collate(self, output, batch):\n        batch_size = len(batch)\n        lam_batch, use_cutmix = self._params_per_elem(batch_size // 2)\n        for i in range(batch_size // 2):\n            j = batch_size - i - 1\n            lam = lam_batch[i]\n            mixed_i = batch[i][0]\n            mixed_j = batch[j][0]\n            assert 0 <= lam <= 1.0\n            if lam < 1.:\n                if use_cutmix[i]:\n                    (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                        output.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n                    patch_i = mixed_i[:, yl:yh, xl:xh].copy()\n                    mixed_i[:, yl:yh, xl:xh] =", "doc_id": "67ab8580-4e23-40ce-be15-b3eebe8165cf", "embedding": null, "doc_hash": "95aa1450e6ba08217998596e62e1aa57cafb8be17406ad4db8c493c9109e0e71", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 13554, "end": 14813}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "2": "4849a526-4b72-4c38-a00c-3d19ce8b0083", "3": "aac418b7-a2f9-4d05-b013-23d1c0cadd30"}}, "__type__": "1"}, "aac418b7-a2f9-4d05-b013-23d1c0cadd30": {"__data__": {"text": "   patch_i = mixed_i[:, yl:yh, xl:xh].copy()\n                    mixed_i[:, yl:yh, xl:xh] = mixed_j[:, yl:yh, xl:xh]\n                    mixed_j[:, yl:yh, xl:xh] = patch_i\n                    lam_batch[i] = lam\n                else:\n                    mixed_temp = mixed_i.astype(np.float32) * lam + mixed_j.astype(np.float32) * (1 - lam)\n                    mixed_j = mixed_j.astype(np.float32) * lam + mixed_i.astype(np.float32) * (1 - lam)\n                    mixed_i = mixed_temp\n                    np.rint(mixed_j, out=mixed_j)\n                    np.rint(mixed_i, out=mixed_i)\n            output[i] += torch.from_numpy(mixed_i.astype(np.uint8))\n            output[j] += torch.from_numpy(mixed_j.astype(np.uint8))\n        lam_batch = np.concatenate((lam_batch, lam_batch[::-1]))\n        return torch.tensor(lam_batch).unsqueeze(1)\n\n    def _mix_batch_collate(self, output, batch):\n        batch_size = len(batch)\n        lam, use_cutmix = self._params_per_batch()\n        if use_cutmix:\n            (yl, yh, xl, xh), lam = cutmix_bbox_and_lam(\n                output.shape, lam, ratio_minmax=self.cutmix_minmax, correct_lam=self.correct_lam)\n        for i in range(batch_size):\n            j = batch_size - i - 1\n            mixed = batch[i][0]\n  ", "doc_id": "aac418b7-a2f9-4d05-b013-23d1c0cadd30", "embedding": null, "doc_hash": "8350d1a0c12cb647c32cffdcb7a45bf96b0e316a71c08c7896ad9f1a5e6d7a0b", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 14840, "end": 16094}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "2": "67ab8580-4e23-40ce-be15-b3eebe8165cf", "3": "0516f22f-f15e-412d-afc6-c369c38e7770"}}, "__type__": "1"}, "0516f22f-f15e-412d-afc6-c369c38e7770": {"__data__": {"text": "       for i in range(batch_size):\n            j = batch_size - i - 1\n            mixed = batch[i][0]\n            if lam != 1.:\n                if use_cutmix:\n                    mixed = mixed.copy()  # don't want to modify the original while iterating\n                    mixed[:, yl:yh, xl:xh] = batch[j][0][:, yl:yh, xl:xh]\n                else:\n                    mixed = mixed.astype(np.float32) * lam + batch[j][0].astype(np.float32) * (1 - lam)\n                    np.rint(mixed, out=mixed)\n            output[i] += torch.from_numpy(mixed.astype(np.uint8))\n        return lam\n\n    def __call__(self, batch, _=None):\n        batch_size = len(batch)\n        assert batch_size % 2 == 0, 'Batch size should be even when using this'\n        half = 'half' in self.mode\n        if half:\n            batch_size //= 2\n        output = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n        if self.mode == 'elem' or self.mode == 'half':\n            lam = self._mix_elem_collate(output, batch, half=half)\n        elif self.mode == 'pair':\n            lam = self._mix_pair_collate(output, batch)\n        else:\n            lam = self._mix_batch_collate(output, batch)\n        target = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n        target = mixup_target(target, self.num_classes, lam, self.label_smoothing, device='cpu')\n        target = target[:batch_size]\n        return output, target\n\n", "doc_id": "0516f22f-f15e-412d-afc6-c369c38e7770", "embedding": null, "doc_hash": "329082ab1dfeca17746004f105df0ddbaf9f52f9ccebf4f0a551fe03ab86f86e", "extra_info": {"file_path": "TinyViT/data/augmentation/mixup.py", "file_name": "mixup.py"}, "node_info": {"start": 16051, "end": 17470}, "relationships": {"1": "360b191a56a8f8d8c63e532bc6f9a29b4dd2c129", "2": "aac418b7-a2f9-4d05-b013-23d1c0cadd30"}}, "__type__": "1"}, "de6ad467-b17f-421f-8d3a-eb53befdc293": {"__data__": {"text": "from .parser_factory import create_parser\n", "doc_id": "de6ad467-b17f-421f-8d3a-eb53befdc293", "embedding": null, "doc_hash": "ee03388453f7d960e1d857f0124f539fef9f7aae049f488e57a64bcba6d607d2", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/__init__.py", "file_name": "__init__.py"}, "node_info": {"start": 0, "end": 42}, "relationships": {"1": "eeb44e3714eff75028e15214e0e65bf2afebd86c"}}, "__type__": "1"}, "6db0684e-84d4-40b3-acfb-66d27fdd275c": {"__data__": {"text": "import os\n\n\ndef load_class_map(map_or_filename, root=''):\n    if isinstance(map_or_filename, dict):\n        assert dict, 'class_map dict must be non-empty'\n        return map_or_filename\n    class_map_path = map_or_filename\n    if not os.path.exists(class_map_path):\n        class_map_path = os.path.join(root, class_map_path)\n        assert os.path.exists(class_map_path), 'Cannot locate specified class map file (%s)' % map_or_filename\n    class_map_ext = os.path.splitext(map_or_filename)[-1].lower()\n    if class_map_ext == '.txt':\n        with open(class_map_path) as f:\n            class_to_idx = {v.strip(): k for k, v in enumerate(f)}\n    else:\n        assert False, f'Unsupported class map file extension ({class_map_ext}).'\n    return class_to_idx\n\n", "doc_id": "6db0684e-84d4-40b3-acfb-66d27fdd275c", "embedding": null, "doc_hash": "e8fb317ca35bc9b243334ec256951c1ecde24babab7e6f4ae0d31c38ba16f75c", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/class_map.py", "file_name": "class_map.py"}, "node_info": {"start": 0, "end": 759}, "relationships": {"1": "6b6fe453a48d2d8db874d230632216cbd4b01753"}}, "__type__": "1"}, "0167559f-bba8-4004-b227-e9ed43aed324": {"__data__": {"text": "IMG_EXTENSIONS = ('.png', '.jpg', '.jpeg')\n", "doc_id": "0167559f-bba8-4004-b227-e9ed43aed324", "embedding": null, "doc_hash": "58947e6617a7c84deadda2c63a4c1d5adedc0e277ab1483e74b8c5d5bb796e2b", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/constants.py", "file_name": "constants.py"}, "node_info": {"start": 0, "end": 43}, "relationships": {"1": "e7ba484e729b7ac976b2cedaa43be1c3b308eeeb"}}, "__type__": "1"}, "5affb53d-f7f9-47fe-8d28-ca463aae0857": {"__data__": {"text": "from abc import abstractmethod\n\n\nclass Parser:\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def _filename(self, index, basename=False, absolute=False):\n        pass\n\n    def filename(self, index, basename=False, absolute=False):\n        return self._filename(index, basename=basename, absolute=absolute)\n\n    def filenames(self, basename=False, absolute=False):\n        return [self._filename(index, basename=basename, absolute=absolute) for index in range(len(self))]\n\n", "doc_id": "5affb53d-f7f9-47fe-8d28-ca463aae0857", "embedding": null, "doc_hash": "5a3b3116b3e8d69857de8619e7c4aec02b876d245010f03a7d128c45cc2886fb", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser.py", "file_name": "parser.py"}, "node_info": {"start": 0, "end": 487}, "relationships": {"1": "76ab6d18283644702424d0ff2af5832d6d6dd3b7"}}, "__type__": "1"}, "9aa0dbc2-9983-498a-a445-8bd38a71de17": {"__data__": {"text": "import os\n\nfrom .parser_image_folder import ParserImageFolder\nfrom .parser_image_tar import ParserImageTar\nfrom .parser_image_in_tar import ParserImageInTar\n\n\ndef create_parser(name, root, split='train', **kwargs):\n    name = name.lower()\n    name = name.split('/', 2)\n    prefix = ''\n    if len(name) > 1:\n        prefix = name[0]\n    name = name[-1]\n\n    # FIXME improve the selection right now just tfds prefix or fallback path, will need options to\n    # explicitly select other options shortly\n    if prefix == 'tfds':\n        from .parser_tfds import ParserTfds  # defer tensorflow import\n        parser = ParserTfds(root, name, split=split, **kwargs)\n    else:\n        assert os.path.exists(root)\n        # default fallback path (backwards compat), use image tar if root is a .tar file, otherwise image folder\n        # FIXME support split here, in parser?\n        if os.path.isfile(root) and os.path.splitext(root)[1] == '.tar':\n            parser = ParserImageInTar(root, **kwargs)\n        else:\n            parser = ParserImageFolder(root, **kwargs)\n    return parser\n", "doc_id": "9aa0dbc2-9983-498a-a445-8bd38a71de17", "embedding": null, "doc_hash": "aff3f9007df58f1180bd839cda992038e81386d733860ae596af74f199e5a659", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_factory.py", "file_name": "parser_factory.py"}, "node_info": {"start": 0, "end": 1078}, "relationships": {"1": "892090adb78ca5157a44170e79d0e3b239129f72"}}, "__type__": "1"}, "e9906428-08f3-4084-8e68-891ed74e7e52": {"__data__": {"text": "\"\"\" A dataset parser that reads images from folders\n\nFolders are scannerd recursively to find image files. Labels are based\non the folder hierarchy, just leaf folders by default.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\n\nfrom timm.utils.misc import natural_key\n\nfrom .parser import Parser\nfrom .class_map import load_class_map\nfrom .constants import IMG_EXTENSIONS\n\n\ndef find_images_and_targets(folder, types=IMG_EXTENSIONS, class_to_idx=None, leaf_name_only=True, sort=True):\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder, topdown=False, followlinks=True):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else ''\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n    if class_to_idx is None:\n        # building class index\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    images_and_targets = [(f, class_to_idx[l]) for f, l in zip(filenames, labels) if l in class_to_idx]\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n    return images_and_targets, class_to_idx\n\n\nclass ParserImageFolder(Parser):\n\n    def __init__(\n            self,\n            root,\n            class_map=''):\n        super().__init__()\n\n        self.root = root\n        class_to_idx = None\n    ", "doc_id": "e9906428-08f3-4084-8e68-891ed74e7e52", "embedding": null, "doc_hash": "304069846fecef0244c3f766ce0ddafcb3d61fd2a9f965ee78d56ca71677160a", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_image_folder.py", "file_name": "parser_image_folder.py"}, "node_info": {"start": 0, "end": 1682}, "relationships": {"1": "ed349009a4caed92e290e05637eca20e46cc275b", "3": "fbc292cd-e83d-4bc5-a6b1-9ab86afd0b57"}}, "__type__": "1"}, "fbc292cd-e83d-4bc5-a6b1-9ab86afd0b57": {"__data__": {"text": "          class_map=''):\n        super().__init__()\n\n        self.root = root\n        class_to_idx = None\n        if class_map:\n            class_to_idx = load_class_map(class_map, root)\n        self.samples, self.class_to_idx = find_images_and_targets(root, class_to_idx=class_to_idx)\n        if len(self.samples) == 0:\n            raise RuntimeError(\n                f'Found 0 images in subfolders of {root}. Supported image extensions are {\", \".join(IMG_EXTENSIONS)}')\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        return open(path, 'rb'), target\n\n    def __len__(self):\n        return len(self.samples)\n\n    def _filename(self, index, basename=False, absolute=False):\n        filename = self.samples[index][0]\n        if basename:\n            filename = os.path.basename(filename)\n        elif not absolute:\n            filename = os.path.relpath(filename, self.root)\n        return filename\n", "doc_id": "fbc292cd-e83d-4bc5-a6b1-9ab86afd0b57", "embedding": null, "doc_hash": "b039ad1fa1bb5681ecd87897e3bd9b7ddd2d2ca90760f9d816b338451ec53a2c", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_image_folder.py", "file_name": "parser_image_folder.py"}, "node_info": {"start": 1572, "end": 2508}, "relationships": {"1": "ed349009a4caed92e290e05637eca20e46cc275b", "2": "e9906428-08f3-4084-8e68-891ed74e7e52"}}, "__type__": "1"}, "5c7898ec-facb-4f3a-886c-af08fba1ea76": {"__data__": {"text": "\"\"\" A dataset parser that reads tarfile based datasets\n\nThis parser can read and extract image samples from:\n* a single tar of image files\n* a folder of multiple tarfiles containing imagefiles\n* a tar of tars containing image files\n\nLabels are based on the combined folder and/or tar name structure.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\nimport tarfile\nimport pickle\nimport logging\nimport numpy as np\nfrom glob import glob\nfrom typing import List, Dict\n\nfrom timm.utils.misc import natural_key\n\nfrom .parser import Parser\nfrom .class_map import load_class_map\nfrom .constants import IMG_EXTENSIONS\n\n\n_logger = logging.getLogger(__name__)\nCACHE_FILENAME_SUFFIX = '_tarinfos.pickle'\n\n\nclass TarState:\n\n    def __init__(self, tf: tarfile.TarFile = None, ti: tarfile.TarInfo = None):\n        self.tf: tarfile.TarFile = tf\n        self.ti: tarfile.TarInfo = ti\n        self.children: Dict[str, TarState] = {}  # child states (tars within tars)\n\n    def reset(self):\n        self.tf = None\n\n\ndef _extract_tarinfo(tf: tarfile.TarFile, parent_info: Dict, extensions=IMG_EXTENSIONS):\n    sample_count = 0\n    for i, ti in enumerate(tf):\n        if not ti.isfile():\n            continue\n        dirname, basename = os.path.split(ti.path)\n        name, ext = os.path.splitext(basename)\n        ext = ext.lower()\n        if ext == '.tar':\n            with tarfile.open(fileobj=tf.extractfile(ti), mode='r|') as ctf:\n                child_info = dict(\n                    name=ti.name, path=os.path.join(parent_info['path'], name), ti=ti, children=[], samples=[])\n                sample_count += _extract_tarinfo(ctf, child_info, extensions=extensions)\n                _logger.debug(f'{i}/?. Extracted child tarinfos from {ti.name}. {len(child_info[\"samples\"])} images.')\n           ", "doc_id": "5c7898ec-facb-4f3a-886c-af08fba1ea76", "embedding": null, "doc_hash": "b36c9d99dee669f07d9e74e18ef86fd1ed9ff921fe2f49400d070a3b5e43d006", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_image_in_tar.py", "file_name": "parser_image_in_tar.py"}, "node_info": {"start": 0, "end": 1797}, "relationships": {"1": "c6ada962ca96eaa7d770014ba130c6de5b36b6ec", "3": "df2b9594-54a4-4b8d-929f-55b105551264"}}, "__type__": "1"}, "df2b9594-54a4-4b8d-929f-55b105551264": {"__data__": {"text": "             _logger.debug(f'{i}/?. Extracted child tarinfos from {ti.name}. {len(child_info[\"samples\"])} images.')\n                parent_info['children'].append(child_info)\n        elif ext in extensions:\n            parent_info['samples'].append(ti)\n            sample_count += 1\n    return sample_count\n\n\ndef extract_tarinfos(root, class_name_to_idx=None, cache_tarinfo=None, extensions=IMG_EXTENSIONS, sort=True):\n    root_is_tar = False\n    if os.path.isfile(root):\n        assert os.path.splitext(root)[-1].lower() == '.tar'\n        tar_filenames = [root]\n        root, root_name = os.path.split(root)\n        root_name = os.path.splitext(root_name)[0]\n        root_is_tar = True\n    else:\n        root_name = root.strip(os.path.sep).split(os.path.sep)[-1]\n        tar_filenames = glob(os.path.join(root, '*.tar'), recursive=True)\n    num_tars = len(tar_filenames)\n    tar_bytes = sum([os.path.getsize(f) for f in tar_filenames])\n    assert num_tars, f'No .tar files found at specified path ({root}).'\n\n    _logger.info(f'Scanning {tar_bytes/1024**2:.2f}MB of tar files...')\n    info = dict(tartrees=[])\n    cache_path = ''\n    if cache_tarinfo is None:\n        cache_tarinfo = True if tar_bytes > 10*1024**3 else False  # FIXME magic number, 10GB\n    if cache_tarinfo:\n        cache_filename = '_' + root_name + CACHE_FILENAME_SUFFIX\n        cache_path = os.path.join(root, cache_filename)\n    if os.path.exists(cache_path):\n        _logger.info(f'Reading tar info from cache file {cache_path}.')\n        with open(cache_path, 'rb') as pf:\n            info = pickle.load(pf)\n ", "doc_id": "df2b9594-54a4-4b8d-929f-55b105551264", "embedding": null, "doc_hash": "62028fcdc45f7361763889c8e5e27506abcffd91fba291791479b4c10d497d65", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_image_in_tar.py", "file_name": "parser_image_in_tar.py"}, "node_info": {"start": 1702, "end": 3286}, "relationships": {"1": "c6ada962ca96eaa7d770014ba130c6de5b36b6ec", "2": "5c7898ec-facb-4f3a-886c-af08fba1ea76", "3": "f2ced293-e6fd-4d8b-8e0c-edffadd66a84"}}, "__type__": "1"}, "f2ced293-e6fd-4d8b-8e0c-edffadd66a84": {"__data__": {"text": "tar info from cache file {cache_path}.')\n        with open(cache_path, 'rb') as pf:\n            info = pickle.load(pf)\n        assert len(info['tartrees']) == num_tars, \"Cached tartree len doesn't match number of tarfiles\"\n    else:\n        for i, fn in enumerate(tar_filenames):\n            path = '' if root_is_tar else os.path.splitext(os.path.basename(fn))[0]\n            with tarfile.open(fn, mode='r|') as tf:  # tarinfo scans done in streaming mode\n                parent_info = dict(name=os.path.relpath(fn, root), path=path, ti=None, children=[], samples=[])\n                num_samples = _extract_tarinfo(tf, parent_info, extensions=extensions)\n                num_children = len(parent_info[\"children\"])\n                _logger.debug(\n                    f'{i}/{num_tars}. Extracted tarinfos from {fn}. {num_children} children, {num_samples} samples.')\n            info['tartrees'].append(parent_info)\n        if cache_path:\n            _logger.info(f'Writing tar info to cache file {cache_path}.')\n            with open(cache_path, 'wb') as pf:\n                pickle.dump(info, pf)\n\n    samples = []\n    labels = []\n    build_class_map = False\n    if class_name_to_idx is None:\n        build_class_map = True\n\n    # Flatten tartree info into lists of samples and targets w/ targets based on label id via\n    # class map arg or from unique paths.\n    # NOTE: currently only flattening up to two-levels, filesystem .tars and then one level of sub-tar children\n    # this covers my current use cases and keeps things a little easier to test for now.\n    tarfiles = []\n\n    def _label_from_paths(*path, leaf_only=True):\n        path =", "doc_id": "f2ced293-e6fd-4d8b-8e0c-edffadd66a84", "embedding": null, "doc_hash": "40aa0cfba66e881aaa53d972cb221e0865f3c2c58816a54df1b4e84c815ffc45", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_image_in_tar.py", "file_name": "parser_image_in_tar.py"}, "node_info": {"start": 3295, "end": 4938}, "relationships": {"1": "c6ada962ca96eaa7d770014ba130c6de5b36b6ec", "2": "df2b9594-54a4-4b8d-929f-55b105551264", "3": "dc23b975-1086-4ff4-8550-cfe41913dbe7"}}, "__type__": "1"}, "dc23b975-1086-4ff4-8550-cfe41913dbe7": {"__data__": {"text": "   # this covers my current use cases and keeps things a little easier to test for now.\n    tarfiles = []\n\n    def _label_from_paths(*path, leaf_only=True):\n        path = os.path.join(*path).strip(os.path.sep)\n        return path.split(os.path.sep)[-1] if leaf_only else path.replace(os.path.sep, '_')\n\n    def _add_samples(info, fn):\n        added = 0\n        for s in info['samples']:\n            label = _label_from_paths(info['path'], os.path.dirname(s.path))\n            if not build_class_map and label not in class_name_to_idx:\n                continue\n            samples.append((s, fn, info['ti']))\n            labels.append(label)\n            added += 1\n        return added\n\n    _logger.info(f'Collecting samples and building tar states.')\n    for parent_info in info['tartrees']:\n        # if tartree has children, we assume all samples are at the child level\n        tar_name = None if root_is_tar else parent_info['name']\n        tar_state = TarState()\n        parent_added = 0\n        for child_info in parent_info['children']:\n            child_added = _add_samples(child_info, fn=tar_name)\n            if child_added:\n                tar_state.children[child_info['name']] = TarState(ti=child_info['ti'])\n            parent_added += child_added\n        parent_added += _add_samples(parent_info, fn=tar_name)\n        if parent_added:\n            tarfiles.append((tar_name, tar_state))\n    del info\n\n    if build_class_map:\n        # build class index\n        sorted_labels = list(sorted(set(labels), key=natural_key))\n        class_name_to_idx = {c: idx for idx, c in", "doc_id": "dc23b975-1086-4ff4-8550-cfe41913dbe7", "embedding": null, "doc_hash": "6e990fd204fd657211588f9a77bb134d0979aef76dcda2c3c72602b531cf49fe", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_image_in_tar.py", "file_name": "parser_image_in_tar.py"}, "node_info": {"start": 4896, "end": 6480}, "relationships": {"1": "c6ada962ca96eaa7d770014ba130c6de5b36b6ec", "2": "f2ced293-e6fd-4d8b-8e0c-edffadd66a84", "3": "9d091990-a81f-4829-b2a5-ce3982b79c92"}}, "__type__": "1"}, "9d091990-a81f-4829-b2a5-ce3982b79c92": {"__data__": {"text": "  # build class index\n        sorted_labels = list(sorted(set(labels), key=natural_key))\n        class_name_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n\n    _logger.info(f'Mapping targets and sorting samples.')\n    samples_and_targets = [(s, class_name_to_idx[l]) for s, l in zip(samples, labels) if l in class_name_to_idx]\n    if sort:\n        samples_and_targets = sorted(samples_and_targets, key=lambda k: natural_key(k[0][0].path))\n    samples, targets = zip(*samples_and_targets)\n    samples = np.array(samples)\n    targets = np.array(targets)\n    _logger.info(f'Finished processing {len(samples)} samples across {len(tarfiles)} tar files.')\n    return samples, targets, class_name_to_idx, tarfiles\n\n\nclass ParserImageInTar(Parser):\n    \"\"\" Multi-tarfile dataset parser where there is one .tar file per class\n    \"\"\"\n\n    def __init__(self, root, class_map='', cache_tarfiles=True, cache_tarinfo=None):\n        super().__init__()\n\n        class_name_to_idx = None\n        if class_map:\n            class_name_to_idx = load_class_map(class_map, root)\n        self.root = root\n        self.samples, self.targets, self.class_name_to_idx, tarfiles = extract_tarinfos(\n            self.root,\n            class_name_to_idx=class_name_to_idx,\n            cache_tarinfo=cache_tarinfo,\n            extensions=IMG_EXTENSIONS)\n        self.class_idx_to_name = {v: k for k, v in self.class_name_to_idx.items()}\n        if len(tarfiles) == 1 and tarfiles[0][0] is None:\n            self.root_is_tar = True\n            self.tar_state = tarfiles[0][1]\n     ", "doc_id": "9d091990-a81f-4829-b2a5-ce3982b79c92", "embedding": null, "doc_hash": "b85a56ef859729d476e62aea7d6b735e4f24f6980d438020425077a071cc93d1", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_image_in_tar.py", "file_name": "parser_image_in_tar.py"}, "node_info": {"start": 6505, "end": 8065}, "relationships": {"1": "c6ada962ca96eaa7d770014ba130c6de5b36b6ec", "2": "dc23b975-1086-4ff4-8550-cfe41913dbe7", "3": "ee1616c6-8d5c-4442-b95a-a1d8b4bd0a7a"}}, "__type__": "1"}, "ee1616c6-8d5c-4442-b95a-a1d8b4bd0a7a": {"__data__": {"text": "1 and tarfiles[0][0] is None:\n            self.root_is_tar = True\n            self.tar_state = tarfiles[0][1]\n        else:\n            self.root_is_tar = False\n            self.tar_state = dict(tarfiles)\n        self.cache_tarfiles = cache_tarfiles\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, index):\n        sample = self.samples[index]\n        target = self.targets[index]\n        sample_ti, parent_fn, child_ti = sample\n        parent_abs = os.path.join(self.root, parent_fn) if parent_fn else self.root\n\n        tf = None\n        cache_state = None\n        if self.cache_tarfiles:\n            cache_state = self.tar_state if self.root_is_tar else self.tar_state[parent_fn]\n            tf = cache_state.tf\n        if tf is None:\n            tf = tarfile.open(parent_abs)\n            if self.cache_tarfiles:\n                cache_state.tf = tf\n        if child_ti is not None:\n            ctf = cache_state.children[child_ti.name].tf if self.cache_tarfiles else None\n            if ctf is None:\n                ctf = tarfile.open(fileobj=tf.extractfile(child_ti))\n                if self.cache_tarfiles:\n                    cache_state.children[child_ti.name].tf = ctf\n            tf = ctf\n\n        return tf.extractfile(sample_ti), target\n\n    def _filename(self, index, basename=False, absolute=False):\n        filename = self.samples[index][0].name\n        if basename:\n            filename = os.path.basename(filename)\n    ", "doc_id": "ee1616c6-8d5c-4442-b95a-a1d8b4bd0a7a", "embedding": null, "doc_hash": "606ce2c5ac3be645ee45e8302190639dae7898c3cf2a9e3ce29b5376e6be8ba6", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_image_in_tar.py", "file_name": "parser_image_in_tar.py"}, "node_info": {"start": 8095, "end": 9565}, "relationships": {"1": "c6ada962ca96eaa7d770014ba130c6de5b36b6ec", "2": "9d091990-a81f-4829-b2a5-ce3982b79c92", "3": "d03d8a03-a100-4f5f-81ad-a0937dcf3a6a"}}, "__type__": "1"}, "d03d8a03-a100-4f5f-81ad-a0937dcf3a6a": {"__data__": {"text": "absolute=False):\n        filename = self.samples[index][0].name\n        if basename:\n            filename = os.path.basename(filename)\n        return filename\n", "doc_id": "d03d8a03-a100-4f5f-81ad-a0937dcf3a6a", "embedding": null, "doc_hash": "41b9ec3c4f0a6c2f98b0344dd0f4b86d9a1b2ad36918a1cbeba56249fbc56b1e", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_image_in_tar.py", "file_name": "parser_image_in_tar.py"}, "node_info": {"start": 9504, "end": 9663}, "relationships": {"1": "c6ada962ca96eaa7d770014ba130c6de5b36b6ec", "2": "ee1616c6-8d5c-4442-b95a-a1d8b4bd0a7a"}}, "__type__": "1"}, "dc3fd654-af14-4f3d-8614-33d930a50759": {"__data__": {"text": "\"\"\" A dataset parser that reads single tarfile based datasets\n\nThis parser can read datasets consisting if a single tarfile containing images.\nI am planning to deprecated it in favour of ParerImageInTar.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\nimport tarfile\n\nfrom .parser import Parser\nfrom .class_map import load_class_map\nfrom .constants import IMG_EXTENSIONS\nfrom timm.utils.misc import natural_key\n\n\ndef extract_tarinfo(tarfile, class_to_idx=None, sort=True):\n    files = []\n    labels = []\n    for ti in tarfile.getmembers():\n        if not ti.isfile():\n            continue\n        dirname, basename = os.path.split(ti.path)\n        label = os.path.basename(dirname)\n        ext = os.path.splitext(basename)[1]\n        if ext.lower() in IMG_EXTENSIONS:\n            files.append(ti)\n            labels.append(label)\n    if class_to_idx is None:\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    tarinfo_and_targets = [(f, class_to_idx[l]) for f, l in zip(files, labels) if l in class_to_idx]\n    if sort:\n        tarinfo_and_targets = sorted(tarinfo_and_targets, key=lambda k: natural_key(k[0].path))\n    return tarinfo_and_targets, class_to_idx\n\n\nclass ParserImageTar(Parser):\n    \"\"\" Single tarfile dataset where classes are mapped to folders within tar\n    NOTE: This class is being deprecated in favour of the more capable ParserImageInTar that can\n    operate on folders of tars or tars in tars.\n    \"\"\"\n    def __init__(self, root, class_map=''):\n        super().__init__()\n\n        class_to_idx = None\n        if class_map:\n            class_to_idx = load_class_map(class_map, root)\n        assert", "doc_id": "dc3fd654-af14-4f3d-8614-33d930a50759", "embedding": null, "doc_hash": "08d3c6f7f7334c07c69816610b1f4789ea701bc479235285f074d074e1dbeba7", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_image_tar.py", "file_name": "parser_image_tar.py"}, "node_info": {"start": 0, "end": 1765}, "relationships": {"1": "467537f479873bbc09fa2b576cdbde9d2a956e7b", "3": "90722ff0-2c72-4294-84f0-6f803ca8f249"}}, "__type__": "1"}, "90722ff0-2c72-4294-84f0-6f803ca8f249": {"__data__": {"text": "   class_to_idx = None\n        if class_map:\n            class_to_idx = load_class_map(class_map, root)\n        assert os.path.isfile(root)\n        self.root = root\n\n        with tarfile.open(root) as tf:  # cannot keep this open across processes, reopen later\n            self.samples, self.class_to_idx = extract_tarinfo(tf, class_to_idx)\n        self.imgs = self.samples\n        self.tarfile = None  # lazy init in __getitem__\n\n    def __getitem__(self, index):\n        if self.tarfile is None:\n            self.tarfile = tarfile.open(self.root)\n        tarinfo, target = self.samples[index]\n        fileobj = self.tarfile.extractfile(tarinfo)\n        return fileobj, target\n\n    def __len__(self):\n        return len(self.samples)\n\n    def _filename(self, index, basename=False, absolute=False):\n        filename = self.samples[index][0].name\n        if basename:\n            filename = os.path.basename(filename)\n        return filename\n", "doc_id": "90722ff0-2c72-4294-84f0-6f803ca8f249", "embedding": null, "doc_hash": "be17d547414f4d6d7a66a28acee1dcb0ecb1a86c42207e1aa0fd6bb2ad90580e", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_image_tar.py", "file_name": "parser_image_tar.py"}, "node_info": {"start": 1647, "end": 2589}, "relationships": {"1": "467537f479873bbc09fa2b576cdbde9d2a956e7b", "2": "dc3fd654-af14-4f3d-8614-33d930a50759"}}, "__type__": "1"}, "b5bddc95-0223-4e1a-adb9-299e83443d83": {"__data__": {"text": "\"\"\" Dataset parser interface that wraps TFDS datasets\n\nWraps many (most?) TFDS image-classification datasets\nfrom https://github.com/tensorflow/datasets\nhttps://www.tensorflow.org/datasets/catalog/overview#image_classification\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\nimport torch\nimport torch.distributed as dist\nfrom PIL import Image\n\ntry:\n    import tensorflow as tf\n    tf.config.set_visible_devices([], 'GPU')  # Hands off my GPU! (or pip install tensorflow-cpu)\n    import tensorflow_datasets as tfds\n    try:\n        tfds.even_splits('', 1, drop_remainder=False)  # non-buggy even_splits has drop_remainder arg\n        has_buggy_even_splits = False\n    except TypeError:\n        print(\"Warning: This version of tfds doesn't have the latest even_splits impl. \"\n              \"Please update or use tfds-nightly for better fine-grained split behaviour.\")\n        has_buggy_even_splits = True\nexcept ImportError as e:\n    print(e)\n    print(\"Please install tensorflow_datasets package `pip install tensorflow-datasets`.\")\n    exit(1)\nfrom .parser import Parser\n\n\nMAX_TP_SIZE = 8  # maximum TF threadpool size, only doing jpeg decodes and queuing activities\nSHUFFLE_SIZE = 8192  # examples to shuffle in DS queue\nPREFETCH_SIZE = 2048  # examples to prefetch\n\n\ndef even_split_indices(split, n, num_examples):\n    partitions = [round(i * num_examples / n) for i in range(n + 1)]\n    return [f\"{split}[{partitions[i]}:{partitions[i + 1]}]\" for i in range(n)]\n\n\ndef get_class_labels(info):\n    if 'label' not in info.features:\n        return {}\n    class_label = info.features['label']\n    class_to_idx = {n: class_label.str2int(n) for n in class_label.names}\n    return class_to_idx\n\n\nclass ParserTfds(Parser):\n    \"\"\" Wrap Tensorflow Datasets for use in PyTorch\n\n    There several things to be aware of:\n      * To prevent excessive examples being dropped per epoch w/ distributed training or multiplicity of\n         dataloader workers, the train iterator wraps to avoid returning partial batches that", "doc_id": "b5bddc95-0223-4e1a-adb9-299e83443d83", "embedding": null, "doc_hash": "d1a0631612f4e3fef119f9a932ccfc3deaff3319aac71da105f6cfc2dce55522", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_tfds.py", "file_name": "parser_tfds.py"}, "node_info": {"start": 0, "end": 2028}, "relationships": {"1": "ee5893c4c84a555eaf98dad03a0cd30c7dafb25e", "3": "98e8d743-2611-448a-8fa4-32a4796e4d73"}}, "__type__": "1"}, "98e8d743-2611-448a-8fa4-32a4796e4d73": {"__data__": {"text": "things to be aware of:\n      * To prevent excessive examples being dropped per epoch w/ distributed training or multiplicity of\n         dataloader workers, the train iterator wraps to avoid returning partial batches that trigger drop_last\n         https://github.com/pytorch/pytorch/issues/33413\n      * With PyTorch IterableDatasets, each worker in each replica operates in isolation, the final batch\n        from each worker could be a different size. For training this is worked around by option above, for\n        validation extra examples are inserted iff distributed mode is enabled so that the batches being reduced\n        across replicas are of same size. This will slightly alter the results, distributed validation will not be\n        100% correct. This is similar to common handling in DistributedSampler for normal Datasets but a bit worse\n        since there are up to N * J extra examples with IterableDatasets.\n      * The sharding (splitting of dataset into TFRecord) files imposes limitations on the number of\n        replicas and dataloader workers you can use. For really small datasets that only contain a few shards\n        you may have to train non-distributed w/ 1-2 dataloader workers. This is likely not a huge concern as the\n        benefit of distributed training or fast dataloading should be much less for small datasets.\n      * This wrapper is currently configured to return individual, decompressed image examples from the TFDS\n        dataset. The augmentation (transforms) and batching is still done in PyTorch. It would be possible\n        to specify TF augmentation fn and return augmented batches w/ some modifications to other downstream\n        components.\n\n    \"\"\"\n\n    def __init__(\n            self,\n            root,\n            name,\n            split='train',\n            is_training=False,\n            batch_size=None,\n            download=False,\n            repeats=0,\n            seed=42,\n            input_name='image',\n            input_image='RGB',\n            target_name='label',\n      ", "doc_id": "98e8d743-2611-448a-8fa4-32a4796e4d73", "embedding": null, "doc_hash": "e0f16e33dad8719650ed014883d33e6d0758fbbb94258a8e77116a0e8788b750", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_tfds.py", "file_name": "parser_tfds.py"}, "node_info": {"start": 1852, "end": 3893}, "relationships": {"1": "ee5893c4c84a555eaf98dad03a0cd30c7dafb25e", "2": "b5bddc95-0223-4e1a-adb9-299e83443d83", "3": "57125805-a724-43f2-8448-7915f58f4e3e"}}, "__type__": "1"}, "57125805-a724-43f2-8448-7915f58f4e3e": {"__data__": {"text": "           input_name='image',\n            input_image='RGB',\n            target_name='label',\n            target_image='',\n            prefetch_size=None,\n            shuffle_size=None,\n            max_threadpool_size=None\n    ):\n        \"\"\" Tensorflow-datasets Wrapper\n\n        Args:\n            root: root data dir (ie your TFDS_DATA_DIR. not dataset specific sub-dir)\n            name: tfds dataset name (eg `imagenet2012`)\n            split: tfds dataset split (can use all TFDS split strings eg `train[:10%]`)\n            is_training: training mode, shuffle enabled, dataset len rounded by batch_size\n            batch_size: batch_size to use to unsure total examples % batch_size == 0 in training across all dis nodes\n            download: download and build TFDS dataset if set, otherwise must use tfds CLI\n            repeats: iterate through (repeat) the dataset this many times per iteration (once if 0 or 1)\n            seed: common seed for shard shuffle across all distributed/worker instances\n            input_name: name of Feature to return as data (input)\n            input_image: image mode if input is an image (currently PIL mode string)\n            target_name: name of Feature to return as target (label)\n            target_image: image mode if target is an image (currently PIL mode string)\n            prefetch_size: override default tf.data prefetch buffer size\n            shuffle_size: override default tf.data shuffle buffer size\n            max_threadpool_size: override default threadpool size for tf.data\n        \"\"\"\n        super().__init__()\n        self.root = root\n        self.split = split\n        self.is_training = is_training\n        if self.is_training:\n            assert", "doc_id": "57125805-a724-43f2-8448-7915f58f4e3e", "embedding": null, "doc_hash": "97d066076416a263224d5c88865d1c9881a6b410e67345ec54a7f2445dda6ed6", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_tfds.py", "file_name": "parser_tfds.py"}, "node_info": {"start": 4011, "end": 5725}, "relationships": {"1": "ee5893c4c84a555eaf98dad03a0cd30c7dafb25e", "2": "98e8d743-2611-448a-8fa4-32a4796e4d73", "3": "d7b286a9-25cc-48e7-bc5c-ff2592b3e85a"}}, "__type__": "1"}, "d7b286a9-25cc-48e7-bc5c-ff2592b3e85a": {"__data__": {"text": " self.root = root\n        self.split = split\n        self.is_training = is_training\n        if self.is_training:\n            assert batch_size is not None, \\\n                \"Must specify batch_size in training mode for reasonable behaviour w/ TFDS wrapper\"\n        self.batch_size = batch_size\n        self.repeats = repeats\n        self.common_seed = seed  # a seed that's fixed across all worker / distributed instances\n\n        # performance settings\n        self.prefetch_size = prefetch_size or PREFETCH_SIZE\n        self.shuffle_size = shuffle_size or SHUFFLE_SIZE\n        self.max_threadpool_size = max_threadpool_size or MAX_TP_SIZE\n\n        # TFDS builder and split information\n        self.input_name = input_name  # FIXME support tuples / lists of inputs and targets and full range of Feature\n        self.input_image = input_image\n        self.target_name = target_name\n        self.target_image = target_image\n        self.builder = tfds.builder(name, data_dir=root)\n        # NOTE: the tfds command line app can be used download & prepare datasets if you don't enable download flag\n        if download:\n            self.builder.download_and_prepare()\n        self.class_to_idx = get_class_labels(self.builder.info) if self.target_name == 'label' else {}\n        self.split_info = self.builder.info.splits[split]\n        self.num_examples = self.split_info.num_examples\n\n        # Distributed world state\n        self.dist_rank = 0\n        self.dist_num_replicas = 1\n        if dist.is_available() and dist.is_initialized() and dist.get_world_size() > 1:\n            self.dist_rank = dist.get_rank()\n            self.dist_num_replicas = dist.get_world_size()\n\n        # Attributes that are updated in _lazy_init, including the tf.data pipeline itself\n       ", "doc_id": "d7b286a9-25cc-48e7-bc5c-ff2592b3e85a", "embedding": null, "doc_hash": "b87749222697f0e65540521bc040835a9cdd44fdc0fa84583221e058abff9490", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_tfds.py", "file_name": "parser_tfds.py"}, "node_info": {"start": 5699, "end": 7471}, "relationships": {"1": "ee5893c4c84a555eaf98dad03a0cd30c7dafb25e", "2": "57125805-a724-43f2-8448-7915f58f4e3e", "3": "0e2475e8-744f-4f3a-aa18-0db3bf6225ff"}}, "__type__": "1"}, "0e2475e8-744f-4f3a-aa18-0db3bf6225ff": {"__data__": {"text": "         self.dist_num_replicas = dist.get_world_size()\n\n        # Attributes that are updated in _lazy_init, including the tf.data pipeline itself\n        self.global_num_workers = 1\n        self.worker_info = None\n        self.worker_seed = 0  # seed unique to each work instance\n        self.subsplit = None  # set when data is distributed across workers using sub-splits\n        self.ds = None  # initialized lazily on each dataloader worker process\n\n    def _lazy_init(self):\n        \"\"\" Lazily initialize the dataset.\n\n        This is necessary to init the Tensorflow dataset pipeline in the (dataloader) process that\n        will be using the dataset instance. The __init__ method is called on the main process,\n        this will be called in a dataloader worker process.\n\n        NOTE: There will be problems if you try to re-use this dataset across different loader/worker\n        instances once it has been initialized. Do not call any dataset methods that can call _lazy_init\n        before it is passed to dataloader.\n        \"\"\"\n        worker_info = torch.utils.data.get_worker_info()\n\n        # setup input context to split dataset across distributed processes\n        num_workers = 1\n        global_worker_id = 0\n        if worker_info is not None:\n            self.worker_info = worker_info\n            self.worker_seed = worker_info.seed\n            num_workers = worker_info.num_workers\n            self.global_num_workers = self.dist_num_replicas * num_workers\n            global_worker_id = self.dist_rank * num_workers + worker_info.id\n\n            \"\"\" Data sharding\n            InputContext will assign subset of underlying TFRecord files to each 'pipeline' if used.\n            My understanding is that using split, the underling TFRecord files will shuffle (shuffle_files=True)\n            between the splits each iteration, but that understanding could be wrong.\n\n            I am currently", "doc_id": "0e2475e8-744f-4f3a-aa18-0db3bf6225ff", "embedding": null, "doc_hash": "ffa57ba1e5a69bf1145d7c31790f1b04041ab46cf872c7400156c7cfa8481df0", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_tfds.py", "file_name": "parser_tfds.py"}, "node_info": {"start": 7441, "end": 9357}, "relationships": {"1": "ee5893c4c84a555eaf98dad03a0cd30c7dafb25e", "2": "d7b286a9-25cc-48e7-bc5c-ff2592b3e85a", "3": "bd533916-c65f-4efb-969f-78e60146d8af"}}, "__type__": "1"}, "bd533916-c65f-4efb-969f-78e60146d8af": {"__data__": {"text": "that using split, the underling TFRecord files will shuffle (shuffle_files=True)\n            between the splits each iteration, but that understanding could be wrong.\n\n            I am currently using a mix of InputContext shard assignment and fine-grained sub-splits for distributing\n            the data across workers. For training InputContext is used to assign shards to nodes unless num_shards\n            in dataset < total number of workers. Otherwise sub-split API is used for datasets without enough shards or\n            for validation where we can't drop examples and need to avoid minimize uneven splits to avoid padding.\n            \"\"\"\n            should_subsplit = self.global_num_workers > 1 and (\n                    self.split_info.num_shards < self.global_num_workers or not self.is_training)\n            if should_subsplit:\n                # split the dataset w/o using sharding for more even examples / worker, can result in less optimal\n                # read patterns for distributed training (overlap across shards) so better to use InputContext there\n                if has_buggy_even_splits:\n                    # my even_split workaround doesn't work on subsplits, upgrade tfds!\n                    if not isinstance(self.split_info, tfds.core.splits.SubSplitInfo):\n                        subsplits = even_split_indices(self.split, self.global_num_workers, self.num_examples)\n                        self.subsplit = subsplits[global_worker_id]\n                else:\n                    subsplits = tfds.even_splits(self.split, self.global_num_workers)\n                    self.subsplit = subsplits[global_worker_id]\n\n        input_context = None\n        if self.global_num_workers > 1 and self.subsplit is None:\n       ", "doc_id": "bd533916-c65f-4efb-969f-78e60146d8af", "embedding": null, "doc_hash": "1899c67ee18d27d534ade9830fff893c610a0e5ff5b99580ef6ee3f06f63b042", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_tfds.py", "file_name": "parser_tfds.py"}, "node_info": {"start": 9327, "end": 11075}, "relationships": {"1": "ee5893c4c84a555eaf98dad03a0cd30c7dafb25e", "2": "0e2475e8-744f-4f3a-aa18-0db3bf6225ff", "3": "54f957d4-7d3c-4580-ba76-e886a0a79094"}}, "__type__": "1"}, "54f957d4-7d3c-4580-ba76-e886a0a79094": {"__data__": {"text": "  self.subsplit = subsplits[global_worker_id]\n\n        input_context = None\n        if self.global_num_workers > 1 and self.subsplit is None:\n            # set input context to divide shards among distributed replicas\n            input_context = tf.distribute.InputContext(\n                num_input_pipelines=self.global_num_workers,\n                input_pipeline_id=global_worker_id,\n                num_replicas_in_sync=self.dist_num_replicas  # FIXME does this arg have any impact?\n            )\n        read_config = tfds.ReadConfig(\n            shuffle_seed=self.common_seed,\n            shuffle_reshuffle_each_iteration=True,\n            input_context=input_context)\n        ds = self.builder.as_dataset(\n            split=self.subsplit or self.split, shuffle_files=self.is_training, read_config=read_config)\n        # avoid overloading threading w/ combo of TF ds threads + PyTorch workers\n        options = tf.data.Options()\n        thread_member = 'threading' if hasattr(options, 'threading') else 'experimental_threading'\n        getattr(options, thread_member).private_threadpool_size = max(1, self.max_threadpool_size // num_workers)\n        getattr(options, thread_member).max_intra_op_parallelism = 1\n        ds = ds.with_options(options)\n        if self.is_training or self.repeats > 1:\n            # to prevent excessive drop_last batch behaviour w/ IterableDatasets\n            # see warnings at https://pytorch.org/docs/stable/data.html#multi-process-data-loading\n            ds = ds.repeat()  # allow wrap around and break iteration manually\n        if self.is_training:\n            ds = ds.shuffle(min(self.num_examples, self.shuffle_size) // self.global_num_workers, seed=self.worker_seed)\n  ", "doc_id": "54f957d4-7d3c-4580-ba76-e886a0a79094", "embedding": null, "doc_hash": "d6e7dba7aca7927d566171005182a814d577c4278973a032715883f00c8b8383", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_tfds.py", "file_name": "parser_tfds.py"}, "node_info": {"start": 11112, "end": 12827}, "relationships": {"1": "ee5893c4c84a555eaf98dad03a0cd30c7dafb25e", "2": "bd533916-c65f-4efb-969f-78e60146d8af", "3": "8bb32434-5d33-4217-a0d4-4972edb06bd5"}}, "__type__": "1"}, "8bb32434-5d33-4217-a0d4-4972edb06bd5": {"__data__": {"text": "  if self.is_training:\n            ds = ds.shuffle(min(self.num_examples, self.shuffle_size) // self.global_num_workers, seed=self.worker_seed)\n        ds = ds.prefetch(min(self.num_examples // self.global_num_workers, self.prefetch_size))\n        self.ds = tfds.as_numpy(ds)\n\n    def __iter__(self):\n        if self.ds is None:\n            self._lazy_init()\n\n        # Compute a rounded up sample count that is used to:\n        #   1. make batches even cross workers & replicas in distributed validation.\n        #     This adds extra examples and will slightly alter validation results.\n        #   2. determine loop ending condition in training w/ repeat enabled so that only full batch_size\n        #     batches are produced (underlying tfds iter wraps around)\n        target_example_count = math.ceil(max(1, self.repeats) * self.num_examples / self.global_num_workers)\n        if self.is_training:\n            # round up to nearest batch_size per worker-replica\n            target_example_count = math.ceil(target_example_count / self.batch_size) * self.batch_size\n\n        # Iterate until exhausted or sample count hits target when training (ds.repeat enabled)\n        example_count = 0\n        for example in self.ds:\n            input_data = example[self.input_name]\n            if self.input_image:\n                input_data = Image.fromarray(input_data, mode=self.input_image)\n            target_data = example[self.target_name]\n            if self.target_image:\n                target_data = Image.fromarray(target_data, mode=self.target_image)\n            yield input_data, target_data\n            example_count += 1\n            if self.is_training and example_count >= target_example_count:\n              ", "doc_id": "8bb32434-5d33-4217-a0d4-4972edb06bd5", "embedding": null, "doc_hash": "7d7a4aa505552d7cc1fa27c8ab8568e1b00037bc58d96fd9dea72768a0823a93", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_tfds.py", "file_name": "parser_tfds.py"}, "node_info": {"start": 12818, "end": 14538}, "relationships": {"1": "ee5893c4c84a555eaf98dad03a0cd30c7dafb25e", "2": "54f957d4-7d3c-4580-ba76-e886a0a79094", "3": "5b33e043-9b68-4b59-9cca-60dd29da9fd8"}}, "__type__": "1"}, "5b33e043-9b68-4b59-9cca-60dd29da9fd8": {"__data__": {"text": "           example_count += 1\n            if self.is_training and example_count >= target_example_count:\n                # Need to break out of loop when repeat() is enabled for training w/ oversampling\n                # this results in extra examples per epoch but seems more desirable than dropping\n                # up to N*J batches per epoch (where N = num distributed processes, and J = num worker processes)\n                break\n\n        # Pad across distributed nodes (make counts equal by adding examples)\n        if not self.is_training and self.dist_num_replicas > 1 and self.subsplit is not None and \\\n                0 < example_count < target_example_count:\n            # Validation batch padding only done for distributed training where results are reduced across nodes.\n            # For single process case, it won't matter if workers return different batch sizes.\n            # If using input_context or % based splits, sample count can vary significantly across workers and this\n            # approach should not be used (hence disabled if self.subsplit isn't set).\n            while example_count < target_example_count:\n                yield input_data, target_data  # yield prev sample again\n                example_count += 1\n\n    def __len__(self):\n        # this is just an estimate and does not factor in extra examples added to pad batches based on\n        # complete worker & replica info (not available until init in dataloader).\n        return math.ceil(max(1, self.repeats) * self.num_examples / self.dist_num_replicas)\n\n    def _filename(self, index, basename=False, absolute=False):\n        assert False, \"Not supported\"  # no random access to examples\n\n    def filenames(self, basename=False, absolute=False):\n        \"\"\" Return all filenames in dataset, overrides base\"\"\"\n        if self.ds is None:\n            self._lazy_init()\n        names = []\n        for sample in self.ds:\n", "doc_id": "5b33e043-9b68-4b59-9cca-60dd29da9fd8", "embedding": null, "doc_hash": "d182eef74379e923ab9ac71f522af54030217431be46e4ae841c5ffcab1f45cd", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_tfds.py", "file_name": "parser_tfds.py"}, "node_info": {"start": 14587, "end": 16503}, "relationships": {"1": "ee5893c4c84a555eaf98dad03a0cd30c7dafb25e", "2": "8bb32434-5d33-4217-a0d4-4972edb06bd5", "3": "35aa54c1-f2ae-45b2-82f1-0cd607f1846e"}}, "__type__": "1"}, "35aa54c1-f2ae-45b2-82f1-0cd607f1846e": {"__data__": {"text": "base\"\"\"\n        if self.ds is None:\n            self._lazy_init()\n        names = []\n        for sample in self.ds:\n            if len(names) > self.num_examples:\n                break  # safety for ds.repeat() case\n            if 'file_name' in sample:\n                name = sample['file_name']\n            elif 'filename' in sample:\n                name = sample['filename']\n            elif 'id' in sample:\n                name = sample['id']\n            else:\n                assert False, \"No supported name field present\"\n            names.append(name)\n        return names\n", "doc_id": "35aa54c1-f2ae-45b2-82f1-0cd607f1846e", "embedding": null, "doc_hash": "7f16208d8a9bfc0b2d9f1b78e3620a30ce90aa59fa810f19767d7b99cd181181", "extra_info": {"file_path": "TinyViT/data/augmentation/parsers/parser_tfds.py", "file_name": "parser_tfds.py"}, "node_info": {"start": 16462, "end": 17043}, "relationships": {"1": "ee5893c4c84a555eaf98dad03a0cd30c7dafb25e", "2": "5b33e043-9b68-4b59-9cca-60dd29da9fd8"}}, "__type__": "1"}, "7c2f200e-2820-4e85-99c3-d728ee8a7f04": {"__data__": {"text": "\"\"\" Random Erasing (Cutout)\n\nOriginally inspired by impl at https://github.com/zhunzhong07/Random-Erasing, Apache 2.0\nCopyright Zhun Zhong & Liang Zheng\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nfrom .aug_random import random, np_random\nimport numpy as np\nimport math\nimport torch\n\n\ndef _get_pixels(per_pixel, rand_color, patch_size, dtype=torch.float32, device='cuda'):\n    # NOTE I've seen CUDA illegal memory access errors being caused by the normal_()\n    # paths, flip the order so normal is run on CPU if this becomes a problem\n    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n    if not per_pixel and not rand_color:\n        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)\n    if per_pixel:\n        shape = patch_size\n    elif rand_color:\n        shape = (patch_size[0], 1, 1)\n    # normal_\n    seed = random.randint(0, 1 << 30)\n    bg = np.random.MT19937(seed)\n    g = np.random.Generator(bg)\n    x = g.normal(size=shape)\n    return torch.tensor(x, dtype=dtype, device=device)\n\n\nclass RandomErasing:\n    \"\"\" Randomly selects a rectangle region in an image and erases its pixels.\n        'Random Erasing Data Augmentation' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n\n        This variant of RandomErasing is intended to be applied to either a batch\n        or single image tensor after it has been normalized by dataset mean and std.\n    Args:\n         probability: Probability that the Random Erasing operation will be performed.\n         min_area: Minimum percentage of erased area wrt input image area.\n         max_area: Maximum percentage of erased area wrt input image area.\n         min_aspect: Minimum aspect ratio of erased area.\n         mode: pixel color mode, one of 'const', 'rand', or 'pixel'\n            'const' - erase block is constant color of 0 for all channels\n            'rand'  - erase block is same per-channel random (normal) color\n           ", "doc_id": "7c2f200e-2820-4e85-99c3-d728ee8a7f04", "embedding": null, "doc_hash": "4215a3d9ef1b5be7a9e0f660dc82d80711c31aa2b6dd45ffe65430dc5bbaedf9", "extra_info": {"file_path": "TinyViT/data/augmentation/random_erasing.py", "file_name": "random_erasing.py"}, "node_info": {"start": 0, "end": 1968}, "relationships": {"1": "e2c035190289caef297e1a952a8411835f67dd8e", "3": "c7e240c8-115c-448a-8404-184ac239af09"}}, "__type__": "1"}, "c7e240c8-115c-448a-8404-184ac239af09": {"__data__": {"text": "     'const' - erase block is constant color of 0 for all channels\n            'rand'  - erase block is same per-channel random (normal) color\n            'pixel' - erase block is per-pixel random (normal) color\n        max_count: maximum number of erasing blocks per image, area per box is scaled by count.\n            per-image count is randomly chosen between 1 and this value.\n    \"\"\"\n    REF_H = 224\n    REF_W = 224\n\n    def __init__(\n            self,\n            probability=0.5, min_area=0.02, max_area=1/3, min_aspect=0.3, max_aspect=None,\n            mode='const', min_count=1, max_count=None, num_splits=0, device='cuda'):\n        self.probability = probability\n        self.min_area = min_area\n        self.max_area = max_area\n        max_aspect = max_aspect or 1 / min_aspect\n        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n        self.min_count = min_count\n        self.max_count = max_count or min_count\n        self.num_splits = num_splits\n        self.mode = mode.lower()\n        self.rand_color = False\n        self.per_pixel = False\n        if self.mode == 'rand':\n            self.rand_color = True  # per block random normal\n        elif self.mode == 'pixel':\n            self.per_pixel = True  # per pixel random normal\n        else:\n            assert not self.mode or self.mode == 'const'\n        self.device = device\n\n    def _erase(self, img, chan, img_h, img_w, dtype):\n        if random.random() > self.probability:\n            return\n        count = self.min_count if self.min_count == self.max_count else \\\n            random.randint(self.min_count,", "doc_id": "c7e240c8-115c-448a-8404-184ac239af09", "embedding": null, "doc_hash": "6396aa842c34242ba3f157c2dd7fe42773516818e471fbe299b53d9ff8163b23", "extra_info": {"file_path": "TinyViT/data/augmentation/random_erasing.py", "file_name": "random_erasing.py"}, "node_info": {"start": 1864, "end": 3473}, "relationships": {"1": "e2c035190289caef297e1a952a8411835f67dd8e", "2": "7c2f200e-2820-4e85-99c3-d728ee8a7f04", "3": "7811493a-bbb6-4c52-8f2b-52534f634851"}}, "__type__": "1"}, "7811493a-bbb6-4c52-8f2b-52534f634851": {"__data__": {"text": "           return\n        count = self.min_count if self.min_count == self.max_count else \\\n            random.randint(self.min_count, self.max_count)\n        ref_h, ref_w = self.REF_H, self.REF_W\n        ref_area = ref_h * ref_w\n        area = img_h * img_w\n        for _ in range(count):\n            for attempt in range(10):\n                r1 = random.uniform(self.min_area, self.max_area)\n                target_area = r1 * ref_area / count\n                r2 = random.uniform(*self.log_aspect_ratio)\n                aspect_ratio = math.exp(r2)\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < ref_w and h < ref_h:\n                    top = random.randint(0, ref_h - h)\n                    left = random.randint(0, ref_w - w)\n                    # ref -> now\n                    top = min(int(round(top * img_h / ref_h)), img_h - 1)\n                    left = min(int(round(left * img_w / ref_w)), img_w - 1)\n                    h = min(int(round(h * img_h / ref_h)), img_h - top)\n                    w = min(int(round(w * img_w / ref_w)), img_w - left)\n                    img[:, top:top + h, left:left + w] = _get_pixels(\n                       ", "doc_id": "7811493a-bbb6-4c52-8f2b-52534f634851", "embedding": null, "doc_hash": "107be0eb8fed950287d978ccafe1fc7a6d10db37f2ce17e6b95b0ad0452c78c1", "extra_info": {"file_path": "TinyViT/data/augmentation/random_erasing.py", "file_name": "random_erasing.py"}, "node_info": {"start": 3484, "end": 4750}, "relationships": {"1": "e2c035190289caef297e1a952a8411835f67dd8e", "2": "c7e240c8-115c-448a-8404-184ac239af09", "3": "81ce16e8-a033-4a7f-bdf0-d497d08c9531"}}, "__type__": "1"}, "81ce16e8-a033-4a7f-bdf0-d497d08c9531": {"__data__": {"text": "                  img[:, top:top + h, left:left + w] = _get_pixels(\n                        self.per_pixel, self.rand_color, (chan, h, w),\n                        dtype=dtype, device=self.device)\n                    break\n\n    def __call__(self, input):\n        if len(input.size()) == 3:\n            self._erase(input, *input.size(), input.dtype)\n        else:\n            batch_size, chan, img_h, img_w = input.size()\n            # skip first slice of batch if num_splits is set (for clean portion of samples)\n            batch_start = batch_size // self.num_splits if self.num_splits > 1 else 0\n            for i in range(batch_start, batch_size):\n                self._erase(input[i], chan, img_h, img_w, input.dtype)\n        return input\n\n    def __repr__(self):\n        # NOTE simplified state for repr\n        fs = self.__class__.__name__ + f'(p={self.probability}, mode={self.mode}'\n        fs += f', count=({self.min_count}, {self.max_count}))'\n        return fs\n", "doc_id": "81ce16e8-a033-4a7f-bdf0-d497d08c9531", "embedding": null, "doc_hash": "a2ad7ee99f8725e9f0ac33ef81bd9bb88d0bc53001dc620e8c2b6b28ddc3f5e7", "extra_info": {"file_path": "TinyViT/data/augmentation/random_erasing.py", "file_name": "random_erasing.py"}, "node_info": {"start": 4754, "end": 5726}, "relationships": {"1": "e2c035190289caef297e1a952a8411835f67dd8e", "2": "7811493a-bbb6-4c52-8f2b-52534f634851"}}, "__type__": "1"}, "345bd739-daf2-4390-981e-9ae0bc2bfda8": {"__data__": {"text": "\"\"\" Real labels evaluator for ImageNet\nPaper: `Are we done with ImageNet?` - https://arxiv.org/abs/2006.07159\nBased on Numpy example at https://github.com/google-research/reassessed-imagenet\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport os\nimport json\nimport numpy as np\n\n\nclass RealLabelsImagenet:\n\n    def __init__(self, filenames, real_json='real.json', topk=(1, 5)):\n        with open(real_json) as real_labels:\n            real_labels = json.load(real_labels)\n            real_labels = {f'ILSVRC2012_val_{i + 1:08d}.JPEG': labels for i, labels in enumerate(real_labels)}\n        self.real_labels = real_labels\n        self.filenames = filenames\n        assert len(self.filenames) == len(self.real_labels)\n        self.topk = topk\n        self.is_correct = {k: [] for k in topk}\n        self.sample_idx = 0\n\n    def add_result(self, output):\n        maxk = max(self.topk)\n        _, pred_batch = output.topk(maxk, 1, True, True)\n        pred_batch = pred_batch.cpu().numpy()\n        for pred in pred_batch:\n            filename = self.filenames[self.sample_idx]\n            filename = os.path.basename(filename)\n            if self.real_labels[filename]:\n                for k in self.topk:\n                    self.is_correct[k].append(\n                        any([p in self.real_labels[filename] for p in pred[:k]]))\n            self.sample_idx += 1\n\n    def get_accuracy(self, k=None):\n        if k is None:\n            return {k: float(np.mean(self.is_correct[k])) * 100 for k in self.topk}\n        else:\n            return", "doc_id": "345bd739-daf2-4390-981e-9ae0bc2bfda8", "embedding": null, "doc_hash": "a26d4aa577ec9c6974b0598d305d8988499ea27e54ac32991a2355e66769781b", "extra_info": {"file_path": "TinyViT/data/augmentation/real_labels.py", "file_name": "real_labels.py"}, "node_info": {"start": 0, "end": 1548}, "relationships": {"1": "939c34867e7915ce3e4cc7da04a5bc1653ec4f2c", "3": "7b228eea-75dd-4f59-bd60-cf8e4172b4c8"}}, "__type__": "1"}, "7b228eea-75dd-4f59-bd60-cf8e4172b4c8": {"__data__": {"text": "           return {k: float(np.mean(self.is_correct[k])) * 100 for k in self.topk}\n        else:\n            return float(np.mean(self.is_correct[k])) * 100\n", "doc_id": "7b228eea-75dd-4f59-bd60-cf8e4172b4c8", "embedding": null, "doc_hash": "03cbf7b9fddca5a14aaf8e5a553437ccee9601724c1f570d1112edd32696b7b5", "extra_info": {"file_path": "TinyViT/data/augmentation/real_labels.py", "file_name": "real_labels.py"}, "node_info": {"start": 1433, "end": 1590}, "relationships": {"1": "939c34867e7915ce3e4cc7da04a5bc1653ec4f2c", "2": "345bd739-daf2-4390-981e-9ae0bc2bfda8"}}, "__type__": "1"}, "01653f63-c7d5-49ce-af09-40b0213f66fc": {"__data__": {"text": "\"\"\" Tensorflow Preprocessing Adapter\n\nAllows use of Tensorflow preprocessing pipeline in PyTorch Transform\n\nCopyright of original Tensorflow code below.\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\n\n# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"ImageNet preprocessing for MnasNet.\"\"\"\nimport tensorflow as tf\nimport numpy as np\n\nIMAGE_SIZE = 224\nCROP_PADDING = 32\n\n\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n    \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n\n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n    Args:\n      image_bytes: `Tensor` of binary image data.\n      bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n          where each coordinate is [0, 1) and the coordinates are arranged\n          as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n          image.\n      min_object_covered: An optional `float`. Defaults to", "doc_id": "01653f63-c7d5-49ce-af09-40b0213f66fc", "embedding": null, "doc_hash": "df1d97ba546bff7c5bdd4a08783d2f5461cedbc4d4e2f2d34f6308cd880eb97b", "extra_info": {"file_path": "TinyViT/data/augmentation/tf_preprocessing.py", "file_name": "tf_preprocessing.py"}, "node_info": {"start": 0, "end": 1887}, "relationships": {"1": "44b4a3af7372c6865b1cdddda0a8da0ccc6b93a0", "3": "1d4e7247-4c68-41c3-a5b1-39e9f075ec5c"}}, "__type__": "1"}, "1d4e7247-4c68-41c3-a5b1-39e9f075ec5c": {"__data__": {"text": "     as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n          image.\n      min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n          area of the image must contain at least this fraction of any bounding\n          box supplied.\n      aspect_ratio_range: An optional list of `float`s. The cropped area of the\n          image must have an aspect ratio = width / height within this range.\n      area_range: An optional list of `float`s. The cropped area of the image\n          must contain a fraction of the supplied image within in this range.\n      max_attempts: An optional `int`. Number of attempts at generating a cropped\n          region of the image of the specified constraints. After `max_attempts`\n          failures, return the entire image.\n      scope: Optional `str` for name scope.\n    Returns:\n      cropped image `Tensor`\n    \"\"\"\n    with tf.name_scope(scope, 'distorted_bounding_box_crop', [image_bytes, bbox]):\n        shape = tf.image.extract_jpeg_shape(image_bytes)\n        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n            shape,\n            bounding_boxes=bbox,\n            min_object_covered=min_object_covered,\n            aspect_ratio_range=aspect_ratio_range,\n            area_range=area_range,\n            max_attempts=max_attempts,\n            use_image_if_no_bounding_boxes=True)\n        bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n        # Crop the image to the specified bounding box.\n        offset_y, offset_x, _ = tf.unstack(bbox_begin)\n        target_height, target_width, _ = tf.unstack(bbox_size)\n        crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n        image =", "doc_id": "1d4e7247-4c68-41c3-a5b1-39e9f075ec5c", "embedding": null, "doc_hash": "c761e773aafb5624d6df56261dbcf00f181d2bd1bcccf4b12d8fb2e0d34934cf", "extra_info": {"file_path": "TinyViT/data/augmentation/tf_preprocessing.py", "file_name": "tf_preprocessing.py"}, "node_info": {"start": 1778, "end": 3500}, "relationships": {"1": "44b4a3af7372c6865b1cdddda0a8da0ccc6b93a0", "2": "01653f63-c7d5-49ce-af09-40b0213f66fc", "3": "36579e79-9794-4cac-a9e3-6fcda655a8b4"}}, "__type__": "1"}, "36579e79-9794-4cac-a9e3-6fcda655a8b4": {"__data__": {"text": "  target_height, target_width, _ = tf.unstack(bbox_size)\n        crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n        image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n\n        return image\n\n\ndef _at_least_x_are_equal(a, b, x):\n    \"\"\"At least `x` of `a` and `b` `Tensors` are equal.\"\"\"\n    match = tf.equal(a, b)\n    match = tf.cast(match, tf.int32)\n    return tf.greater_equal(tf.reduce_sum(match), x)\n\n\ndef _decode_and_random_crop(image_bytes, image_size, resize_method):\n    \"\"\"Make a random crop of image_size.\"\"\"\n    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n    image = distorted_bounding_box_crop(\n        image_bytes,\n        bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=(3. / 4, 4. / 3.),\n        area_range=(0.08, 1.0),\n        max_attempts=10,\n        scope=None)\n    original_shape = tf.image.extract_jpeg_shape(image_bytes)\n    bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n    image = tf.cond(\n        bad,\n        lambda: _decode_and_center_crop(image_bytes, image_size),\n        lambda: tf.image.resize([image], [image_size, image_size], resize_method)[0])\n\n    return image\n\n\ndef _decode_and_center_crop(image_bytes, image_size, resize_method):\n    \"\"\"Crops to center of image with padding then scales image_size.\"\"\"\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n     ", "doc_id": "36579e79-9794-4cac-a9e3-6fcda655a8b4", "embedding": null, "doc_hash": "632aba40447a4ab74b21332b4b5883d041a41f21b2e0b5a327dc187f1281e942", "extra_info": {"file_path": "TinyViT/data/augmentation/tf_preprocessing.py", "file_name": "tf_preprocessing.py"}, "node_info": {"start": 3485, "end": 5125}, "relationships": {"1": "44b4a3af7372c6865b1cdddda0a8da0ccc6b93a0", "2": "1d4e7247-4c68-41c3-a5b1-39e9f075ec5c", "3": "eb757c94-a8eb-44a7-b07d-f3da39308a32"}}, "__type__": "1"}, "eb757c94-a8eb-44a7-b07d-f3da39308a32": {"__data__": {"text": "       ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n\n    return image\n\n\ndef _flip(image):\n    \"\"\"Random horizontal image flip.\"\"\"\n    image = tf.image.random_flip_left_right(image)\n    return image\n\n\ndef preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic'):\n    \"\"\"Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    \"\"\"\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n\ndef preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic'):\n    \"\"\"Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to", "doc_id": "eb757c94-a8eb-44a7-b07d-f3da39308a32", "embedding": null, "doc_hash": "85660aa7e96c5dbfefb32a2a428747fdfe859626b53b77ab9173d49d111ebe02", "extra_info": {"file_path": "TinyViT/data/augmentation/tf_preprocessing.py", "file_name": "tf_preprocessing.py"}, "node_info": {"start": 5152, "end": 6997}, "relationships": {"1": "44b4a3af7372c6865b1cdddda0a8da0ccc6b93a0", "2": "36579e79-9794-4cac-a9e3-6fcda655a8b4", "3": "479a153c-da35-4d6a-8915-4e6b3fc8586b"}}, "__type__": "1"}, "479a153c-da35-4d6a-8915-4e6b3fc8586b": {"__data__": {"text": "   \"\"\"Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    \"\"\"\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == 'bicubic' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n\ndef preprocess_image(image_bytes,\n                     is_training=False,\n                     use_bfloat16=False,\n                     image_size=IMAGE_SIZE,\n                     interpolation='bicubic'):\n    \"\"\"Preprocesses the given image.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      is_training: `bool` for whether the preprocessing is for training.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor` with value range of [0, 255].\n    \"\"\"\n    if is_training:\n        return preprocess_for_train(image_bytes, use_bfloat16, image_size, interpolation)\n    else:\n        return preprocess_for_eval(image_bytes, use_bfloat16, image_size, interpolation)\n\n\nclass TfPreprocessTransform:\n\n    def __init__(self, is_training=False, size=224, interpolation='bicubic'):\n        self.is_training = is_training\n        self.size = size[0] if isinstance(size, tuple) else size\n        self.interpolation =", "doc_id": "479a153c-da35-4d6a-8915-4e6b3fc8586b", "embedding": null, "doc_hash": "d0c7309b7ab1a8e4d2bb888b651878911c438d0be67391734766e1caf31f8b84", "extra_info": {"file_path": "TinyViT/data/augmentation/tf_preprocessing.py", "file_name": "tf_preprocessing.py"}, "node_info": {"start": 6955, "end": 8740}, "relationships": {"1": "44b4a3af7372c6865b1cdddda0a8da0ccc6b93a0", "2": "eb757c94-a8eb-44a7-b07d-f3da39308a32", "3": "ed887cf6-0891-4b10-a3ac-a39cc1759831"}}, "__type__": "1"}, "ed887cf6-0891-4b10-a3ac-a39cc1759831": {"__data__": {"text": "       self.is_training = is_training\n        self.size = size[0] if isinstance(size, tuple) else size\n        self.interpolation = interpolation\n        self._image_bytes = None\n        self.process_image = self._build_tf_graph()\n        self.sess = None\n\n    def _build_tf_graph(self):\n        with tf.device('/cpu:0'):\n            self._image_bytes = tf.placeholder(\n                shape=[],\n                dtype=tf.string,\n            )\n            img = preprocess_image(\n                self._image_bytes, self.is_training, False, self.size, self.interpolation)\n        return img\n\n    def __call__(self, image_bytes):\n        if self.sess is None:\n            self.sess = tf.Session()\n        img = self.sess.run(self.process_image, feed_dict={self._image_bytes: image_bytes})\n        img = img.round().clip(0, 255).astype(np.uint8)\n        if img.ndim < 3:\n            img = np.expand_dims(img, axis=-1)\n        img = np.rollaxis(img, 2)  # HWC to CHW\n        return img\n", "doc_id": "ed887cf6-0891-4b10-a3ac-a39cc1759831", "embedding": null, "doc_hash": "bb067477fe13e0b009ba62ba48ddb397184d16c5489ebdeaa2ec7c92ec7ecb39", "extra_info": {"file_path": "TinyViT/data/augmentation/tf_preprocessing.py", "file_name": "tf_preprocessing.py"}, "node_info": {"start": 8752, "end": 9733}, "relationships": {"1": "44b4a3af7372c6865b1cdddda0a8da0ccc6b93a0", "2": "479a153c-da35-4d6a-8915-4e6b3fc8586b"}}, "__type__": "1"}, "c0d04927-bf51-4e0f-8067-cbd675a5778d": {"__data__": {"text": "import torch\nimport torchvision.transforms.functional as F\ntry:\n    from torchvision.transforms.functional import InterpolationMode\n    has_interpolation_mode = True\nexcept ImportError:\n    has_interpolation_mode = False\nfrom PIL import Image\nimport warnings\nimport math\nfrom .aug_random import random\nimport numpy as np\n\n\nclass ToNumpy:\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return np_img\n\n\nclass ToTensor:\n\n    def __init__(self, dtype=torch.float32):\n        self.dtype = dtype\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return torch.from_numpy(np_img).to(dtype=self.dtype)\n\n\n_pil_interpolation_to_str = {\n    Image.NEAREST: 'nearest',\n    Image.BILINEAR: 'bilinear',\n    Image.BICUBIC: 'bicubic',\n    Image.BOX: 'box',\n    Image.HAMMING: 'hamming',\n    Image.LANCZOS: 'lanczos',\n}\n_str_to_pil_interpolation = {b: a for a, b in _pil_interpolation_to_str.items()}\n\n\nif has_interpolation_mode:\n    _torch_interpolation_to_str = {\n        InterpolationMode.NEAREST: 'nearest',\n        InterpolationMode.BILINEAR: 'bilinear',\n        InterpolationMode.BICUBIC: 'bicubic',\n        InterpolationMode.BOX: 'box',\n        InterpolationMode.HAMMING: 'hamming',\n        InterpolationMode.LANCZOS: 'lanczos',\n    }\n", "doc_id": "c0d04927-bf51-4e0f-8067-cbd675a5778d", "embedding": null, "doc_hash": "1c68b28923292d5476a9a64ae40227e1c0ec08a950bef411235620198cf41730", "extra_info": {"file_path": "TinyViT/data/augmentation/transforms.py", "file_name": "transforms.py"}, "node_info": {"start": 0, "end": 1582}, "relationships": {"1": "a482298d3978271732a6e4db9737656e71b984f2", "3": "63e36658-7fe3-4f8d-b57c-ab37d776d9fa"}}, "__type__": "1"}, "63e36658-7fe3-4f8d-b57c-ab37d776d9fa": {"__data__": {"text": "    InterpolationMode.BOX: 'box',\n        InterpolationMode.HAMMING: 'hamming',\n        InterpolationMode.LANCZOS: 'lanczos',\n    }\n    _str_to_torch_interpolation = {b: a for a, b in _torch_interpolation_to_str.items()}\nelse:\n    _pil_interpolation_to_torch = {}\n    _torch_interpolation_to_str = {}\n\n\ndef str_to_pil_interp(mode_str):\n    return _str_to_pil_interpolation[mode_str]\n\n\ndef str_to_interp_mode(mode_str):\n    if has_interpolation_mode:\n        return _str_to_torch_interpolation[mode_str]\n    else:\n        return _str_to_pil_interpolation[mode_str]\n\n\ndef interp_mode_to_str(mode):\n    if has_interpolation_mode:\n        return _torch_interpolation_to_str[mode]\n    else:\n        return _pil_interpolation_to_str[mode]\n\n\n_RANDOM_INTERPOLATION = (str_to_interp_mode('bilinear'), str_to_interp_mode('bicubic'))\n\n\nclass RandomResizedCropAndInterpolation:\n    \"\"\"Crop the given PIL Image to random size and aspect ratio with random interpolation.\n\n    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n    is finally resized to given size.\n    This is popularly used to train the Inception networks.\n\n    Args:\n        size: expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped\n        interpolation: Default: PIL.Image.BILINEAR\n    \"\"\"\n\n    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.),\n                 interpolation='bilinear'):\n        if isinstance(size, (list, tuple)):\n            self.size = tuple(size)\n        else:\n            self.size =", "doc_id": "63e36658-7fe3-4f8d-b57c-ab37d776d9fa", "embedding": null, "doc_hash": "4939e2e0c49c1b213812fa63ad3b86f3fab562a35844aafc34ec06e9a99372c6", "extra_info": {"file_path": "TinyViT/data/augmentation/transforms.py", "file_name": "transforms.py"}, "node_info": {"start": 1478, "end": 3192}, "relationships": {"1": "a482298d3978271732a6e4db9737656e71b984f2", "2": "c0d04927-bf51-4e0f-8067-cbd675a5778d", "3": "dad04241-328d-49b2-bf34-583633f98936"}}, "__type__": "1"}, "dad04241-328d-49b2-bf34-583633f98936": {"__data__": {"text": "    if isinstance(size, (list, tuple)):\n            self.size = tuple(size)\n        else:\n            self.size = (size, size)\n        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n            warnings.warn(\"range should be of kind (min, max)\")\n\n        if interpolation == 'random':\n            self.interpolation = _RANDOM_INTERPOLATION\n        else:\n            self.interpolation = str_to_interp_mode(interpolation)\n        self.scale = scale\n        self.ratio = ratio\n\n    @staticmethod\n    def get_params(img, scale, ratio):\n        \"\"\"Get parameters for ``crop`` for a random sized crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        \"\"\"\n        area = img.size[0] * img.size[1]\n\n        for attempt in range(10):\n            target_area = random.uniform(*scale) * area\n            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w <= img.size[0] and h <= img.size[1]:\n                i = random.randint(0, img.size[1] - h)\n                j =", "doc_id": "dad04241-328d-49b2-bf34-583633f98936", "embedding": null, "doc_hash": "329afdb7106a52b7c290b0868b7396bdf04ec9e7fbc756499612597e57fa3414", "extra_info": {"file_path": "TinyViT/data/augmentation/transforms.py", "file_name": "transforms.py"}, "node_info": {"start": 3227, "end": 4703}, "relationships": {"1": "a482298d3978271732a6e4db9737656e71b984f2", "2": "63e36658-7fe3-4f8d-b57c-ab37d776d9fa", "3": "71c78f67-5c2c-4a8d-a07a-b21c4270eb30"}}, "__type__": "1"}, "71c78f67-5c2c-4a8d-a07a-b21c4270eb30": {"__data__": {"text": "and h <= img.size[1]:\n                i = random.randint(0, img.size[1] - h)\n                j = random.randint(0, img.size[0] - w)\n                return i, j, h, w\n\n        # Fallback to central crop\n        in_ratio = img.size[0] / img.size[1]\n        if in_ratio < min(ratio):\n            w = img.size[0]\n            h = int(round(w / min(ratio)))\n        elif in_ratio > max(ratio):\n            h = img.size[1]\n            w = int(round(h * max(ratio)))\n        else:  # whole image\n            w = img.size[0]\n            h = img.size[1]\n        i = (img.size[1] - h) // 2\n        j = (img.size[0] - w) // 2\n        return i, j, h, w\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be cropped and resized.\n\n        Returns:\n            PIL Image: Randomly cropped and resized image.\n        \"\"\"\n        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n        if isinstance(self.interpolation, (tuple, list)):\n            interpolation = random.choice(self.interpolation)\n        else:\n            interpolation = self.interpolation\n        return F.resized_crop(img, i, j, h, w, self.size, interpolation)\n\n    def __repr__(self):\n        if isinstance(self.interpolation, (tuple, list)):\n            interpolate_str = ' '.join([interp_mode_to_str(x) for x in self.interpolation])\n        else:\n           ", "doc_id": "71c78f67-5c2c-4a8d-a07a-b21c4270eb30", "embedding": null, "doc_hash": "0d0f156abcad08dce9e993b7fd2753dce78e1f2fcec6a20562e8ba479b23192e", "extra_info": {"file_path": "TinyViT/data/augmentation/transforms.py", "file_name": "transforms.py"}, "node_info": {"start": 4720, "end": 6087}, "relationships": {"1": "a482298d3978271732a6e4db9737656e71b984f2", "2": "dad04241-328d-49b2-bf34-583633f98936", "3": "46cd618b-9346-445f-a948-e8c00b8ec94f"}}, "__type__": "1"}, "46cd618b-9346-445f-a948-e8c00b8ec94f": {"__data__": {"text": "           interpolate_str = ' '.join([interp_mode_to_str(x) for x in self.interpolation])\n        else:\n            interpolate_str = interp_mode_to_str(self.interpolation)\n        format_string = self.__class__.__name__ + '(size={0}'.format(self.size)\n        format_string += ', scale={0}'.format(tuple(round(s, 4) for s in self.scale))\n        format_string += ', ratio={0}'.format(tuple(round(r, 4) for r in self.ratio))\n        format_string += ', interpolation={0})'.format(interpolate_str)\n        return format_string\n\n\n", "doc_id": "46cd618b-9346-445f-a948-e8c00b8ec94f", "embedding": null, "doc_hash": "be1eb3963c090feaad03594e13967e6e48d69fc9fbe57fadf494cac3be436894", "extra_info": {"file_path": "TinyViT/data/augmentation/transforms.py", "file_name": "transforms.py"}, "node_info": {"start": 6026, "end": 6555}, "relationships": {"1": "a482298d3978271732a6e4db9737656e71b984f2", "2": "71c78f67-5c2c-4a8d-a07a-b21c4270eb30"}}, "__type__": "1"}, "148e5af5-e25a-45ef-80f5-84a1505ea4ed": {"__data__": {"text": "\"\"\" Transforms Factory\nFactory methods for building image transforms for use with TIMM (PyTorch Image Models)\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport math\n\nimport torch\nfrom . import aug_tv_transforms as transforms\n\nfrom .constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, DEFAULT_CROP_PCT\nfrom .auto_augment import rand_augment_transform, augment_and_mix_transform, auto_augment_transform\nfrom .transforms import str_to_interp_mode, str_to_pil_interp, RandomResizedCropAndInterpolation, ToNumpy\nfrom .random_erasing import RandomErasing\n\n\ndef transforms_noaug_train(\n        img_size=224,\n        interpolation='bilinear',\n        use_prefetcher=False,\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n):\n    if interpolation == 'random':\n        # random interpolation not supported with no-aug\n        interpolation = 'bilinear'\n    tfl = [\n        transforms.Resize(img_size, interpolation=str_to_interp_mode(interpolation)),\n        transforms.CenterCrop(img_size)\n    ]\n    if use_prefetcher:\n        # prefetcher and collate will handle tensor conversion and norm\n        tfl += [ToNumpy()]\n    else:\n        tfl += [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=torch.tensor(mean),\n                std=torch.tensor(std))\n        ]\n    return transforms.Compose(tfl)\n\n\ndef transforms_imagenet_train(\n        img_size=224,\n        scale=None,\n        ratio=None,\n        hflip=0.5,\n        vflip=0.,\n        color_jitter=0.4,\n        auto_augment=None,\n        interpolation='random',\n        use_prefetcher=False,\n       ", "doc_id": "148e5af5-e25a-45ef-80f5-84a1505ea4ed", "embedding": null, "doc_hash": "f2ad06e0915e65ddddf287ebe96485fb024286e7da1dfd86b5779e06a427f47f", "extra_info": {"file_path": "TinyViT/data/augmentation/transforms_factory.py", "file_name": "transforms_factory.py"}, "node_info": {"start": 0, "end": 1627}, "relationships": {"1": "defea4b7d942fa503736f79e36e2bd9fa09a526a", "3": "7aa851a2-ea74-48cd-8410-c0933f23757e"}}, "__type__": "1"}, "7aa851a2-ea74-48cd-8410-c0933f23757e": {"__data__": {"text": "  color_jitter=0.4,\n        auto_augment=None,\n        interpolation='random',\n        use_prefetcher=False,\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n        re_prob=0.,\n        re_mode='const',\n        re_count=1,\n        re_num_splits=0,\n        separate=False,\n):\n    \"\"\"\n    If separate==True, the transforms are returned as a tuple of 3 separate transforms\n    for use in a mixing dataset that passes\n     * all data through the first (primary) transform, called the 'clean' data\n     * a portion of the data through the secondary transform\n     * normalizes and converts the branches above with the third, final transform\n    \"\"\"\n    scale = tuple(scale or (0.08, 1.0))  # default imagenet scale range\n    ratio = tuple(ratio or (3./4., 4./3.))  # default imagenet ratio range\n    primary_tfl = [\n        RandomResizedCropAndInterpolation(img_size, scale=scale, ratio=ratio, interpolation=interpolation)]\n    if hflip > 0.:\n        primary_tfl += [transforms.RandomHorizontalFlip(p=hflip)]\n    if vflip > 0.:\n        primary_tfl += [transforms.RandomVerticalFlip(p=vflip)]\n\n    secondary_tfl = []\n    if auto_augment:\n        assert isinstance(auto_augment, str)\n        if isinstance(img_size, (tuple, list)):\n            img_size_min = min(img_size)\n        else:\n            img_size_min = img_size\n        aa_params = dict(\n            translate_const=int(img_size_min * 0.45),\n            img_mean=tuple([min(255, round(255 * x)) for x in mean]),\n        )\n        if interpolation and interpolation != 'random':\n            aa_params['interpolation'] = str_to_pil_interp(interpolation)\n  ", "doc_id": "7aa851a2-ea74-48cd-8410-c0933f23757e", "embedding": null, "doc_hash": "718854bbd59097c36bcb5fa843e1a7b2b7818918057852bb64bf668d967e594a", "extra_info": {"file_path": "TinyViT/data/augmentation/transforms_factory.py", "file_name": "transforms_factory.py"}, "node_info": {"start": 1545, "end": 3174}, "relationships": {"1": "defea4b7d942fa503736f79e36e2bd9fa09a526a", "2": "148e5af5-e25a-45ef-80f5-84a1505ea4ed", "3": "dce3e166-d0de-4be0-8538-044df38100cd"}}, "__type__": "1"}, "dce3e166-d0de-4be0-8538-044df38100cd": {"__data__": {"text": "    )\n        if interpolation and interpolation != 'random':\n            aa_params['interpolation'] = str_to_pil_interp(interpolation)\n        if auto_augment.startswith('rand'):\n            secondary_tfl += [rand_augment_transform(auto_augment, aa_params)]\n        elif auto_augment.startswith('augmix'):\n            aa_params['translate_pct'] = 0.3\n            secondary_tfl += [augment_and_mix_transform(auto_augment, aa_params)]\n        else:\n            secondary_tfl += [auto_augment_transform(auto_augment, aa_params)]\n    elif color_jitter is not None:\n        # color jitter is enabled when not using AA\n        if isinstance(color_jitter, (list, tuple)):\n            # color jitter should be a 3-tuple/list if spec brightness/contrast/saturation\n            # or 4 if also augmenting hue\n            assert len(color_jitter) in (3, 4)\n        else:\n            # if it's a scalar, duplicate for brightness, contrast, and saturation, no hue\n            color_jitter = (float(color_jitter),) * 3\n        secondary_tfl += [transforms.ColorJitter(*color_jitter)]\n\n    final_tfl = []\n    if use_prefetcher:\n        # prefetcher and collate will handle tensor conversion and norm\n        final_tfl += [ToNumpy()]\n    else:\n        final_tfl += [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=torch.tensor(mean),\n                std=torch.tensor(std))\n        ]\n        if re_prob > 0.:\n            final_tfl.append(\n                RandomErasing(re_prob, mode=re_mode,", "doc_id": "dce3e166-d0de-4be0-8538-044df38100cd", "embedding": null, "doc_hash": "5db68a68d642ed1cf5f55a1c976e2ab48a2b2e7c0d38c91cbdb3553fec3a106e", "extra_info": {"file_path": "TinyViT/data/augmentation/transforms_factory.py", "file_name": "transforms_factory.py"}, "node_info": {"start": 3153, "end": 4670}, "relationships": {"1": "defea4b7d942fa503736f79e36e2bd9fa09a526a", "2": "7aa851a2-ea74-48cd-8410-c0933f23757e", "3": "74d3b28c-9e66-4700-876e-ad19cd492af8"}}, "__type__": "1"}, "74d3b28c-9e66-4700-876e-ad19cd492af8": {"__data__": {"text": "      if re_prob > 0.:\n            final_tfl.append(\n                RandomErasing(re_prob, mode=re_mode, max_count=re_count, num_splits=re_num_splits, device='cpu'))\n\n    if separate:\n        return transforms.Compose(primary_tfl), transforms.Compose(secondary_tfl), transforms.Compose(final_tfl)\n    else:\n        return transforms.Compose(primary_tfl + secondary_tfl + final_tfl)\n\n\ndef transforms_imagenet_eval(\n        img_size=224,\n        crop_pct=None,\n        interpolation='bilinear',\n        use_prefetcher=False,\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD):\n    crop_pct = crop_pct or DEFAULT_CROP_PCT\n\n    if isinstance(img_size, (tuple, list)):\n        assert len(img_size) == 2\n        if img_size[-1] == img_size[-2]:\n            # fall-back to older behaviour so Resize scales to shortest edge if target is square\n            scale_size = int(math.floor(img_size[0] / crop_pct))\n        else:\n            scale_size = tuple([int(x / crop_pct) for x in img_size])\n    else:\n        scale_size = int(math.floor(img_size / crop_pct))\n\n    tfl = [\n        transforms.Resize(scale_size, interpolation=str_to_interp_mode(interpolation)),\n        transforms.CenterCrop(img_size),\n    ]\n    if use_prefetcher:\n        # prefetcher and collate will handle tensor conversion and norm\n        tfl += [ToNumpy()]\n    else:\n        tfl += [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                     mean=torch.tensor(mean),\n             ", "doc_id": "74d3b28c-9e66-4700-876e-ad19cd492af8", "embedding": null, "doc_hash": "e5eb8283e13083207e54a014b67cb16661408cc4579a3f5544c24e43d606546f", "extra_info": {"file_path": "TinyViT/data/augmentation/transforms_factory.py", "file_name": "transforms_factory.py"}, "node_info": {"start": 4709, "end": 6207}, "relationships": {"1": "defea4b7d942fa503736f79e36e2bd9fa09a526a", "2": "dce3e166-d0de-4be0-8538-044df38100cd", "3": "feb00c1d-0c65-42a1-b540-cc261c19c998"}}, "__type__": "1"}, "feb00c1d-0c65-42a1-b540-cc261c19c998": {"__data__": {"text": "           transforms.Normalize(\n                     mean=torch.tensor(mean),\n                     std=torch.tensor(std))\n        ]\n\n    return transforms.Compose(tfl)\n\n\ndef create_transform(\n        input_size,\n        is_training=False,\n        use_prefetcher=False,\n        no_aug=False,\n        scale=None,\n        ratio=None,\n        hflip=0.5,\n        vflip=0.,\n        color_jitter=0.4,\n        auto_augment=None,\n        interpolation='bilinear',\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n        re_prob=0.,\n        re_mode='const',\n        re_count=1,\n        re_num_splits=0,\n        crop_pct=None,\n        tf_preprocessing=False,\n        separate=False):\n\n    if isinstance(input_size, (tuple, list)):\n        img_size = input_size[-2:]\n    else:\n        img_size = input_size\n\n    if tf_preprocessing and use_prefetcher:\n        assert not separate, \"Separate transforms not supported for TF preprocessing\"\n        from .tf_preprocessing import TfPreprocessTransform\n        transform = TfPreprocessTransform(\n            is_training=is_training, size=img_size, interpolation=interpolation)\n    else:\n        if is_training and no_aug:\n            assert not separate, \"Cannot perform split augmentation with no_aug\"\n            transform = transforms_noaug_train(\n                img_size,\n                interpolation=interpolation,\n                use_prefetcher=use_prefetcher,\n   ", "doc_id": "feb00c1d-0c65-42a1-b540-cc261c19c998", "embedding": null, "doc_hash": "3e857cfa976cae97fcd75c66fa3692156772ddbc5b4d2ad2681a95c726cce7bd", "extra_info": {"file_path": "TinyViT/data/augmentation/transforms_factory.py", "file_name": "transforms_factory.py"}, "node_info": {"start": 6228, "end": 7656}, "relationships": {"1": "defea4b7d942fa503736f79e36e2bd9fa09a526a", "2": "74d3b28c-9e66-4700-876e-ad19cd492af8", "3": "5424a7dd-7d6e-4c51-81c7-04023f81b4d9"}}, "__type__": "1"}, "5424a7dd-7d6e-4c51-81c7-04023f81b4d9": {"__data__": {"text": "    img_size,\n                interpolation=interpolation,\n                use_prefetcher=use_prefetcher,\n                mean=mean,\n                std=std)\n        elif is_training:\n            transform = transforms_imagenet_train(\n                img_size,\n                scale=scale,\n                ratio=ratio,\n                hflip=hflip,\n                vflip=vflip,\n                color_jitter=color_jitter,\n                auto_augment=auto_augment,\n                interpolation=interpolation,\n                use_prefetcher=use_prefetcher,\n                mean=mean,\n                std=std,\n                re_prob=re_prob,\n                re_mode=re_mode,\n                re_count=re_count,\n                re_num_splits=re_num_splits,\n                separate=separate)\n        else:\n            assert not separate, \"Separate transforms not supported for validation preprocessing\"\n            transform = transforms_imagenet_eval(\n                img_size,\n                interpolation=interpolation,\n                use_prefetcher=use_prefetcher,\n                mean=mean,\n                std=std,\n                crop_pct=crop_pct)\n\n    return transform\n", "doc_id": "5424a7dd-7d6e-4c51-81c7-04023f81b4d9", "embedding": null, "doc_hash": "b1e735f5ea3e8e6615cd2269a7b6e626adc1f0c0876730196ad460e9fcdfea92", "extra_info": {"file_path": "TinyViT/data/augmentation/transforms_factory.py", "file_name": "transforms_factory.py"}, "node_info": {"start": 7594, "end": 8771}, "relationships": {"1": "defea4b7d942fa503736f79e36e2bd9fa09a526a", "2": "feb00c1d-0c65-42a1-b540-cc261c19c998"}}, "__type__": "1"}, "3e64d32c-6f0e-4e50-ab1d-4c5f0ebae469": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT Data Builder\n# Copyright (c) 2022 Microsoft\n# Based on the code: Swin Transformer\n#   (https://github.com/microsoft/swin-transformer)\n# Adapted for TinyVIT\n# --------------------------------------------------------\n\nimport os\nimport torch\nimport numpy as np\nimport torch.distributed as dist\nfrom torchvision import datasets, transforms\nfrom timm.data.constants import \\\n    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.data import Mixup\nfrom timm.data import create_transform\n\nfrom .augmentation import create_transform as create_transform_record\nfrom .augmentation.mixup import Mixup as Mixup_record\nfrom .augmentation.dataset_wrapper import DatasetWrapper\nfrom .imagenet22k_dataset import IN22KDataset\nfrom .sampler import MyDistributedSampler\n\ntry:\n    from timm.data import TimmDatasetTar\nexcept ImportError:\n    # for higher version of timm\n    from timm.data import ImageDataset as TimmDatasetTar\n\ntry:\n    from torchvision.transforms import InterpolationMode\n\n    def _pil_interp(method):\n        if method == 'bicubic':\n            return InterpolationMode.BICUBIC\n        elif method == 'lanczos':\n            return InterpolationMode.LANCZOS\n        elif method == 'hamming':\n            return InterpolationMode.HAMMING\n        else:\n            # default bilinear, do we want to allow nearest?\n            return InterpolationMode.BILINEAR\nexcept:\n    from timm.data.transforms import _pil_interp\n\n\ndef build_loader(config):\n    config.defrost()\n    dataset_train, config.MODEL.NUM_CLASSES = build_dataset(\n        is_train=True, config=config)\n    config.freeze()\n\n    print(\n        f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()} successfully build train dataset\")\n    dataset_val, _ = build_dataset(is_train=False, config=config)\n    print(\n        f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()} successfully build val dataset\")\n\n    mixup_active =", "doc_id": "3e64d32c-6f0e-4e50-ab1d-4c5f0ebae469", "embedding": null, "doc_hash": "4b6e8dadf0454bd31bc8c973003839a1d061b794dc56c3fd3aba09b66b88f857", "extra_info": {"file_path": "TinyViT/data/build.py", "file_name": "build.py"}, "node_info": {"start": 0, "end": 2026}, "relationships": {"1": "7ccb1a8daeca307f49d81084ee91d87fd57ae80c", "3": "1e3fd23c-efae-4242-bda6-378a889da189"}}, "__type__": "1"}, "1e3fd23c-efae-4242-bda6-378a889da189": {"__data__": {"text": "config=config)\n    print(\n        f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()} successfully build val dataset\")\n\n    mixup_active = config.AUG.MIXUP > 0 or config.AUG.CUTMIX > 0. or config.AUG.CUTMIX_MINMAX is not None\n\n    sampler_train = MyDistributedSampler(\n        dataset_train, shuffle=True,\n        drop_last=False, padding=True, pair=mixup_active and config.DISTILL.ENABLED,\n    )\n\n    sampler_val = MyDistributedSampler(\n        dataset_val, shuffle=False,\n        drop_last=False, padding=False, pair=False,\n    )\n\n    # TinyViT Dataset Wrapper\n    if config.DISTILL.ENABLED:\n        dataset_train = DatasetWrapper(dataset_train,\n                                       logits_path=config.DISTILL.TEACHER_LOGITS_PATH,\n                                       topk=config.DISTILL.LOGITS_TOPK,\n                                       write=config.DISTILL.SAVE_TEACHER_LOGITS,\n                                       )\n\n    data_loader_train = torch.utils.data.DataLoader(\n        dataset_train, sampler=sampler_train,\n        batch_size=config.DATA.BATCH_SIZE,\n        num_workers=config.DATA.NUM_WORKERS,\n        pin_memory=config.DATA.PIN_MEMORY,\n        # modified for TinyViT, we save logits of all samples\n        drop_last=not config.DISTILL.SAVE_TEACHER_LOGITS,\n    )\n\n    data_loader_val = torch.utils.data.DataLoader(\n        dataset_val, sampler=sampler_val,\n        batch_size=config.DATA.BATCH_SIZE,\n        shuffle=False,\n       ", "doc_id": "1e3fd23c-efae-4242-bda6-378a889da189", "embedding": null, "doc_hash": "0db155cbda708d00d6255d51df7446d01a9a9c3a1308943b50381f59a904269a", "extra_info": {"file_path": "TinyViT/data/build.py", "file_name": "build.py"}, "node_info": {"start": 1903, "end": 3369}, "relationships": {"1": "7ccb1a8daeca307f49d81084ee91d87fd57ae80c", "2": "3e64d32c-6f0e-4e50-ab1d-4c5f0ebae469", "3": "ba9c0a37-dcfb-4973-961c-293b3d8c4169"}}, "__type__": "1"}, "ba9c0a37-dcfb-4973-961c-293b3d8c4169": {"__data__": {"text": "       dataset_val, sampler=sampler_val,\n        batch_size=config.DATA.BATCH_SIZE,\n        shuffle=False,\n        num_workers=config.DATA.NUM_WORKERS,\n        pin_memory=config.DATA.PIN_MEMORY,\n        drop_last=False\n    )\n\n    # setup mixup / cutmix\n    mixup_fn = None\n    if mixup_active:\n        mixup_t = Mixup if not config.DISTILL.ENABLED else Mixup_record\n        if config.DISTILL.ENABLED and config.AUG.MIXUP_MODE != \"pair2\":\n            # change to pair2 mode for saving logits\n            config.defrost()\n            config.AUG.MIXUP_MODE = 'pair2'\n            config.freeze()\n        mixup_fn = mixup_t(\n            mixup_alpha=config.AUG.MIXUP, cutmix_alpha=config.AUG.CUTMIX, cutmix_minmax=config.AUG.CUTMIX_MINMAX,\n            prob=config.AUG.MIXUP_PROB, switch_prob=config.AUG.MIXUP_SWITCH_PROB, mode=config.AUG.MIXUP_MODE,\n            label_smoothing=config.MODEL.LABEL_SMOOTHING, num_classes=config.MODEL.NUM_CLASSES)\n\n    return dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn\n\n\ndef build_dataset(is_train, config):\n\n    transform = build_transform(is_train, config)\n    dataset_tar_t = TimmDatasetTar\n\n    if config.DATA.DATASET == 'imagenet':\n        prefix = 'train' if is_train else 'val'\n        # load tar dataset\n        data_dir = os.path.join(config.DATA.DATA_PATH, f'{prefix}.tar')\n        if os.path.exists(data_dir):\n            dataset = dataset_tar_t(data_dir, transform=transform)\n        else:\n            root = os.path.join(config.DATA.DATA_PATH, prefix)\n    ", "doc_id": "ba9c0a37-dcfb-4973-961c-293b3d8c4169", "embedding": null, "doc_hash": "94c170dbf9d5c45ea1d71a2605cf65044a509d49ed2cb5862303c393a61efe89", "extra_info": {"file_path": "TinyViT/data/build.py", "file_name": "build.py"}, "node_info": {"start": 3411, "end": 4936}, "relationships": {"1": "7ccb1a8daeca307f49d81084ee91d87fd57ae80c", "2": "1e3fd23c-efae-4242-bda6-378a889da189", "3": "ea8f4396-2ac7-429b-8e3c-6c8eaed77413"}}, "__type__": "1"}, "ea8f4396-2ac7-429b-8e3c-6c8eaed77413": {"__data__": {"text": "= dataset_tar_t(data_dir, transform=transform)\n        else:\n            root = os.path.join(config.DATA.DATA_PATH, prefix)\n            dataset = datasets.ImageFolder(root, transform=transform)\n        nb_classes = 1000\n    elif config.DATA.DATASET == 'imagenet22k':\n        if is_train:\n            dataset = IN22KDataset(data_root=config.DATA.DATA_PATH, transform=transform,\n                                   fname_format=config.DATA.FNAME_FORMAT, debug=config.DATA.DEBUG)\n            nb_classes = 21841\n        else:\n            # load ImageNet-1k validation set\n            '''\n            datasets/\n            \u251c\u2500\u2500 ImageNet-22k/  # the folder of IN-22k\n            \u2514\u2500\u2500 ImageNet/  # the folder of IN-1k\n            '''\n            old_data_path = config.DATA.DATA_PATH\n            config.defrost()\n            config.DATA.DATA_PATH = os.path.normpath(\n                os.path.join(old_data_path, '../ImageNet'))\n            config.DATA.DATASET = 'imagenet'\n            dataset, nb_classes = build_dataset(is_train=False, config=config)\n            config.DATA.DATA_PATH = old_data_path\n            config.DATA.DATASET = 'imagenet22k'\n            config.freeze()\n    else:\n        raise NotImplementedError(\"We only support ImageNet Now.\")\n\n    return dataset, nb_classes\n\n\ndef build_transform(is_train, config):\n    resize_im = config.DATA.IMG_SIZE > 32\n\n    # RGB: mean, std\n    rgbs = dict(\n        default=(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n     ", "doc_id": "ea8f4396-2ac7-429b-8e3c-6c8eaed77413", "embedding": null, "doc_hash": "700f4835f7c6d96e22bf05a8ad0856b55ef39b00cb422f12127b786d590dc50a", "extra_info": {"file_path": "TinyViT/data/build.py", "file_name": "build.py"}, "node_info": {"start": 4921, "end": 6387}, "relationships": {"1": "7ccb1a8daeca307f49d81084ee91d87fd57ae80c", "2": "ba9c0a37-dcfb-4973-961c-293b3d8c4169", "3": "0019eab9-59b1-443c-9108-21604c4e9767"}}, "__type__": "1"}, "0019eab9-59b1-443c-9108-21604c4e9767": {"__data__": {"text": "> 32\n\n    # RGB: mean, std\n    rgbs = dict(\n        default=(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n        inception=(IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD),\n        clip=((0.48145466, 0.4578275, 0.40821073),\n              (0.26862954, 0.26130258, 0.27577711)),\n    )\n    mean, std = rgbs[config.DATA.MEAN_AND_STD_TYPE]\n\n    if is_train:\n        # this should always dispatch to transforms_imagenet_train\n        create_transform_t = create_transform if not config.DISTILL.ENABLED else create_transform_record\n        transform = create_transform_t(\n            input_size=config.DATA.IMG_SIZE,\n            is_training=True,\n            color_jitter=config.AUG.COLOR_JITTER if config.AUG.COLOR_JITTER > 0 else None,\n            auto_augment=config.AUG.AUTO_AUGMENT if config.AUG.AUTO_AUGMENT != 'none' else None,\n            re_prob=config.AUG.REPROB,\n            re_mode=config.AUG.REMODE,\n            re_count=config.AUG.RECOUNT,\n            interpolation=config.DATA.INTERPOLATION,\n            mean=mean,\n            std=std,\n        )\n        if not resize_im:\n            # replace RandomResizedCropAndInterpolation with\n            # RandomCrop\n            transform.transforms[0] = transforms.RandomCrop(\n                config.DATA.IMG_SIZE, padding=4)\n\n        return transform\n\n    t = []\n    if resize_im:\n        if config.TEST.CROP:\n            size = int((256 / 224) * config.DATA.IMG_SIZE)\n            t.append(\n        ", "doc_id": "0019eab9-59b1-443c-9108-21604c4e9767", "embedding": null, "doc_hash": "23fd011bd449a9cd45fdd402340ae182603ee8b434c65bd02d2559b2a80fdd33", "extra_info": {"file_path": "TinyViT/data/build.py", "file_name": "build.py"}, "node_info": {"start": 6403, "end": 7856}, "relationships": {"1": "7ccb1a8daeca307f49d81084ee91d87fd57ae80c", "2": "ea8f4396-2ac7-429b-8e3c-6c8eaed77413", "3": "62ea21be-6db0-418b-9b1d-bba9531e982c"}}, "__type__": "1"}, "62ea21be-6db0-418b-9b1d-bba9531e982c": {"__data__": {"text": "if config.TEST.CROP:\n            size = int((256 / 224) * config.DATA.IMG_SIZE)\n            t.append(\n                transforms.Resize(size, interpolation=_pil_interp(\n                    config.DATA.INTERPOLATION)),\n                # to maintain same ratio w.r.t. 224 images\n            )\n            t.append(transforms.CenterCrop(config.DATA.IMG_SIZE))\n        else:\n            t.append(\n                transforms.Resize((config.DATA.IMG_SIZE, config.DATA.IMG_SIZE),\n                                  interpolation=_pil_interp(config.DATA.INTERPOLATION))\n            )\n\n    t.append(transforms.ToTensor())\n    t.append(transforms.Normalize(mean, std))\n    transform = transforms.Compose(t)\n    return transform\n", "doc_id": "62ea21be-6db0-418b-9b1d-bba9531e982c", "embedding": null, "doc_hash": "a3f95c6fa3d2f17d5c6bc32206f7746dfd053f0a110ac89a3d8994b801536414", "extra_info": {"file_path": "TinyViT/data/build.py", "file_name": "build.py"}, "node_info": {"start": 7830, "end": 8547}, "relationships": {"1": "7ccb1a8daeca307f49d81084ee91d87fd57ae80c", "2": "0019eab9-59b1-443c-9108-21604c4e9767"}}, "__type__": "1"}, "4b18f380-abff-4db2-b9ad-fa22dedd76a1": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT ImageNet 22k Dataset\n# Copyright (c) 2022 Microsoft\n# --------------------------------------------------------\n\nimport io\nimport os\nimport torch\nfrom collections import defaultdict\nfrom PIL import Image\nimport zipfile\n\n\nclass IN22KDataset(torch.utils.data.Dataset):\n    def __init__(self, data_root, transform, fname_format='{}.jpeg', debug=False):\n        super().__init__()\n        self.data_root = data_root\n        self.transform = transform\n        self.debug = debug\n        self.fname_format = fname_format\n\n        info_fname = os.path.join(data_root, 'in22k_image_names.txt')\n        assert os.path.isfile(\n            info_fname), f'IN22k-List filelist: {info_fname} does not exist'\n\n        folders = defaultdict(list)\n        with open(info_fname, 'r') as f:\n            for iname in f:\n                iname = iname.strip()\n                class_name = iname[:iname.index('_')]\n                folders[class_name].append(iname)\n        class_names = sorted(folders.keys())\n        self.nb_classes = len(class_names)\n\n        if debug:\n            for name in class_names:\n                if not name.startswith('n00007846'):\n                    folders[name] = []\n\n        self.data = []\n        for cls_id, cls_name in enumerate(class_names):\n            self.data.extend([(iname, cls_id) for iname in folders[cls_name]])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        iname, target = self.data[idx]\n        iob = self._read_file(iname)\n        img = Image.open(iob).convert('RGB')\n\n       ", "doc_id": "4b18f380-abff-4db2-b9ad-fa22dedd76a1", "embedding": null, "doc_hash": "92d1e468635a7ac256815dbdca175421cc1d09063277757233baedf31efe5748", "extra_info": {"file_path": "TinyViT/data/imagenet22k_dataset.py", "file_name": "imagenet22k_dataset.py"}, "node_info": {"start": 0, "end": 1621}, "relationships": {"1": "1d864c9d7b306b8e4d9c114de5b2005f8de5545b", "3": "353c4864-57af-4648-9132-3f924225f507"}}, "__type__": "1"}, "353c4864-57af-4648-9132-3f924225f507": {"__data__": {"text": "  iname, target = self.data[idx]\n        iob = self._read_file(iname)\n        img = Image.open(iob).convert('RGB')\n\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, target\n\n    def _read_file(self, iname):\n        # Example:\n        # iname: 'n00007846_10001'\n        # fname: 'n00007846_10001.jpeg'\n        cls_name = iname[:iname.index('_')]\n        fname = self.fname_format.format(iname)\n        zip_fname = os.path.join(self.data_root, cls_name + '.zip')\n        handle = zipfile.ZipFile(zip_fname, 'r')\n        bstr = handle.read(fname)\n        iob = io.BytesIO(bstr)\n        return iob\n\n    def get_keys(self):\n        return [e[0] for e in self.data]\n\n\nif __name__ == '__main__':\n    data_root = './ImageNet-22k'\n    def transform(x): return x\n    fname_format = 'imagenet22k/{}.JPEG'\n    dataset = IN22KDataset(data_root, transform, fname_format, debug=True)\n    for img, target in dataset:\n        print(type(img), target)\n        break\n", "doc_id": "353c4864-57af-4648-9132-3f924225f507", "embedding": null, "doc_hash": "cc003e0e28e559ccf2e024a6b14e84102a5dcd9a4d0dcb27d0425d3fb015a671", "extra_info": {"file_path": "TinyViT/data/imagenet22k_dataset.py", "file_name": "imagenet22k_dataset.py"}, "node_info": {"start": 1498, "end": 2493}, "relationships": {"1": "1d864c9d7b306b8e4d9c114de5b2005f8de5545b", "2": "4b18f380-abff-4db2-b9ad-fa22dedd76a1"}}, "__type__": "1"}, "4dfda049-5073-4029-a0e8-4da9966e3985": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT Data Sampler\n# Copyright (c) 2022 Microsoft\n# Refer to https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py\n# --------------------------------------------------------\nimport torch\nfrom typing import TypeVar, Optional, Iterator\n\nimport torch\nfrom torch.utils.data import Sampler, Dataset\nimport torch.distributed as dist\n\n\nT_co = TypeVar('T_co', covariant=True)\n\n\nclass MyDistributedSampler(Sampler[T_co]):\n    r\"\"\"Sampler that restricts data loading to a subset of the dataset.\n\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such a case, each\n    process can pass a :class:`~torch.utils.data.DistributedSampler` instance as a\n    :class:`~torch.utils.data.DataLoader` sampler, and load a subset of the\n    original dataset that is exclusive to it.\n\n    .. note::\n        Dataset is assumed to be of constant size and that any instance of it always\n        returns the same elements in the same order.\n\n    Args:\n        dataset: Dataset used for sampling.\n        num_replicas (int, optional): Number of processes participating in\n            distributed training. By default, :attr:`world_size` is retrieved from the\n            current distributed group.\n        rank (int, optional): Rank of the current process within :attr:`num_replicas`.\n            By default, :attr:`rank` is retrieved from the current distributed\n            group.\n        shuffle (bool, optional): If ``True`` (default), sampler will shuffle the\n            indices.\n        seed (int, optional): random seed used to shuffle the sampler if\n            :attr:`shuffle=True`. This number should be identical across all\n            processes in the distributed group. Default: ``0``.\n        drop_last (bool, optional): if ``True``, then the sampler will drop the\n            tail of the data to make it evenly divisible across the number of\n            replicas. If ``False``, the sampler will add extra indices to make\n            the data evenly divisible across the replicas. Default: ``False``.\n    ", "doc_id": "4dfda049-5073-4029-a0e8-4da9966e3985", "embedding": null, "doc_hash": "c973a1e545ba7e163cf320c5b12ef672a875f5f46f091d6c62008db5d0031373", "extra_info": {"file_path": "TinyViT/data/sampler.py", "file_name": "sampler.py"}, "node_info": {"start": 0, "end": 2135}, "relationships": {"1": "46e3d7596893011787f432b40d6a1df8350a2a5d", "3": "59f45a9f-5cd7-4986-89b8-a7838c3fba9a"}}, "__type__": "1"}, "59f45a9f-5cd7-4986-89b8-a7838c3fba9a": {"__data__": {"text": "           replicas. If ``False``, the sampler will add extra indices to make\n            the data evenly divisible across the replicas. Default: ``False``.\n        padding: (bool, optional): Whether to pad the dataset. Default: ``True``.\n        pair: (bool, optional): Pair output for Mixup. Default: ``False``.\n\n    .. warning::\n        In distributed mode, calling the :meth:`set_epoch` method at\n        the beginning of each epoch **before** creating the :class:`DataLoader` iterator\n        is necessary to make shuffling work properly across multiple epochs. Otherwise,\n        the same ordering will be always used.\n\n    Example::\n\n        >>> sampler = DistributedSampler(dataset) if is_distributed else None\n        >>> loader = DataLoader(dataset, shuffle=(sampler is None),\n        ...                     sampler=sampler)\n        >>> for epoch in range(start_epoch, n_epochs):\n        ...     if is_distributed:\n        ...         sampler.set_epoch(epoch)\n        ...     train(loader)\n    \"\"\"\n\n    def __init__(self, dataset: Dataset, num_replicas: Optional[int] = None,\n                 rank: Optional[int] = None, shuffle: bool = True,\n                 seed: int = 0, drop_last: bool = False,\n                 padding: bool = True,\n                 pair: bool = False) -> None:\n        if num_replicas is None:\n            if not dist.is_available():\n                num_replicas = 1\n            else:\n                num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                rank = 0\n            else:\n                rank =", "doc_id": "59f45a9f-5cd7-4986-89b8-a7838c3fba9a", "embedding": null, "doc_hash": "d9501b67728ffd303933cec0da08c3c159e4814776de59ac3594181b0cb50f23", "extra_info": {"file_path": "TinyViT/data/sampler.py", "file_name": "sampler.py"}, "node_info": {"start": 2020, "end": 3623}, "relationships": {"1": "46e3d7596893011787f432b40d6a1df8350a2a5d", "2": "4dfda049-5073-4029-a0e8-4da9966e3985", "3": "83c854b5-359d-40db-87e1-bcb5731b662b"}}, "__type__": "1"}, "83c854b5-359d-40db-87e1-bcb5731b662b": {"__data__": {"text": "     if not dist.is_available():\n                rank = 0\n            else:\n                rank = dist.get_rank()\n        if rank >= num_replicas or rank < 0:\n            raise ValueError(\n                \"Invalid rank {}, rank should be in the interval\"\n                \" [0, {}]\".format(rank, num_replicas - 1))\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.drop_last = drop_last\n        self.pair = pair\n        self.padding = padding\n        # If the dataset length is evenly divisible by # of replicas, then there\n        # is no need to drop any data, since the dataset will be split equally.\n        T = self.num_replicas if not self.pair else self.num_replicas * 2\n        self.total_size = len(self.dataset)\n        if self.padding:\n            num_parts = self.total_size // T\n            has_rest = bool(self.total_size % T)\n            if self.drop_last:\n                self.total_size = num_parts * T\n            else:\n                self.total_size = (num_parts + has_rest) * T\n        self.num_samples = (\n            self.total_size + self.num_replicas - 1) // self.num_replicas\n        self.shuffle = shuffle\n        self.seed = seed\n\n    def __iter__(self) -> Iterator[T_co]:\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            indices =", "doc_id": "83c854b5-359d-40db-87e1-bcb5731b662b", "embedding": null, "doc_hash": "ed0520074cf155436014fda6529d3a583607af48d0734fd491e1d7ab6d6dab50", "extra_info": {"file_path": "TinyViT/data/sampler.py", "file_name": "sampler.py"}, "node_info": {"start": 3696, "end": 5174}, "relationships": {"1": "46e3d7596893011787f432b40d6a1df8350a2a5d", "2": "59f45a9f-5cd7-4986-89b8-a7838c3fba9a", "3": "b409c2ab-bbc1-4596-a594-1b53621372bd"}}, "__type__": "1"}, "b409c2ab-bbc1-4596-a594-1b53621372bd": {"__data__": {"text": "seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g)\n        else:\n            indices = torch.arange(len(self.dataset))\n\n        if not self.drop_last:\n            # add extra samples to make it evenly divisible\n            if self.padding:\n                padding_size = self.total_size - len(indices)\n                # pad to total_size\n                if padding_size <= len(indices):\n                    indices = torch.cat(\n                        [indices, indices[:padding_size]], dim=0)\n                else:\n                    repeat_times = (self.total_size +\n                                    len(indices) - 1) // len(indices)\n                    indices = indices.repeat(repeat_times)[:self.total_size]\n        else:\n            # remove tail of data to make it evenly divisible.\n            indices = indices[:self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        if self.pair:\n            indices = indices.view(-1, 2)\n        indices = indices[self.rank:self.total_size:self.num_replicas].flatten(\n        ).tolist()\n        assert len(indices) == self.num_samples or (\n            not self.padding and len(indices) == self.num_samples - 1)\n\n        return iter(indices)\n\n    def __len__(self) -> int:\n        return self.num_samples\n\n   ", "doc_id": "b409c2ab-bbc1-4596-a594-1b53621372bd", "embedding": null, "doc_hash": "0b5ffcd026c4a81e48bda688925c4110aeaf16932fbf0e8ec126aff5835c35e2", "extra_info": {"file_path": "TinyViT/data/sampler.py", "file_name": "sampler.py"}, "node_info": {"start": 5150, "end": 6551}, "relationships": {"1": "46e3d7596893011787f432b40d6a1df8350a2a5d", "2": "83c854b5-359d-40db-87e1-bcb5731b662b", "3": "b2c6f2d2-f75c-4c9a-addc-ea5f3189d5e8"}}, "__type__": "1"}, "b2c6f2d2-f75c-4c9a-addc-ea5f3189d5e8": {"__data__": {"text": "and len(indices) == self.num_samples - 1)\n\n        return iter(indices)\n\n    def __len__(self) -> int:\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -> None:\n        r\"\"\"\n        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas\n        use a different random ordering for each epoch. Otherwise, the next iteration of this\n        sampler will yield the same ordering.\n\n        Args:\n            epoch (int): Epoch number.\n        \"\"\"\n        self.epoch = epoch\n", "doc_id": "b2c6f2d2-f75c-4c9a-addc-ea5f3189d5e8", "embedding": null, "doc_hash": "8fbe693004ed35b2222259c9197a0556511b347cf7dc5c420710dbe09936eccc", "extra_info": {"file_path": "TinyViT/data/sampler.py", "file_name": "sampler.py"}, "node_info": {"start": 6481, "end": 7002}, "relationships": {"1": "46e3d7596893011787f432b40d6a1df8350a2a5d", "2": "b409c2ab-bbc1-4596-a594-1b53621372bd"}}, "__type__": "1"}, "8700e7eb-76f8-4ca1-a65f-b0e516f5e079": {"__data__": {"text": "# Evaluation\n\nBefore evaluation, we need to prepare [the ImageNet-1k dataset](./PREPARATION.md) and [the checkpoints in model zoo](../README.md).\n\nRun the following command for evaluation:\n\n**Evaluate TinyViT with pretraining distillation**\n\n<details>\n<summary>Evaluate TinyViT-5M <img src=\"../.figure/distill.png\"></summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/22kto1k/tiny_vit_5m_22kto1k.yaml --data-path ./ImageNet --batch-size 128 --eval --resume ./checkpoints/tiny_vit_5m_22kto1k_distill.pth\n</code></pre>\n</details>\n\n<details>\n<summary>Evaluate TinyViT-11M <img src=\"../.figure/distill.png\"></summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/22kto1k/tiny_vit_11m_22kto1k.yaml --data-path ./ImageNet --batch-size 128 --eval --resume ./checkpoints/tiny_vit_11m_22kto1k_distill.pth\n</code></pre>\n</details>\n\n<details>\n<summary>Evaluate TinyViT-21M <img src=\"../.figure/distill.png\"></summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/22kto1k/tiny_vit_21m_22kto1k.yaml --data-path ./ImageNet --batch-size 128 --eval --resume ./checkpoints/tiny_vit_21m_22kto1k_distill.pth\n</code></pre>\n</details>\n\n<details>\n<summary>Evaluate TinyViT-21M-384 <img src=\"../.figure/distill.png\"></summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/higher_resolution/tiny_vit_21m_224to384.yaml --data-path ./ImageNet --batch-size 64 --eval --resume ./checkpoints/tiny_vit_21m_22kto1k_384_distill.pth\n</code></pre>\n</details>\n\n<details>\n<summary>Evaluate TinyViT-21M-512 <img src=\"../.figure/distill.png\"></summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/higher_resolution/tiny_vit_21m_384to512.yaml --data-path ./ImageNet --batch-size 32 --eval --resume", "doc_id": "8700e7eb-76f8-4ca1-a65f-b0e516f5e079", "embedding": null, "doc_hash": "7857b0edda7604820fdd40960f8c3c570b6da01a42917c9b1ee59584c9939f18", "extra_info": {"file_path": "TinyViT/docs/EVALUATION.md", "file_name": "EVALUATION.md"}, "node_info": {"start": 0, "end": 1872}, "relationships": {"1": "81184ca5ff0b547ad19b0d8cd380b448d2ea8d45", "3": "b8702bf6-4034-433c-b27b-12fc12099b6e"}}, "__type__": "1"}, "b8702bf6-4034-433c-b27b-12fc12099b6e": {"__data__": {"text": "-m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/higher_resolution/tiny_vit_21m_384to512.yaml --data-path ./ImageNet --batch-size 32 --eval --resume ./checkpoints/tiny_vit_21m_22kto1k_512_distill.pth\n</code></pre>\n</details>\n\n**Evaluate TinyViT trained from scratch in IN-1k**\n\n<details>\n<summary>Evaluate TinyViT-5M</summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/1k/tiny_vit_5m.yaml --data-path ./ImageNet --batch-size 128 --eval --resume ./checkpoints/tiny_vit_5m_1k.pth\n</code></pre>\n</details>\n\n<details>\n<summary>Evaluate TinyViT-11M</summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/1k/tiny_vit_11m.yaml --data-path ./ImageNet --batch-size 128 --eval --resume ./checkpoints/tiny_vit_11m_1k.pth\n</code></pre>\n</details>\n\n<details>\n<summary>Evaluate TinyViT-21M</summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/1k/tiny_vit_21m.yaml --data-path ./ImageNet --batch-size 128 --eval --resume ./checkpoints/tiny_vit_21m_1k.pth\n</code></pre>\n</details>\n\n**The model pretrained on IN-22k can be evaluated directly on IN-1k**\n\nSince the model pretrained on IN-22k is not finetuned on IN-1k, the accuracy is lower than the model finetuned 22kto1k.\n\n<details>\n<summary>Evaluate TinyViT-5M-22k <img src=\"../.figure/distill.png\"></summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/22k_distill/tiny_vit_5m_22k_distill.yaml --data-path ./ImageNet --batch-size 128 --eval --resume ./checkpoints/tiny_vit_5m_22k_distill.pth --opts DATA.DATASET imagenet\n</code></pre>\n</details>\n\n<details>\n<summary>Evaluate TinyViT-11M-22k <img src=\"../.figure/distill.png\"></summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg", "doc_id": "b8702bf6-4034-433c-b27b-12fc12099b6e", "embedding": null, "doc_hash": "f8f9c341df410bc8440b30ed821b8ee5c9cfbd148233c28ea3d51f9853755d2a", "extra_info": {"file_path": "TinyViT/docs/EVALUATION.md", "file_name": "EVALUATION.md"}, "node_info": {"start": 1717, "end": 3562}, "relationships": {"1": "81184ca5ff0b547ad19b0d8cd380b448d2ea8d45", "2": "8700e7eb-76f8-4ca1-a65f-b0e516f5e079", "3": "50237b84-7b73-43a9-8ba1-670a752adaeb"}}, "__type__": "1"}, "50237b84-7b73-43a9-8ba1-670a752adaeb": {"__data__": {"text": "TinyViT-11M-22k <img src=\"../.figure/distill.png\"></summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/22k_distill/tiny_vit_11m_22k_distill.yaml --data-path ./ImageNet --batch-size 128 --eval --resume ./checkpoints/tiny_vit_11m_22k_distill.pth --opts DATA.DATASET imagenet\n</code></pre>\n</details>\n\n<details>\n<summary>Evaluate TinyViT-21M-22k <img src=\"../.figure/distill.png\"></summary>\n<pre><code>python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/22k_distill/tiny_vit_21m_22k_distill.yaml --data-path ./ImageNet --batch-size 128 --eval --resume ./checkpoints/tiny_vit_21m_22k_distill.pth --opts DATA.DATASET imagenet\n</code></pre>\n</details>\n", "doc_id": "50237b84-7b73-43a9-8ba1-670a752adaeb", "embedding": null, "doc_hash": "d35979f8f52181a2fca0752346d1d8c6fcc167857ada1764e0cefc18b5f2fffc", "extra_info": {"file_path": "TinyViT/docs/EVALUATION.md", "file_name": "EVALUATION.md"}, "node_info": {"start": 3579, "end": 4294}, "relationships": {"1": "81184ca5ff0b547ad19b0d8cd380b448d2ea8d45", "2": "b8702bf6-4034-433c-b27b-12fc12099b6e"}}, "__type__": "1"}, "3a640ebb-98d9-4b35-8262-88d2cd2cad41": {"__data__": {"text": "# Preparation\n\n### Install the requirements\n\nRun the following command to install the dependences:\n\n```bash\npip install -r requirements.txt\n```\n\n### Data Preparation\n\nWe need to prepare ImageNet-1k and ImageNet-22k datasets from [`http://www.image-net.org/`](http://www.image-net.org/).\n\n- ImageNet-1k\n\nImageNet-1k contains 1.28 M images for training and 50 K images for validation.\nThe train set and validation set should be saved as the `*.tar` archives:\n\n```\nImageNet/\n\u251c\u2500\u2500 train.tar\n\u2514\u2500\u2500 val.tar\n```\n\nOur code also supports storing images as individual files as follow:\n```\nImageNet/\n\u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 n01440764\n\u2502   \u2502   \u251c\u2500\u2500 n01440764_10026.JPEG\n\u2502   \u2502   \u251c\u2500\u2500 n01440764_10027.JPEG\n...\n\u251c\u2500\u2500 val\n\u2502   \u251c\u2500\u2500 n01440764\n\u2502   \u2502   \u251c\u2500\u2500 ILSVRC2012_val_00000293.JPEG\n```\n\n- ImageNet-22k\n\nImageNet-22k contains 14 M images with 21,841 classes, without overlapping part with ImageNet-1k validation set.\n\nThe filelist (`in22k_image_names.txt`) can be download at [here](https://github.com/wkcn/TinyViT-model-zoo/releases/download/datasets/in22k_image_names.txt).\n\nEach class is stored as an archive file.\n```\nImageNet-22k/\n\u251c\u2500\u2500 in22k_image_names.txt\n\u251c\u2500\u2500 n00004475.zip\n\u251c\u2500\u2500 n00005787.zip\n\u251c\u2500\u2500 n00006024.zip\n...\n\u251c\u2500\u2500 n15102455.zip\n\u2514\u2500\u2500 n15102894.zip\n```\n\nThe config `DATA.FNAME_FORMAT` defines the image filename format in the archive file, default: `{}.jpeg`.\n\nWe need IN-1k to evaluate the model, so the folders should be placed like the following (`a soft link is available`):\n```\ndatasets/\n\u251c\u2500\u2500 ImageNet-22k/  # the folder of IN-22k\n\u2514\u2500\u2500 ImageNet/  # the folder of IN-1k\n```\n", "doc_id": "3a640ebb-98d9-4b35-8262-88d2cd2cad41", "embedding": null, "doc_hash": "186327e396279ff89ab170aed0d86731c4b6b4d0fe4d1daa71dc93f0451265b6", "extra_info": {"file_path": "TinyViT/docs/PREPARATION.md", "file_name": "PREPARATION.md"}, "node_info": {"start": 0, "end": 1551}, "relationships": {"1": "d16cc4fd2d0959e625fff46701a9a26e2631b212"}}, "__type__": "1"}, "efd68da9-04f6-40f3-82c9-609631199b25": {"__data__": {"text": "# The tutorial of saving teacher sparse logits\n\nThis document shows how to save and check teacher sparse soft labels.\n\nWe provide an example to store the sparse soft labels of **CLIP-ViT-Large/14-22k** on ImageNet-22k. With the pretrained teacher, **TinyViT-5/11/21M** will achieve the Top-1 accuracy of **80.7/83.2/84.8 %** on ImageNet-1k valiadation set.\n\n## Save teacher sparse logits\nFirstly, we prepare the IN-22k dataset ([Data Preparation](./PREPARATION.md)), then download the checkpoint of CLIP-ViT-Large/14-22k in [the link](https://github.com/wkcn/TinyViT-model-zoo/releases/download/pretrained_teacher/clip_vit_large_patch14_22k.pth).\n\nThe following command will store the teacher sparse logits.\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 save_logits.py --cfg configs/teacher/clip_vit_large_patch14_22k.yaml --data-path ./ImageNet-22k --batch-size 128 --eval --resume checkpoints/clip_vit_large_patch14_22k.pth --opts DISTILL.TEACHER_LOGITS_PATH ./teacher_logits/\n```\n\n**The accuracy of CLIP-ViT-Large/14-22k (w/o finetune on IN-1k) on IN-1k is Acc@1 85.894 Acc@5 97.566.**\n\nSince IN-22k is too large, we recommend to use few data to debug by adding the argument `DATA.DEBUG True`.\n\n- How to save sparse logits **in parallel** ?\n\nSince the teacher logits per epoch is independent, they can be saved in parallel. Specifically, each machine saves a segment of the whole epochs individually.\nWe can add the epoch interval into the command, e.g.\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 save_logits.py --cfg configs/teacher/clip_vit_large_patch14_22k.yaml --data-path ./ImageNet-22k --batch-size 128 --eval --resume checkpoints/clip_vit_large_patch14_22k.pth --opts DISTILL.TEACHER_LOGITS_PATH ./teacher_logits/ TRAIN.START_EPOCH 30 TRAIN.EPOCHS 40\n```\nThe sparse logits between 30 to 40 will be saved.\n\n## Check teacher sparse logits\nAfter saving the logits, we can check them by adding the extra argument `--check-saved-logits`.\n```bash\npython -m torch.distributed.launch --nproc_per_node 8", "doc_id": "efd68da9-04f6-40f3-82c9-609631199b25", "embedding": null, "doc_hash": "c19ade17e90dc6d264c10bbab92e3a756cde9928acf2d62b94c474d7d8a8703c", "extra_info": {"file_path": "TinyViT/docs/SAVE_TEACHER_LOGITS.md", "file_name": "SAVE_TEACHER_LOGITS.md"}, "node_info": {"start": 0, "end": 2041}, "relationships": {"1": "8db5d1e6d439df72d2e2288b334620e1c466d4b2", "3": "4f5db08a-ecd2-40f9-b3b5-96d55954b7a3"}}, "__type__": "1"}, "4f5db08a-ecd2-40f9-b3b5-96d55954b7a3": {"__data__": {"text": "will be saved.\n\n## Check teacher sparse logits\nAfter saving the logits, we can check them by adding the extra argument `--check-saved-logits`.\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 save_logits.py --cfg configs/teacher/clip_vit_large_patch14_22k.yaml --data-path ./ImageNet-22k --batch-size 128 --eval --resume checkpoints/clip_vit_large_patch14_22k.pth --check-saved-logits --opts DISTILL.TEACHER_LOGITS_PATH ./teacher_logits\n```\n", "doc_id": "4f5db08a-ecd2-40f9-b3b5-96d55954b7a3", "embedding": null, "doc_hash": "82f72b889bd34edd2e07b223075defecaced495e5ab379e2669e265df18cbfe3", "extra_info": {"file_path": "TinyViT/docs/SAVE_TEACHER_LOGITS.md", "file_name": "SAVE_TEACHER_LOGITS.md"}, "node_info": {"start": 1837, "end": 2291}, "relationships": {"1": "8db5d1e6d439df72d2e2288b334620e1c466d4b2", "2": "efd68da9-04f6-40f3-82c9-609631199b25"}}, "__type__": "1"}, "99c27247-dafc-48de-832b-e9ec1dd087bc": {"__data__": {"text": "# Training TinyViT\n\nIn this document, we introduce how to pretrain TinyViT with the proposed fast pretraining distillation.\n\nNote: If the GPU memory is not enough to fit the batch size, you can use `Gradient accumulation steps` by adding the argument `--accumulation-steps <acc_steps>`. For example, the accumulated batch size per GPU is 128 (= 32 x 4) when passing the arguments `--batch-size 32 --accumulation-steps 4`.\n\n## Pretrain the model on ImageNet-22k with the proposed fast pretraining distillation.\n\nBefore training with the proposed fast pretraining distillation, we need to store the teacher sparse soft labels by [the tutorial](./SAVE_TEACHER_LOGITS.md).\n\nAssume that the teacher sparse soft labels are stored in the path `./teacher_logits/`, and the IN-22k dataset is stored in the folder `./ImageNet-22k`.\n\nWe use 4 nodes (8 GPUs per node) to pretrain the model on IN-22k with the distillation of stored soft labels.\n\n```bash\npython -m torch.distributed.launch --master_addr=$MASTER_ADDR --nproc_per_node 8 --nnodes=4 --node_rank=$NODE_RANK main.py --cfg configs/22k_distill/tiny_vit_21m_22k_distill.yaml --data-path ./ImageNet-22k --batch-size 128 --output ./output --opts DISTILL.TEACHER_LOGITS_PATH ./teacher_logits/\n```\n\nwhere `$NODE_RANK` and `$MASTER_ADDR` are the rank of a node and the IP address of the master node.\n\n## Finetune on ImageNet-1k\n\n- Finetune the pretrained model from IN-22k to IN-1k\n\nAfter pretrained on IN-22k, the model can be finetuned on IN-1k by the following command.\n\n```\npython -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/22kto1k/tiny_vit_21m_22kto1k.yaml --data-path ./ImageNet --batch-size 128 --pretrained ./checkpoints/tiny_vit_21m_22k_distill.pth --output ./output\n```\n\nwhere `tiny_vit_21m_22k.pth` is the checkpoint of pretrained TinyViT-21M on IN-22k dataset.\n\n- Finetune with higher resolution\n\nTo obtain better accuracy, we finetune the model to higher resolution progressively (224 -> 384 -> 512).\n\n<details>\n<summary>Finetune with higher resolution from 224 to 384</summary>\n<pre><code> python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/higher_resolution/tiny_vit_21m_224to384.yaml --data-path", "doc_id": "99c27247-dafc-48de-832b-e9ec1dd087bc", "embedding": null, "doc_hash": "370c42cd9b0ad0994161c57b66aa4ff2e6aca97de9cab28403417de44516d8d3", "extra_info": {"file_path": "TinyViT/docs/TRAINING.md", "file_name": "TRAINING.md"}, "node_info": {"start": 0, "end": 2203}, "relationships": {"1": "3997001189ab390bb5556fbaa4f192d21a5ca97e", "3": "5e6655a5-0b0e-463b-b383-1e5b6c51bf7f"}}, "__type__": "1"}, "5e6655a5-0b0e-463b-b383-1e5b6c51bf7f": {"__data__": {"text": "with higher resolution from 224 to 384</summary>\n<pre><code> python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/higher_resolution/tiny_vit_21m_224to384.yaml --data-path ./ImageNet --batch-size 32 --pretrained ./checkpoints/tiny_vit_21m_22kto1k_distill.pth --output ./output  --accumulation-steps 4\n</code></pre>\n</details>\n\n<details>\n<summary>Finetune with higher resolution from 384 to 512</summary>\n<pre><code> python -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/higher_resolution/tiny_vit_21m_384to512.yaml --data-path ./ImageNet --batch-size 32 --pretrained ./checkpoints/tiny_vit_21m_22kto1k_384_distill.pth --output ./output  --accumulation-steps 4\n</code></pre>\n</details>\n\n## Train the model from scratch on ImageNet-1k\n\nHere is the command to train TinyViT from scratch on ImageNet-1k.\n\n```bash\npython -m torch.distributed.launch --nproc_per_node 8 main.py --cfg configs/1k/tiny_vit_21m.yaml --data-path ./ImageNet --batch-size 128 --output ./output\n```\n", "doc_id": "5e6655a5-0b0e-463b-b383-1e5b6c51bf7f", "embedding": null, "doc_hash": "c2b33f57417e5b48c204c75cab4d8afb0983072d8d00f9266770d7bdf46c6ba1", "extra_info": {"file_path": "TinyViT/docs/TRAINING.md", "file_name": "TRAINING.md"}, "node_info": {"start": 2010, "end": 3022}, "relationships": {"1": "3997001189ab390bb5556fbaa4f192d21a5ca97e", "2": "99c27247-dafc-48de-832b-e9ec1dd087bc"}}, "__type__": "1"}, "e1c5028b-c6ac-4360-8ca3-3a9ca2a3e271": {"__data__": {"text": "# --------------------------------------------------------\n# Logger\n# Copyright (c) 2022 Microsoft\n# Based on the code: Swin Transformer\n#   (https://github.com/microsoft/swin-transformer)\n# --------------------------------------------------------\n\nimport os\nimport sys\nimport logging\nimport functools\nfrom termcolor import colored\n\n\n@functools.lru_cache()\ndef create_logger(output_dir, dist_rank=0, name=''):\n    # create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    logger.propagate = False\n\n    # create formatter\n    fmt = '[%(asctime)s %(name)s] (%(filename)s %(lineno)d): %(levelname)s %(message)s'\n    color_fmt = colored('[%(asctime)s %(name)s]', 'green') + \\\n        colored('(%(filename)s %(lineno)d)', 'yellow') + \\\n        ': %(levelname)s %(message)s'\n\n    # create console handlers for master process\n    if dist_rank == 0:\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(logging.DEBUG)\n        console_handler.setFormatter(\n            logging.Formatter(fmt=color_fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n        logger.addHandler(console_handler)\n\n    # create file handlers\n    file_handler = logging.FileHandler(os.path.join(\n        output_dir, f'log_rank{dist_rank}.txt'), mode='a')\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(logging.Formatter(\n        fmt=fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n    logger.addHandler(file_handler)\n\n    return logger\n", "doc_id": "e1c5028b-c6ac-4360-8ca3-3a9ca2a3e271", "embedding": null, "doc_hash": "6dd5dbeb69c43b0cd283dfc3511b4823270205123dc94212722c92655fa762ed", "extra_info": {"file_path": "TinyViT/logger.py", "file_name": "logger.py"}, "node_info": {"start": 0, "end": 1472}, "relationships": {"1": "ea407d8f75c82d2340a4822768dea1b1d1dc5f3b"}}, "__type__": "1"}, "c99921b7-8bc1-4328-a4cd-2037b078b2ae": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT Learning rate scheduler\n# Copyright (c) 2022 Microsoft\n# Based on the code: Swin Transformer\n#   (https://github.com/microsoft/swin-transformer)\n# --------------------------------------------------------\n\nimport torch\nfrom timm.scheduler.cosine_lr import CosineLRScheduler\nfrom timm.scheduler.step_lr import StepLRScheduler\nfrom timm.scheduler.scheduler import Scheduler\n# Modified for TinyViT\nfrom tinyvit_utils import LRSchedulerWrapper\n\n\ndef build_scheduler(config, optimizer, n_iter_per_epoch):\n    num_steps = int(config.TRAIN.EPOCHS * n_iter_per_epoch)\n    warmup_steps = int(config.TRAIN.WARMUP_EPOCHS * n_iter_per_epoch)\n    decay_steps = int(\n        config.TRAIN.LR_SCHEDULER.DECAY_EPOCHS * n_iter_per_epoch)\n\n    lr_scheduler = None\n    if config.TRAIN.LR_SCHEDULER.NAME == 'cosine':\n        lr_scheduler = CosineLRScheduler(\n            optimizer,\n            t_initial=num_steps,\n            lr_min=config.TRAIN.MIN_LR,\n            warmup_lr_init=config.TRAIN.WARMUP_LR,\n            warmup_t=warmup_steps,\n            cycle_limit=1,\n            t_in_epochs=False,\n        )\n    elif config.TRAIN.LR_SCHEDULER.NAME == 'linear':\n        lr_scheduler = LinearLRScheduler(\n            optimizer,\n            t_initial=num_steps,\n            lr_min_rate=0.01,\n            warmup_lr_init=config.TRAIN.WARMUP_LR,\n            warmup_t=warmup_steps,\n            t_in_epochs=False,\n        )\n    elif config.TRAIN.LR_SCHEDULER.NAME == 'step':\n        lr_scheduler = StepLRScheduler(\n         ", "doc_id": "c99921b7-8bc1-4328-a4cd-2037b078b2ae", "embedding": null, "doc_hash": "369ec29b0f5d7c0b8e495f7d7ff3f695a0ada4b7b27c146e50992e144811fd19", "extra_info": {"file_path": "TinyViT/lr_scheduler.py", "file_name": "lr_scheduler.py"}, "node_info": {"start": 0, "end": 1564}, "relationships": {"1": "320cb3c16cf85f2501d99a058f1de1500cf8b7c5", "3": "832f1438-4af1-4f17-8151-f8c1c0758516"}}, "__type__": "1"}, "832f1438-4af1-4f17-8151-f8c1c0758516": {"__data__": {"text": "       )\n    elif config.TRAIN.LR_SCHEDULER.NAME == 'step':\n        lr_scheduler = StepLRScheduler(\n            optimizer,\n            decay_t=decay_steps,\n            decay_rate=config.TRAIN.LR_SCHEDULER.DECAY_RATE,\n            warmup_lr_init=config.TRAIN.WARMUP_LR,\n            warmup_t=warmup_steps,\n            t_in_epochs=False,\n        )\n\n    # Modified for TinyViT\n    if config.TRAIN.LAYER_LR_DECAY != 1.0:\n        lr_scheduler = LRSchedulerWrapper(lr_scheduler, optimizer)\n    return lr_scheduler\n\n\nclass LinearLRScheduler(Scheduler):\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 t_initial: int,\n                 lr_min_rate: float,\n                 warmup_t=0,\n                 warmup_lr_init=0.,\n                 t_in_epochs=True,\n                 noise_range_t=None,\n                 noise_pct=0.67,\n                 noise_std=1.0,\n                 noise_seed=42,\n                 initialize=True,\n                 ) -> None:\n        super().__init__(\n            optimizer, param_group_field=\"lr\",\n            noise_range_t=noise_range_t, noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed,\n            initialize=initialize)\n\n       ", "doc_id": "832f1438-4af1-4f17-8151-f8c1c0758516", "embedding": null, "doc_hash": "9c9148d0914c9deb50d1b036fc1083c792d830d0501c351ab761404480204913", "extra_info": {"file_path": "TinyViT/lr_scheduler.py", "file_name": "lr_scheduler.py"}, "node_info": {"start": 1489, "end": 2695}, "relationships": {"1": "320cb3c16cf85f2501d99a058f1de1500cf8b7c5", "2": "c99921b7-8bc1-4328-a4cd-2037b078b2ae", "3": "6ce7b357-b9e4-415f-a789-64cf4b1c792b"}}, "__type__": "1"}, "6ce7b357-b9e4-415f-a789-64cf4b1c792b": {"__data__": {"text": "noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed,\n            initialize=initialize)\n\n        self.t_initial = t_initial\n        self.lr_min_rate = lr_min_rate\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.t_in_epochs = t_in_epochs\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) /\n                                 self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            t = t - self.warmup_t\n            total_t = self.t_initial - self.warmup_t\n            lrs = [v - ((v - v * self.lr_min_rate) * (t / total_t))\n                   for v in self.base_values]\n        return lrs\n\n    def get_epoch_values(self, epoch: int):\n        if self.t_in_epochs:\n            return self._get_lr(epoch)\n        else:\n            return None\n\n    def get_update_values(self, num_updates: int):\n        if not self.t_in_epochs:\n            return self._get_lr(num_updates)\n        else:\n            return None\n", "doc_id": "6ce7b357-b9e4-415f-a789-64cf4b1c792b", "embedding": null, "doc_hash": "323d6673d46cec29a66c5c1da3b74e74c5553dc2b8dc4ca22e350029f54dbab5", "extra_info": {"file_path": "TinyViT/lr_scheduler.py", "file_name": "lr_scheduler.py"}, "node_info": {"start": 2663, "end": 3927}, "relationships": {"1": "320cb3c16cf85f2501d99a058f1de1500cf8b7c5", "2": "832f1438-4af1-4f17-8151-f8c1c0758516"}}, "__type__": "1"}, "c57337dc-f595-4db5-9a49-070a4c777c8d": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT Main (train/validate)\n# Copyright (c) 2022 Microsoft\n# Based on the code: Swin Transformer\n#   (https://github.com/microsoft/swin-transformer)\n# Add distillation with saved teacher logits\n# --------------------------------------------------------\n\nimport os\nimport time\nimport random\nimport argparse\nimport datetime\nfrom collections import defaultdict\nimport numpy as np\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\n\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom timm.utils import accuracy\nfrom my_meter import AverageMeter\n\nfrom config import get_config\nfrom models import build_model\nfrom data import build_loader\nfrom lr_scheduler import build_scheduler\nfrom optimizer import build_optimizer\nfrom logger import create_logger\nfrom utils import load_checkpoint, load_pretrained, save_checkpoint,\\\n    NativeScalerWithGradNormCount,\\\n    auto_resume_helper, is_main_process,\\\n    add_common_args,\\\n    get_git_info\n\nfrom models.remap_layer import RemapLayer\nremap_layer_22kto1k = RemapLayer('./imagenet_1kto22k.txt')\n\ntry:\n    import wandb\nexcept ImportError:\n    wandb = None\nNORM_ITER_LEN = 100\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser(\n        'TinyViT training and evaluation script', add_help=False)\n    add_common_args(parser)\n    args = parser.parse_args()\n\n    config = get_config(args)\n\n    return args, config\n\n\ndef main(args, config):\n    dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn = build_loader(\n        config)\n\n    logger.info(f\"Creating model:{config.MODEL.TYPE}/{config.MODEL.NAME}\")\n    model = build_model(config)\n    if not args.only_cpu:\n        model.cuda()\n\n    if args.use_sync_bn:\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n\n    logger.info(str(model))\n\n    optimizer = build_optimizer(config, model)\n\n    if not args.only_cpu:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[config.LOCAL_RANK], broadcast_buffers=False)\n        model_without_ddp = model.module\n", "doc_id": "c57337dc-f595-4db5-9a49-070a4c777c8d", "embedding": null, "doc_hash": "39dcd4de2ca1e3c7cc9b058934f88933792be7a63b8cfff525c5cd96a0c82908", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 0, "end": 2143}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "3": "611d1788-3577-444c-a877-98961480fa18"}}, "__type__": "1"}, "611d1788-3577-444c-a877-98961480fa18": {"__data__": {"text": "           model, device_ids=[config.LOCAL_RANK], broadcast_buffers=False)\n        model_without_ddp = model.module\n    else:\n        model_without_ddp = model\n\n    loss_scaler = NativeScalerWithGradNormCount(grad_scaler_enabled=config.AMP_ENABLE)\n\n    n_parameters = sum(p.numel()\n                       for p in model.parameters() if p.requires_grad)\n    logger.info(f\"number of params: {n_parameters}\")\n    if hasattr(model_without_ddp, 'flops'):\n        flops = model_without_ddp.flops()\n        logger.info(f\"number of GFLOPs: {flops / 1e9}\")\n\n    lr_scheduler = build_scheduler(config, optimizer, len(\n        data_loader_train) // config.TRAIN.ACCUMULATION_STEPS)\n\n    if config.DISTILL.ENABLED:\n        # we disable MIXUP and CUTMIX when knowledge distillation\n        assert len(\n            config.DISTILL.TEACHER_LOGITS_PATH) > 0, \"Please fill in DISTILL.TEACHER_LOGITS_PATH\"\n        criterion = SoftTargetCrossEntropy()\n    else:\n        if config.AUG.MIXUP > 0.:\n            # smoothing is handled with mixup label transform\n            criterion = SoftTargetCrossEntropy()\n        elif config.MODEL.LABEL_SMOOTHING > 0.:\n            criterion = LabelSmoothingCrossEntropy(\n                smoothing=config.MODEL.LABEL_SMOOTHING)\n        else:\n            criterion = torch.nn.CrossEntropyLoss()\n\n    max_accuracy = 0.0\n\n    if config.TRAIN.AUTO_RESUME:\n        resume_file = auto_resume_helper(config.OUTPUT)\n        if resume_file:\n            if config.MODEL.RESUME:\n                logger.warning(\n                    f\"auto-resume changing resume file from", "doc_id": "611d1788-3577-444c-a877-98961480fa18", "embedding": null, "doc_hash": "8d814c56ce87d8a73b773d74491a2f4e5d87fd52c52396c200436a391ea95f38", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 2051, "end": 3625}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "c57337dc-f595-4db5-9a49-070a4c777c8d", "3": "2e797632-b08b-44fd-b688-806b424d8a49"}}, "__type__": "1"}, "2e797632-b08b-44fd-b688-806b424d8a49": {"__data__": {"text": "   if config.MODEL.RESUME:\n                logger.warning(\n                    f\"auto-resume changing resume file from {config.MODEL.RESUME} to {resume_file}\")\n            config.defrost()\n            config.MODEL.RESUME = resume_file\n            config.freeze()\n            logger.info(f'auto resuming from {resume_file}')\n        else:\n            logger.info(\n                f'no checkpoint found in {config.OUTPUT}, ignoring auto resume')\n\n    if config.MODEL.RESUME:\n        max_accuracy = load_checkpoint(\n            config, model_without_ddp, optimizer, lr_scheduler, loss_scaler, logger)\n        acc1, acc5, loss = validate(args, config, data_loader_val, model)\n        logger.info(\n            f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n        if config.EVAL_MODE:\n            return\n\n    if config.MODEL.PRETRAINED and (not config.MODEL.RESUME):\n        load_pretrained(config, model_without_ddp, logger)\n        acc1, acc5, loss = validate(args, config, data_loader_val, model)\n        logger.info(\n            f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n\n    if config.THROUGHPUT_MODE:\n        throughput(data_loader_val, model, logger)\n        return\n\n    logger.info(\"Start training\")\n    start_time = time.time()\n    for epoch in range(config.TRAIN.START_EPOCH, config.TRAIN.EPOCHS):\n        # set_epoch for dataset_train when distillation\n        if hasattr(dataset_train, 'set_epoch'):\n            dataset_train.set_epoch(epoch)\n       ", "doc_id": "2e797632-b08b-44fd-b688-806b424d8a49", "embedding": null, "doc_hash": "4c06ad9f56e7b561326d59863f54a2019ec6b9dd54dc2354843c43d6658a2a24", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 3645, "end": 5172}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "611d1788-3577-444c-a877-98961480fa18", "3": "6933b04f-451f-429a-b5c3-4888f773e05a"}}, "__type__": "1"}, "6933b04f-451f-429a-b5c3-4888f773e05a": {"__data__": {"text": "when distillation\n        if hasattr(dataset_train, 'set_epoch'):\n            dataset_train.set_epoch(epoch)\n        data_loader_train.sampler.set_epoch(epoch)\n\n        if config.DISTILL.ENABLED:\n            train_one_epoch_distill_using_saved_logits(\n                args, config, model, criterion, data_loader_train, optimizer, epoch, mixup_fn, lr_scheduler, loss_scaler)\n        else:\n            train_one_epoch(args, config, model, criterion,\n                            data_loader_train, optimizer, epoch, mixup_fn, lr_scheduler, loss_scaler)\n        if dist.get_rank() == 0 and (epoch % config.SAVE_FREQ == 0 or epoch == (config.TRAIN.EPOCHS - 1)):\n            save_checkpoint(config, epoch, model_without_ddp,\n                            max_accuracy, optimizer, lr_scheduler, loss_scaler, logger)\n\n        acc1, acc5, loss = validate(args, config, data_loader_val, model)\n        logger.info(\n            f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n        max_accuracy = max(max_accuracy, acc1)\n        logger.info(f'Max accuracy: {max_accuracy:.2f}%')\n\n        if is_main_process() and args.use_wandb:\n            wandb.log({\n                f\"val/acc@1\": acc1,\n                f\"val/acc@5\": acc5,\n                f\"val/loss\": loss,\n                \"epoch\": epoch,\n            })\n            wandb.run.summary['epoch'] = epoch\n           ", "doc_id": "6933b04f-451f-429a-b5c3-4888f773e05a", "embedding": null, "doc_hash": "21fdff41289a633fb3a606d9572de258658fed3807708db36c5e593d70603bf1", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 5161, "end": 6546}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "2e797632-b08b-44fd-b688-806b424d8a49", "3": "d11afd8e-a3be-4dbb-b0cd-76d4926b9dd3"}}, "__type__": "1"}, "d11afd8e-a3be-4dbb-b0cd-76d4926b9dd3": {"__data__": {"text": "      \"epoch\": epoch,\n            })\n            wandb.run.summary['epoch'] = epoch\n            wandb.run.summary['best_acc@1'] = max_accuracy\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info('Training time {}'.format(total_time_str))\n\n\ndef is_valid_grad_norm(num):\n    if num is None:\n        return False\n    return not bool(torch.isinf(num)) and not bool(torch.isnan(num))\n\n\ndef set_bn_state(config, model):\n    if config.TRAIN.EVAL_BN_WHEN_TRAINING:\n        for m in model.modules():\n            if isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n                m.eval()\n\n\ndef train_one_epoch(args, config, model, criterion, data_loader, optimizer, epoch, mixup_fn, lr_scheduler, loss_scaler):\n    model.train()\n    set_bn_state(config, model)\n    optimizer.zero_grad()\n\n    num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    norm_meter = AverageMeter()\n    scaler_meter = AverageMeter()\n    acc1_meter = AverageMeter()\n    acc5_meter = AverageMeter()\n\n    start = time.time()\n    end = time.time()\n    for idx, (samples, targets) in enumerate(data_loader):\n        normal_global_idx = epoch * NORM_ITER_LEN + \\\n            (idx * NORM_ITER_LEN // num_steps)\n\n        samples = samples.cuda(non_blocking=True)\n        targets = targets.cuda(non_blocking=True)\n\n        if mixup_fn is not None:\n            samples, targets = mixup_fn(samples, targets)\n            original_targets = targets.argmax(dim=1)\n        else:\n            original_targets = targets\n\n        with", "doc_id": "d11afd8e-a3be-4dbb-b0cd-76d4926b9dd3", "embedding": null, "doc_hash": "3dad1e90ca8f66c9ad168d63c907ea4017e6b4f4c545ec56b49b653bf1e3e964", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 6582, "end": 8190}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "6933b04f-451f-429a-b5c3-4888f773e05a", "3": "7fa0b494-f58f-4488-ac8e-4dc3043b6d2b"}}, "__type__": "1"}, "7fa0b494-f58f-4488-ac8e-4dc3043b6d2b": {"__data__": {"text": "       original_targets = targets.argmax(dim=1)\n        else:\n            original_targets = targets\n\n        with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n            outputs = model(samples)\n\n        loss = criterion(outputs, targets)\n        loss = loss / config.TRAIN.ACCUMULATION_STEPS\n\n        # this attribute is added by timm on one optimizer (adahessian)\n        is_second_order = hasattr(\n            optimizer, 'is_second_order') and optimizer.is_second_order\n        grad_norm = loss_scaler(loss, optimizer, clip_grad=config.TRAIN.CLIP_GRAD,\n                                parameters=model.parameters(), create_graph=is_second_order,\n                                update_grad=(idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0)\n        if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n            optimizer.zero_grad()\n            lr_scheduler.step_update(\n                (epoch * num_steps + idx) // config.TRAIN.ACCUMULATION_STEPS)\n        loss_scale_value = loss_scaler.state_dict().get(\"scale\", 1.0)\n\n        with torch.no_grad():\n            acc1, acc5 = accuracy(outputs, original_targets, topk=(1, 5))\n        acc1_meter.update(acc1.item(), targets.size(0))\n        acc5_meter.update(acc5.item(), targets.size(0))\n\n        torch.cuda.synchronize()\n\n        loss_meter.update(loss.item(), targets.size(0))\n        if is_valid_grad_norm(grad_norm):\n            norm_meter.update(grad_norm)\n        scaler_meter.update(loss_scale_value)\n       ", "doc_id": "7fa0b494-f58f-4488-ac8e-4dc3043b6d2b", "embedding": null, "doc_hash": "3abe5a82141e0a08379dabe09edabe523521b889c85528016811b4b5e7c65f56", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 8167, "end": 9644}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "d11afd8e-a3be-4dbb-b0cd-76d4926b9dd3", "3": "d0484994-2212-444c-9d91-370f052c5d56"}}, "__type__": "1"}, "d0484994-2212-444c-9d91-370f052c5d56": {"__data__": {"text": "if is_valid_grad_norm(grad_norm):\n            norm_meter.update(grad_norm)\n        scaler_meter.update(loss_scale_value)\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            lr = optimizer.param_groups[0]['lr']\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            etas = batch_time.avg * (num_steps - idx)\n            logger.info(\n                f'Train: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]\\t'\n                f'eta {datetime.timedelta(seconds=int(etas))} lr {lr:.6f}\\t'\n                f'time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'Acc@1 {acc1_meter.val:.3f} ({acc1_meter.avg:.3f})\\t'\n                f'Acc@5 {acc5_meter.val:.3f} ({acc5_meter.avg:.3f})\\t'\n                f'grad_norm {norm_meter.val:.4f} ({norm_meter.avg:.4f})\\t'\n                f'loss_scale {scaler_meter.val:.4f} ({scaler_meter.avg:.4f})\\t'\n                f'mem {memory_used:.0f}MB')\n\n            if is_main_process() and args.use_wandb:\n                wandb.log({\n                    \"train/acc@1\": acc1_meter.val,\n                   ", "doc_id": "d0484994-2212-444c-9d91-370f052c5d56", "embedding": null, "doc_hash": "838643fcad2681a214eef6fa6936be73d028f43142a9b872489f1ff5dee86052", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 9620, "end": 10851}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "7fa0b494-f58f-4488-ac8e-4dc3043b6d2b", "3": "f2dec06b-d9b0-49a8-b2c5-e22e3521d50d"}}, "__type__": "1"}, "f2dec06b-d9b0-49a8-b2c5-e22e3521d50d": {"__data__": {"text": "    wandb.log({\n                    \"train/acc@1\": acc1_meter.val,\n                    \"train/acc@5\": acc5_meter.val,\n                    \"train/loss\": loss_meter.val,\n                    \"train/grad_norm\": norm_meter.val,\n                    \"train/loss_scale\": scaler_meter.val,\n                    \"train/lr\": lr,\n                }, step=normal_global_idx)\n    epoch_time = time.time() - start\n    logger.info(\n        f\"EPOCH {epoch} training takes {datetime.timedelta(seconds=int(epoch_time))}\")\n\n\ndef train_one_epoch_distill_using_saved_logits(args, config, model, criterion, data_loader, optimizer, epoch, mixup_fn, lr_scheduler, loss_scaler):\n    model.train()\n    set_bn_state(config, model)\n    optimizer.zero_grad()\n\n    num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    norm_meter = AverageMeter()\n    scaler_meter = AverageMeter()\n    meters = defaultdict(AverageMeter)\n\n    start = time.time()\n    end = time.time()\n    data_tic = time.time()\n\n    num_classes = config.MODEL.NUM_CLASSES\n    topk = config.DISTILL.LOGITS_TOPK\n\n    for idx, ((samples, targets), (logits_index, logits_value, seeds)) in enumerate(data_loader):\n        normal_global_idx = epoch * NORM_ITER_LEN + \\\n            (idx * NORM_ITER_LEN // num_steps)\n\n        samples = samples.cuda(non_blocking=True)\n        targets = targets.cuda(non_blocking=True)\n\n        if mixup_fn is not None:\n            samples, targets = mixup_fn(samples, targets, seeds)\n            original_targets =", "doc_id": "f2dec06b-d9b0-49a8-b2c5-e22e3521d50d", "embedding": null, "doc_hash": "da9570e610302a2ea55488931bc89c98febbc0a050f144867419017a88c4e669", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 10910, "end": 12426}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "d0484994-2212-444c-9d91-370f052c5d56", "3": "fcba4aa7-15f7-4f80-99b6-c6ce8a295f9f"}}, "__type__": "1"}, "fcba4aa7-15f7-4f80-99b6-c6ce8a295f9f": {"__data__": {"text": "   if mixup_fn is not None:\n            samples, targets = mixup_fn(samples, targets, seeds)\n            original_targets = targets.argmax(dim=1)\n        else:\n            original_targets = targets\n        meters['data_time'].update(time.time() - data_tic)\n\n        with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n            outputs = model(samples)\n\n        # recover teacher logits\n        logits_index = logits_index.long()\n        logits_value = logits_value.float()\n        logits_index = logits_index.cuda(non_blocking=True)\n        logits_value = logits_value.cuda(non_blocking=True)\n        minor_value = (1.0 - logits_value.sum(-1, keepdim=True)\n                       ) / (num_classes - topk)\n        minor_value = minor_value.repeat_interleave(num_classes, dim=-1)\n        outputs_teacher = minor_value.scatter_(-1, logits_index, logits_value)\n\n        loss = criterion(outputs, outputs_teacher)\n        loss = loss / config.TRAIN.ACCUMULATION_STEPS\n\n        # this attribute is added by timm on one optimizer (adahessian)\n        is_second_order = hasattr(\n            optimizer, 'is_second_order') and optimizer.is_second_order\n        grad_norm = loss_scaler(loss, optimizer, clip_grad=config.TRAIN.CLIP_GRAD,\n                                parameters=model.parameters(), create_graph=is_second_order,\n                                update_grad=(idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0)\n        if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n            optimizer.zero_grad()\n      ", "doc_id": "fcba4aa7-15f7-4f80-99b6-c6ce8a295f9f", "embedding": null, "doc_hash": "33dcfda4793c9199b56b1e54a6768ba3ab9b5fb76bc241658076d2feb8cff046", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 12383, "end": 13906}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "f2dec06b-d9b0-49a8-b2c5-e22e3521d50d", "3": "cf46471c-f41c-4630-b4d8-d58f38692d65"}}, "__type__": "1"}, "cf46471c-f41c-4630-b4d8-d58f38692d65": {"__data__": {"text": "== 0)\n        if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n            optimizer.zero_grad()\n            lr_scheduler.step_update(\n                (epoch * num_steps + idx) // config.TRAIN.ACCUMULATION_STEPS)\n        loss_scale_value = loss_scaler.state_dict().get(\"scale\", 1.0)\n\n        # compute accuracy\n        real_batch_size = len(original_targets)\n        acc1, acc5 = accuracy(outputs, original_targets, topk=(1, 5))\n        meters['train_acc1'].update(acc1.item(), real_batch_size)\n        meters['train_acc5'].update(acc5.item(), real_batch_size)\n        teacher_acc1, teacher_acc5 = accuracy(\n            outputs_teacher, original_targets, topk=(1, 5))\n        meters['teacher_acc1'].update(teacher_acc1.item(), real_batch_size)\n        meters['teacher_acc5'].update(teacher_acc5.item(), real_batch_size)\n\n        torch.cuda.synchronize()\n\n        loss_meter.update(loss.item(), real_batch_size)\n        if is_valid_grad_norm(grad_norm):\n            norm_meter.update(grad_norm)\n        scaler_meter.update(loss_scale_value)\n        batch_time.update(time.time() - end)\n        end = time.time()\n        data_tic = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            lr = optimizer.param_groups[0]['lr']\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            etas = batch_time.avg * (num_steps - idx)\n\n            extra_meters_str = ''\n            for k, v in meters.items():\n                extra_meters_str += f'{k}", "doc_id": "cf46471c-f41c-4630-b4d8-d58f38692d65", "embedding": null, "doc_hash": "d93a2e8b92d9f391e740f925d454b6caff9fb4e0628a9ec80b725b0f51713959", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 13920, "end": 15407}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "fcba4aa7-15f7-4f80-99b6-c6ce8a295f9f", "3": "0f23af1f-9916-4454-ac7f-37edc8161b2d"}}, "__type__": "1"}, "0f23af1f-9916-4454-ac7f-37edc8161b2d": {"__data__": {"text": "      extra_meters_str = ''\n            for k, v in meters.items():\n                extra_meters_str += f'{k} {v.val:.4f} ({v.avg:.4f})\\t'\n            logger.info(\n                f'Train: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]\\t'\n                f'eta {datetime.timedelta(seconds=int(etas))} lr {lr:.6f}\\t'\n                f'time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'grad_norm {norm_meter.val:.4f} ({norm_meter.avg:.4f})\\t'\n                f'loss_scale {scaler_meter.val:.4f} ({scaler_meter.avg:.4f})\\t'\n                f'{extra_meters_str}'\n                f'mem {memory_used:.0f}MB')\n\n            if is_main_process() and args.use_wandb:\n                acc1_meter, acc5_meter = meters['train_acc1'], meters['train_acc5']\n                wandb.log({\n                    \"train/acc@1\": acc1_meter.val,\n                    \"train/acc@5\": acc5_meter.val,\n                    \"train/loss\": loss_meter.val,\n                    \"train/grad_norm\": norm_meter.val,\n                    \"train/loss_scale\": scaler_meter.val,\n                    \"train/lr\": lr,\n                },", "doc_id": "0f23af1f-9916-4454-ac7f-37edc8161b2d", "embedding": null, "doc_hash": "b71145044b9cb7813326b53233b3040479fea7c5096eccc9dd2db096bb12a2be", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 15414, "end": 16596}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "cf46471c-f41c-4630-b4d8-d58f38692d65", "3": "cee3d84b-a893-42a3-a4aa-b8db7ac38666"}}, "__type__": "1"}, "cee3d84b-a893-42a3-a4aa-b8db7ac38666": {"__data__": {"text": "      \"train/loss_scale\": scaler_meter.val,\n                    \"train/lr\": lr,\n                }, step=normal_global_idx)\n    epoch_time = time.time() - start\n    extra_meters_str = f'Train-Summary: [{epoch}/{config.TRAIN.EPOCHS}]\\t'\n    for k, v in meters.items():\n        v.sync()\n        extra_meters_str += f'{k} {v.val:.4f} ({v.avg:.4f})\\t'\n    logger.info(extra_meters_str)\n    logger.info(\n        f\"EPOCH {epoch} training takes {datetime.timedelta(seconds=int(epoch_time))}\")\n\n\n@torch.no_grad()\ndef validate(args, config, data_loader, model, num_classes=1000):\n    criterion = torch.nn.CrossEntropyLoss()\n    model.eval()\n\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    acc1_meter = AverageMeter()\n    acc5_meter = AverageMeter()\n\n    end = time.time()\n    for idx, (images, target) in enumerate(data_loader):\n        if not args.only_cpu:\n            images = images.cuda(non_blocking=True)\n            target = target.cuda(non_blocking=True)\n\n        # compute output\n        with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n            output = model(images)\n        if num_classes == 1000:\n            output_num_classes = output.size(-1)\n            if output_num_classes == 21841:\n                output = remap_layer_22kto1k(output)\n\n        # measure accuracy and record loss\n        loss = criterion(output, target)\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        loss_meter.update(loss.item(), target.size(0))\n        acc1_meter.update(acc1.item(), target.size(0))\n        acc5_meter.update(acc5.item(),", "doc_id": "cee3d84b-a893-42a3-a4aa-b8db7ac38666", "embedding": null, "doc_hash": "ccb598dab78078e31714fbc69780874f967bdf25670b252933f24d287d9b9dd7", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 16610, "end": 18181}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "0f23af1f-9916-4454-ac7f-37edc8161b2d", "3": "de17ed8e-a338-468c-98aa-de7d440fef6d"}}, "__type__": "1"}, "de17ed8e-a338-468c-98aa-de7d440fef6d": {"__data__": {"text": "  loss_meter.update(loss.item(), target.size(0))\n        acc1_meter.update(acc1.item(), target.size(0))\n        acc5_meter.update(acc5.item(), target.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            logger.info(\n                f'Test: [{idx}/{len(data_loader)}]\\t'\n                f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                f'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'Acc@1 {acc1_meter.val:.3f} ({acc1_meter.avg:.3f})\\t'\n                f'Acc@5 {acc5_meter.val:.3f} ({acc5_meter.avg:.3f})\\t'\n                f'Mem {memory_used:.0f}MB')\n\n    acc1_meter.sync()\n    acc5_meter.sync()\n    logger.info(\n        f' The number of validation samples is {int(acc1_meter.count)}')\n    logger.info(f' * Acc@1 {acc1_meter.avg:.3f} Acc@5 {acc5_meter.avg:.3f}')\n    return acc1_meter.avg, acc5_meter.avg, loss_meter.avg\n\n\n@torch.no_grad()\ndef throughput(data_loader, model, logger):\n    # we follow the throughput measurement of LeViT repo (https://github.com/facebookresearch/LeViT/blob/main/speed_test.py)\n    model.eval()\n\n    T0, T1 = 10, 60\n    images, _ = next(iter(data_loader))\n    batch_size, _, H, W = images.shape\n    inputs = torch.randn(batch_size, 3, H, W).cuda(non_blocking=True)\n\n    # trace model", "doc_id": "de17ed8e-a338-468c-98aa-de7d440fef6d", "embedding": null, "doc_hash": "a175ef87a465420375c88d34b670609d42d24b7d259e9a3d51017b5d7d5a26f3", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 18114, "end": 19566}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "cee3d84b-a893-42a3-a4aa-b8db7ac38666", "3": "69e99923-bc45-4146-a7ac-cf265829bd97"}}, "__type__": "1"}, "69e99923-bc45-4146-a7ac-cf265829bd97": {"__data__": {"text": "next(iter(data_loader))\n    batch_size, _, H, W = images.shape\n    inputs = torch.randn(batch_size, 3, H, W).cuda(non_blocking=True)\n\n    # trace model to avoid python overhead\n    model = torch.jit.trace(model, inputs)\n\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n\n    start = time.time()\n    with torch.cuda.amp.autocast():\n        while time.time() - start < T0:\n            model(inputs)\n    timing = []\n    torch.cuda.synchronize()\n    with torch.cuda.amp.autocast():\n        while sum(timing) < T1:\n            start = time.time()\n            model(inputs)\n            torch.cuda.synchronize()\n            timing.append(time.time() - start)\n    timing = torch.as_tensor(timing, dtype=torch.float32)\n    throughput = batch_size / timing.mean().item()\n    logger.info(f\"batch_size {batch_size} throughput {throughput}\")\n\n\nif __name__ == '__main__':\n    args, config = parse_option()\n    config.defrost()\n    if config.DISTILL.TEACHER_LOGITS_PATH:\n        config.DISTILL.ENABLED = True\n    config.freeze()\n\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        rank = int(os.environ[\"RANK\"])\n        world_size = int(os.environ['WORLD_SIZE'])\n        print(f\"RANK and WORLD_SIZE in environ: {rank}/{world_size}\")\n    else:\n        rank = -1\n        world_size = -1\n\n    if args.only_cpu:\n        ddp_backend = 'gloo'\n    else:\n        torch.cuda.set_device(config.LOCAL_RANK)\n        ddp_backend = 'nccl'\n\n    torch.distributed.init_process_group(\n        backend=ddp_backend, init_method='env://', world_size=world_size, rank=rank)\n   ", "doc_id": "69e99923-bc45-4146-a7ac-cf265829bd97", "embedding": null, "doc_hash": "f0cc291d43444e9a3aff2bcc75f4d880e7bd00a91bc968ab8bcf380c10d02e58", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 19562, "end": 21129}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "de17ed8e-a338-468c-98aa-de7d440fef6d", "3": "dae1d2cc-8b94-4510-b1c8-04f77893cfb2"}}, "__type__": "1"}, "dae1d2cc-8b94-4510-b1c8-04f77893cfb2": {"__data__": {"text": "= 'nccl'\n\n    torch.distributed.init_process_group(\n        backend=ddp_backend, init_method='env://', world_size=world_size, rank=rank)\n    torch.distributed.barrier()\n\n    seed = config.SEED + dist.get_rank()\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    cudnn.benchmark = True\n\n    # linear scale the learning rate according to total batch size, may not be optimal\n    linear_scaled_lr = config.TRAIN.BASE_LR * \\\n        config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_warmup_lr = config.TRAIN.WARMUP_LR * \\\n        config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_min_lr = config.TRAIN.MIN_LR * \\\n        config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    # gradient accumulation also need to scale the learning rate\n    if config.TRAIN.ACCUMULATION_STEPS > 1:\n        linear_scaled_lr = linear_scaled_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_warmup_lr = linear_scaled_warmup_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_min_lr = linear_scaled_min_lr * config.TRAIN.ACCUMULATION_STEPS\n    config.defrost()\n    config.TRAIN.BASE_LR = linear_scaled_lr\n    config.TRAIN.WARMUP_LR = linear_scaled_warmup_lr\n    config.TRAIN.MIN_LR = linear_scaled_min_lr\n    config.freeze()\n\n    os.makedirs(config.OUTPUT, exist_ok=True)\n    logger = create_logger(output_dir=config.OUTPUT,\n                           dist_rank=dist.get_rank(), name=f\"{config.MODEL.NAME}\")\n\n    if is_main_process():\n        path = os.path.join(config.OUTPUT, \"config.json\")\n        with open(path, \"w\") as f:\n            f.write(config.dump())\n  ", "doc_id": "dae1d2cc-8b94-4510-b1c8-04f77893cfb2", "embedding": null, "doc_hash": "a8fa0969455867fdcfdfaafc7bab4489448f85897ae9b94b28a24d975e908ceb", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 21136, "end": 22805}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "69e99923-bc45-4146-a7ac-cf265829bd97", "3": "fe23e9f1-52d9-491a-863f-372ee85f6a19"}}, "__type__": "1"}, "fe23e9f1-52d9-491a-863f-372ee85f6a19": {"__data__": {"text": "   path = os.path.join(config.OUTPUT, \"config.json\")\n        with open(path, \"w\") as f:\n            f.write(config.dump())\n        logger.info(f\"Full config saved to {path}\")\n\n        config_dict = dict(config)\n        config_dict['git'] = get_git_info()\n        if args.use_wandb:\n            wandb_output_path = config.OUTPUT\n            wandb.init(project=\"TinyViT\", config=config_dict,\n                       dir=wandb_output_path)\n\n    # print git info\n    logger.info('===== git =====')\n    logger.info(str(get_git_info()))\n\n    # print config\n    logger.info(config.dump())\n\n    main(args, config)\n", "doc_id": "fe23e9f1-52d9-491a-863f-372ee85f6a19", "embedding": null, "doc_hash": "fd418d39905ab729ce2563cd2ec5b067ffaf92999a122aa5d908ae0dbaef7878", "extra_info": {"file_path": "TinyViT/main.py", "file_name": "main.py"}, "node_info": {"start": 22801, "end": 23406}, "relationships": {"1": "944c74eec7af84e779aad540f55ce7dd5f22f943", "2": "dae1d2cc-8b94-4510-b1c8-04f77893cfb2"}}, "__type__": "1"}, "38f62aa5-439b-494f-bbe3-76874724a7a6": {"__data__": {"text": "from .build import build_model\n", "doc_id": "38f62aa5-439b-494f-bbe3-76874724a7a6", "embedding": null, "doc_hash": "53f429ce830920960a68e7ba70d4c502cf67e12b21cd811fd81e8795670dce1a", "extra_info": {"file_path": "TinyViT/models/__init__.py", "file_name": "__init__.py"}, "node_info": {"start": 0, "end": 31}, "relationships": {"1": "59774f75d33735d65860d860c995b1268238da57"}}, "__type__": "1"}, "a5268c29-8ad3-41e6-9fdf-6b82250d5486": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT Model Builder\n# Copyright (c) 2022 Microsoft\n# --------------------------------------------------------\n\nfrom .tiny_vit import TinyViT\n\n\ndef build_model(config):\n    model_type = config.MODEL.TYPE\n    if model_type == 'tiny_vit':\n        M = config.MODEL.TINY_VIT\n        model = TinyViT(img_size=config.DATA.IMG_SIZE,\n                        in_chans=M.IN_CHANS,\n                        num_classes=config.MODEL.NUM_CLASSES,\n                        embed_dims=M.EMBED_DIMS,\n                        depths=M.DEPTHS,\n                        num_heads=M.NUM_HEADS,\n                        window_sizes=M.WINDOW_SIZES,\n                        mlp_ratio=M.MLP_RATIO,\n                        drop_rate=config.MODEL.DROP_RATE,\n                        drop_path_rate=config.MODEL.DROP_PATH_RATE,\n                        use_checkpoint=config.TRAIN.USE_CHECKPOINT,\n                        mbconv_expand_ratio=M.MBCONV_EXPAND_RATIO,\n                        local_conv_size=M.LOCAL_CONV_SIZE,\n                        layer_lr_decay=config.TRAIN.LAYER_LR_DECAY,\n                        )\n    elif model_type == 'clip_vit_large14_224':\n        from .clip import CLIP\n        kwargs = {\n            'embed_dim': 768, 'image_resolution': 224,\n          ", "doc_id": "a5268c29-8ad3-41e6-9fdf-6b82250d5486", "embedding": null, "doc_hash": "c722fb54ae6e9de7f8b67e2b354bb437d815b76c87d9d27b07d1d9f7f44d1651", "extra_info": {"file_path": "TinyViT/models/build.py", "file_name": "build.py"}, "node_info": {"start": 0, "end": 1308}, "relationships": {"1": "181b8f2a3b6e340f6c3b6836765827ef093fb03a", "3": "2beb1718-4cfe-4730-9eb9-f8bdee87580d"}}, "__type__": "1"}, "2beb1718-4cfe-4730-9eb9-f8bdee87580d": {"__data__": {"text": "       from .clip import CLIP\n        kwargs = {\n            'embed_dim': 768, 'image_resolution': 224,\n            'vision_layers': 24, 'vision_width': 1024, 'vision_patch_size': 14,\n            \"num_classes\": config.MODEL.NUM_CLASSES,\n        }\n        model = CLIP(**kwargs)\n    else:\n        raise NotImplementedError(f\"Unkown model: {model_type}\")\n\n    return model\n", "doc_id": "2beb1718-4cfe-4730-9eb9-f8bdee87580d", "embedding": null, "doc_hash": "e759fef7dfbb662eb7bbb18851ad8c84508af02fddcb5ac031f124da67306bf5", "extra_info": {"file_path": "TinyViT/models/build.py", "file_name": "build.py"}, "node_info": {"start": 1194, "end": 1565}, "relationships": {"1": "181b8f2a3b6e340f6c3b6836765827ef093fb03a", "2": "a5268c29-8ad3-41e6-9fdf-6b82250d5486"}}, "__type__": "1"}, "6e173971-2ecf-4eb1-b117-6b9cc416a954": {"__data__": {"text": "# --------------------------------------------------------\n# CLIP Model\n# Copyright (c) 2021 OpenAI\n# MIT License\n# Link: https://github.com/openai/CLIP\n# --------------------------------------------------------\n\nfrom collections import OrderedDict\nfrom typing import Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom timm.models.layers import trunc_normal_\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask\n\n    def attention(self, x: torch.Tensor):\n        self.attn_mask = self.attn_mask.to(\n            dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n\n    def forward(self, x: torch.Tensor):\n        x = x + self.attention(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int,", "doc_id": "6e173971-2ecf-4eb1-b117-6b9cc416a954", "embedding": null, "doc_hash": "f08b801031f6a4fb0c5348b0a90ce837172a60c324fb0c1e59348204eeda810a", "extra_info": {"file_path": "TinyViT/models/clip.py", "file_name": "clip.py"}, "node_info": {"start": 0, "end": 1736}, "relationships": {"1": "2cf913651adb3d5b107864c0de10d3792c1e5a80", "3": "b607aef3-1e6e-4289-ac75-dfd558dc34b4"}}, "__type__": "1"}, "b607aef3-1e6e-4289-ac75-dfd558dc34b4": {"__data__": {"text": "   x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(\n            *[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width,\n                               kernel_size=patch_size, stride=patch_size, bias=False)\n\n        scale = width ** -0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(\n            scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n        self.ln_pre = LayerNorm(width)\n\n        self.transformer = Transformer(width, layers, heads)\n\n        self.ln_post = LayerNorm(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n\n    def forward(self, x: torch.Tensor):\n        x = self.conv1(x)  # shape = [*, width, grid, grid]\n        # shape = [*, width, grid ** 2]\n        x = x.reshape(x.shape[0], x.shape[1], -1)\n        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) +", "doc_id": "b607aef3-1e6e-4289-ac75-dfd558dc34b4", "embedding": null, "doc_hash": "f14f14c2f2667fc683054c78ee6b98f881384dba58dc9fe60ba9dd3533663743", "extra_info": {"file_path": "TinyViT/models/clip.py", "file_name": "clip.py"}, "node_info": {"start": 1623, "end": 3267}, "relationships": {"1": "2cf913651adb3d5b107864c0de10d3792c1e5a80", "2": "6e173971-2ecf-4eb1-b117-6b9cc416a954", "3": "aac6ad79-0a81-44dd-91bb-a33242fab243"}}, "__type__": "1"}, "aac6ad79-0a81-44dd-91bb-a33242fab243": {"__data__": {"text": "       x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1],\n                      dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.positional_embedding.to(x.dtype)\n        x = self.ln_pre(x)\n\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n\n        x = self.ln_post(x[:, 0, :])\n\n        if self.proj is not None:\n            x = x @ self.proj\n\n        return x\n\n\nclass CLIP(nn.Module):\n    def __init__(self,\n                 embed_dim: int,\n                 # vision\n                 image_resolution: int,\n                 vision_layers: Union[Tuple[int, int, int, int], int],\n                 vision_width: int,\n                 vision_patch_size: int,\n                 num_classes=1000,\n                 ):\n        super().__init__()\n\n        vision_heads = vision_width // 64\n        self.visual = VisionTransformer(\n            input_resolution=image_resolution,\n            patch_size=vision_patch_size,\n            width=vision_width,\n            layers=vision_layers,\n            heads=vision_heads,\n            output_dim=embed_dim,\n        )\n\n        self.head = nn.Linear(embed_dim, num_classes)\n\n      ", "doc_id": "aac6ad79-0a81-44dd-91bb-a33242fab243", "embedding": null, "doc_hash": "293fa64c6f0d32004394e8eff175fd53d0d5de20e91dd153b315d3d8a3acf2d9", "extra_info": {"file_path": "TinyViT/models/clip.py", "file_name": "clip.py"}, "node_info": {"start": 3293, "end": 4636}, "relationships": {"1": "2cf913651adb3d5b107864c0de10d3792c1e5a80", "2": "b607aef3-1e6e-4289-ac75-dfd558dc34b4", "3": "b04bc038-7f5b-4d65-bcb1-6715c13b8c91"}}, "__type__": "1"}, "b04bc038-7f5b-4d65-bcb1-6715c13b8c91": {"__data__": {"text": "           output_dim=embed_dim,\n        )\n\n        self.head = nn.Linear(embed_dim, num_classes)\n\n        self.apply(self._init_weights)\n\n        self.num_features = embed_dim\n\n    def forward_features(self, x):\n        return self.visual(x)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @property\n    def dtype(self):\n        return self.visual.conv1.weight.dtype\n\n    def encode_image(self, image):\n        return self.visual(image.type(self.dtype))\n\n    def forward(self, image):\n        image_features = self.encode_image(image)\n        logits = self.head(image_features)\n\n        return logits\n", "doc_id": "b04bc038-7f5b-4d65-bcb1-6715c13b8c91", "embedding": null, "doc_hash": "c457a971573ae123e597c876d62b05f129be1c29c2c2d549e843f6c120ebfe18", "extra_info": {"file_path": "TinyViT/models/clip.py", "file_name": "clip.py"}, "node_info": {"start": 4620, "end": 5531}, "relationships": {"1": "2cf913651adb3d5b107864c0de10d3792c1e5a80", "2": "aac6ad79-0a81-44dd-91bb-a33242fab243"}}, "__type__": "1"}, "a36793cc-78b6-4902-817a-51b6d90b84a0": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT Utils\n# Copyright (c) 2022 Microsoft\n# --------------------------------------------------------\n\nimport torch\nfrom torch import nn\n\n\nclass RemapLayer(nn.Module):\n    def __init__(self, fname):\n        super().__init__()\n        with open(fname) as fin:\n            self.mapping = torch.Tensor(\n                list(map(int, fin.readlines()))).to(torch.long)\n\n    def forward(self, x):\n        '''\n        x: [batch_size, class]\n        '''\n        B = len(x)\n        dummy_cls = x.new_zeros((B, 1))\n        expand_x = torch.cat([x, dummy_cls], dim=1)\n        return expand_x[:, self.mapping]\n", "doc_id": "a36793cc-78b6-4902-817a-51b6d90b84a0", "embedding": null, "doc_hash": "fd5476451b94d7a24abf2e9f3c649d9164c57eca359746eaff7c44ccb750be52", "extra_info": {"file_path": "TinyViT/models/remap_layer.py", "file_name": "remap_layer.py"}, "node_info": {"start": 0, "end": 661}, "relationships": {"1": "e1863d3d8cc4a4f6e4a3e679efb1150554856a4d"}}, "__type__": "1"}, "9758ed6a-8fbf-464e-8e9d-b709dbe4f3af": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT Model Architecture\n# Copyright (c) 2022 Microsoft\n# Adapted from LeViT and Swin Transformer\n#   LeViT: (https://github.com/facebookresearch/levit)\n#   Swin: (https://github.com/microsoft/swin-transformer)\n# Build the TinyViT Model\n# --------------------------------------------------------\n\nimport itertools\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath as TimmDropPath,\\\n    to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\nfrom timm.models.helpers import build_model_with_cfg\nfrom typing import Tuple\n\n\nclass Conv2d_BN(torch.nn.Sequential):\n    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n                 groups=1, bn_weight_init=1):\n        super().__init__()\n        self.add_module('c', torch.nn.Conv2d(\n            a, b, ks, stride, pad, dilation, groups, bias=False))\n        bn = torch.nn.BatchNorm2d(b)\n        torch.nn.init.constant_(bn.weight, bn_weight_init)\n        torch.nn.init.constant_(bn.bias, 0)\n        self.add_module('bn', bn)\n\n    @torch.no_grad()\n    def fuse(self):\n        c, bn = self._modules.values()\n        w = bn.weight / (bn.running_var + bn.eps)**0.5\n        w = c.weight * w[:, None, None, None]\n        b = bn.bias - bn.running_mean * bn.weight / \\\n            (bn.running_var + bn.eps)**0.5\n        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m\n\n\nclass", "doc_id": "9758ed6a-8fbf-464e-8e9d-b709dbe4f3af", "embedding": null, "doc_hash": "525045969e10b4b95f944ca99d68085eab3e27b5fde2b54001de541ea0acb294", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 0, "end": 1713}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "3": "b5bb56ff-f993-4449-ac09-6ece198678fc"}}, "__type__": "1"}, "b5bb56ff-f993-4449-ac09-6ece198678fc": {"__data__": {"text": "groups=self.c.groups)\n        m.weight.data.copy_(w)\n        m.bias.data.copy_(b)\n        return m\n\n\nclass DropPath(TimmDropPath):\n    def __init__(self, drop_prob=None):\n        super().__init__(drop_prob=drop_prob)\n        self.drop_prob = drop_prob\n\n    def __repr__(self):\n        msg = super().__repr__()\n        msg += f'(drop_prob={self.drop_prob})'\n        return msg\n\n\nclass PatchEmbed(nn.Module):\n    def __init__(self, in_chans, embed_dim, resolution, activation):\n        super().__init__()\n        img_size: Tuple[int, int] = to_2tuple(resolution)\n        self.patches_resolution = (img_size[0] // 4, img_size[1] // 4)\n        self.num_patches = self.patches_resolution[0] * \\\n            self.patches_resolution[1]\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        n = embed_dim\n        self.seq = nn.Sequential(\n            Conv2d_BN(in_chans, n // 2, 3, 2, 1),\n            activation(),\n            Conv2d_BN(n // 2, n, 3, 2, 1),\n        )\n\n    def forward(self, x):\n        return self.seq(x)\n\n\nclass MBConv(nn.Module):\n    def __init__(self, in_chans, out_chans, expand_ratio,\n                 activation, drop_path):\n        super().__init__()\n        self.in_chans = in_chans\n        self.hidden_chans = int(in_chans * expand_ratio)\n        self.out_chans = out_chans\n\n        self.conv1 = Conv2d_BN(in_chans, self.hidden_chans, ks=1)\n        self.act1 = activation()\n\n        self.conv2 =", "doc_id": "b5bb56ff-f993-4449-ac09-6ece198678fc", "embedding": null, "doc_hash": "17edc50a29657174aaa8c6a8b9d86b989279e41937b4873fb3f90585e91b7827", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 1633, "end": 3068}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "9758ed6a-8fbf-464e-8e9d-b709dbe4f3af", "3": "d189df59-6ef4-4403-92ac-7010119eb443"}}, "__type__": "1"}, "d189df59-6ef4-4403-92ac-7010119eb443": {"__data__": {"text": "       self.conv1 = Conv2d_BN(in_chans, self.hidden_chans, ks=1)\n        self.act1 = activation()\n\n        self.conv2 = Conv2d_BN(self.hidden_chans, self.hidden_chans,\n                               ks=3, stride=1, pad=1, groups=self.hidden_chans)\n        self.act2 = activation()\n\n        self.conv3 = Conv2d_BN(\n            self.hidden_chans, out_chans, ks=1, bn_weight_init=0.0)\n        self.act3 = activation()\n\n        self.drop_path = DropPath(\n            drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        shortcut = x\n\n        x = self.conv1(x)\n        x = self.act1(x)\n\n        x = self.conv2(x)\n        x = self.act2(x)\n\n        x = self.conv3(x)\n\n        x = self.drop_path(x)\n\n        x += shortcut\n        x = self.act3(x)\n\n        return x\n\n\nclass PatchMerging(nn.Module):\n    def __init__(self, input_resolution, dim, out_dim, activation):\n        super().__init__()\n\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.out_dim = out_dim\n        self.act = activation()\n        self.conv1 = Conv2d_BN(dim, out_dim, 1, 1, 0)\n        self.conv2 = Conv2d_BN(out_dim, out_dim, 3, 2, 1, groups=out_dim)\n        self.conv3 = Conv2d_BN(out_dim, out_dim, 1, 1, 0)\n\n    def forward(self, x):\n        if x.ndim == 3:\n            H, W = self.input_resolution\n            B = len(x)\n            # (B, C, H,", "doc_id": "d189df59-6ef4-4403-92ac-7010119eb443", "embedding": null, "doc_hash": "e35361e5f9fd90556dc0a5ececf8f3d2ca6eb4f92fe14e89a53453dab0c092a5", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 3061, "end": 4436}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "b5bb56ff-f993-4449-ac09-6ece198678fc", "3": "a076bffd-ea9e-4d88-819b-36fbe8bfad04"}}, "__type__": "1"}, "a076bffd-ea9e-4d88-819b-36fbe8bfad04": {"__data__": {"text": "== 3:\n            H, W = self.input_resolution\n            B = len(x)\n            # (B, C, H, W)\n            x = x.view(B, H, W, -1).permute(0, 3, 1, 2)\n\n        x = self.conv1(x)\n        x = self.act(x)\n        x = self.conv2(x)\n        x = self.act(x)\n        x = self.conv3(x)\n\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\n\nclass ConvLayer(nn.Module):\n    def __init__(self, dim, input_resolution, depth,\n                 activation,\n                 drop_path=0., downsample=None, use_checkpoint=False,\n                 out_dim=None,\n                 conv_expand_ratio=4.,\n                 ):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            MBConv(dim, dim, conv_expand_ratio, activation,\n                   drop_path[i] if isinstance(drop_path, list) else drop_path,\n                   )\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(\n                input_resolution, dim=dim, out_dim=out_dim, activation=activation)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if", "doc_id": "a076bffd-ea9e-4d88-819b-36fbe8bfad04", "embedding": null, "doc_hash": "4bc76d72af6404b1edce8ed6744217e08d1e0a6adc3a34b911bc4153814f74c2", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 4478, "end": 5832}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "d189df59-6ef4-4403-92ac-7010119eb443", "3": "de339d77-5d63-4e26-8d56-10c13ffa5d56"}}, "__type__": "1"}, "de339d77-5d63-4e26-8d56-10c13ffa5d56": {"__data__": {"text": "    else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None,\n                 out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.norm = nn.LayerNorm(in_features)\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.act = act_layer()\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.norm(x)\n\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(torch.nn.Module):\n    def __init__(self, dim, key_dim, num_heads=8,\n                 attn_ratio=4,\n                 resolution=(14, 14),\n                 ):\n        super().__init__()\n        # (h, w)\n        assert isinstance(resolution, tuple) and len(resolution) == 2\n        self.num_heads = num_heads\n        self.scale = key_dim ** -0.5\n        self.key_dim =", "doc_id": "de339d77-5d63-4e26-8d56-10c13ffa5d56", "embedding": null, "doc_hash": "6ceeb6f28dc4fdbd2736f0684e9721fe3de903ace8e7e5dadefb6de5b1679cdc", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 5810, "end": 7227}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "a076bffd-ea9e-4d88-819b-36fbe8bfad04", "3": "b4c1445b-a8cb-45ad-89b6-83450b257d79"}}, "__type__": "1"}, "b4c1445b-a8cb-45ad-89b6-83450b257d79": {"__data__": {"text": "tuple) and len(resolution) == 2\n        self.num_heads = num_heads\n        self.scale = key_dim ** -0.5\n        self.key_dim = key_dim\n        self.nh_kd = nh_kd = key_dim * num_heads\n        self.d = int(attn_ratio * key_dim)\n        self.dh = int(attn_ratio * key_dim) * num_heads\n        self.attn_ratio = attn_ratio\n        h = self.dh + nh_kd * 2\n\n        self.norm = nn.LayerNorm(dim)\n        self.qkv = nn.Linear(dim, h)\n        self.proj = nn.Linear(self.dh, dim)\n\n        points = list(itertools.product(\n            range(resolution[0]), range(resolution[1])))\n        N = len(points)\n        attention_offsets = {}\n        idxs = []\n        for p1 in points:\n            for p2 in points:\n                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n                if offset not in attention_offsets:\n                    attention_offsets[offset] = len(attention_offsets)\n                idxs.append(attention_offsets[offset])\n        self.attention_biases = torch.nn.Parameter(\n            torch.zeros(num_heads, len(attention_offsets)))\n        self.register_buffer('attention_bias_idxs',\n                             torch.LongTensor(idxs).view(N, N),\n                             persistent=False)\n\n    @torch.no_grad()\n    def train(self, mode=True):\n        super().train(mode)\n        if mode and hasattr(self, 'ab'):\n            del self.ab\n", "doc_id": "b4c1445b-a8cb-45ad-89b6-83450b257d79", "embedding": null, "doc_hash": "cb8ac6a23193cb2fdbb4851f20164e9a880011b2f170f06338b2c14a7d558826", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 7208, "end": 8574}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "de339d77-5d63-4e26-8d56-10c13ffa5d56", "3": "f0aacc91-81d9-4a58-a944-251ea27fe8c0"}}, "__type__": "1"}, "f0aacc91-81d9-4a58-a944-251ea27fe8c0": {"__data__": {"text": "   def train(self, mode=True):\n        super().train(mode)\n        if mode and hasattr(self, 'ab'):\n            del self.ab\n        else:\n            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n\n    def forward(self, x):  # x (B,N,C)\n        B, N, _ = x.shape\n\n        # Normalization\n        x = self.norm(x)\n\n        qkv = self.qkv(x)\n        # (B, N, num_heads, d)\n        q, k, v = qkv.view(B, N, self.num_heads, -\n                           1).split([self.key_dim, self.key_dim, self.d], dim=3)\n        # (B, num_heads, N, d)\n        q = q.permute(0, 2, 1, 3)\n        k = k.permute(0, 2, 1, 3)\n        v = v.permute(0, 2, 1, 3)\n\n        attn = (\n            (q @ k.transpose(-2, -1)) * self.scale\n            +\n            (self.attention_biases[:, self.attention_bias_idxs]\n             if self.training else self.ab)\n        )\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, self.dh)\n        x = self.proj(x)\n        return x\n\n\nclass TinyViTBlock(nn.Module):\n    r\"\"\" TinyViT Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int, int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout", "doc_id": "f0aacc91-81d9-4a58-a944-251ea27fe8c0", "embedding": null, "doc_hash": "f9a0b9bdae1bf5dcca4bc5c0b8f176b1240cc4ee3c0c51eb941887a376c22001", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 8580, "end": 9946}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "b4c1445b-a8cb-45ad-89b6-83450b257d79", "3": "1fe18a65-ef0b-49ec-96d6-0e3b1ab24665"}}, "__type__": "1"}, "1fe18a65-ef0b-49ec-96d6-0e3b1ab24665": {"__data__": {"text": "heads.\n        window_size (int): Window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        local_conv_size (int): the kernel size of the convolution between\n                               Attention and MLP. Default: 3\n        activation: the activation function. Default: nn.GELU\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7,\n                 mlp_ratio=4., drop=0., drop_path=0.,\n                 local_conv_size=3,\n                 activation=nn.GELU,\n                 ):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        assert window_size > 0, 'window_size must be greater than 0'\n        self.window_size = window_size\n        self.mlp_ratio = mlp_ratio\n\n        self.drop_path = DropPath(\n            drop_path) if drop_path > 0. else nn.Identity()\n\n        assert dim % num_heads == 0, 'dim must be divisible by num_heads'\n        head_dim = dim // num_heads\n\n        window_resolution = (window_size, window_size)\n        self.attn = Attention(dim, head_dim, num_heads,\n                              attn_ratio=1, resolution=window_resolution)\n\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        mlp_activation = activation\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n                       act_layer=mlp_activation, drop=drop)\n\n      ", "doc_id": "1fe18a65-ef0b-49ec-96d6-0e3b1ab24665", "embedding": null, "doc_hash": "33128567e3eaec018bc6d39276b4b2c8ebf1f9883d8e4172932b7300855ecb70", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 9917, "end": 11482}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "f0aacc91-81d9-4a58-a944-251ea27fe8c0", "3": "c8e93190-0346-4f8b-be86-e66fcefb4092"}}, "__type__": "1"}, "c8e93190-0346-4f8b-be86-e66fcefb4092": {"__data__": {"text": "= Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n                       act_layer=mlp_activation, drop=drop)\n\n        pad = local_conv_size // 2\n        self.local_conv = Conv2d_BN(\n            dim, dim, ks=local_conv_size, stride=1, pad=pad, groups=dim)\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        res_x = x\n        if H == self.window_size and W == self.window_size:\n            x = self.attn(x)\n        else:\n            x = x.view(B, H, W, C)\n            pad_b = (self.window_size - H %\n                     self.window_size) % self.window_size\n            pad_r = (self.window_size - W %\n                     self.window_size) % self.window_size\n            padding = pad_b > 0 or pad_r > 0\n\n            if padding:\n                x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))\n\n            pH, pW = H + pad_b, W + pad_r\n            nH = pH // self.window_size\n            nW = pW // self.window_size\n            # window partition\n            x = x.view(B, nH, self.window_size, nW, self.window_size, C).transpose(2, 3).reshape(\n                B * nH * nW, self.window_size * self.window_size, C\n            )\n            x = self.attn(x)\n            # window reverse\n            x = x.view(B,", "doc_id": "c8e93190-0346-4f8b-be86-e66fcefb4092", "embedding": null, "doc_hash": "54f6e25f451200dfda5e34750dda092250dd8e00719a0750bafdadaa498a6b95", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 11509, "end": 12822}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "1fe18a65-ef0b-49ec-96d6-0e3b1ab24665", "3": "b03f73d1-85bd-4275-b1d9-375923cd615d"}}, "__type__": "1"}, "b03f73d1-85bd-4275-b1d9-375923cd615d": {"__data__": {"text": "       )\n            x = self.attn(x)\n            # window reverse\n            x = x.view(B, nH, nW, self.window_size, self.window_size,\n                       C).transpose(2, 3).reshape(B, pH, pW, C)\n\n            if padding:\n                x = x[:, :H, :W].contiguous()\n\n            x = x.view(B, L, C)\n\n        x = res_x + self.drop_path(x)\n\n        x = x.transpose(1, 2).reshape(B, C, H, W)\n        x = self.local_conv(x)\n        x = x.view(B, C, L).transpose(1, 2)\n\n        x = x + self.drop_path(self.mlp(x))\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n               f\"window_size={self.window_size}, mlp_ratio={self.mlp_ratio}\"\n\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic TinyViT layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        drop (float, optional): Dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        local_conv_size: the kernel size of the depthwise convolution between attention and MLP. Default: 3\n        activation: the activation function.", "doc_id": "b03f73d1-85bd-4275-b1d9-375923cd615d", "embedding": null, "doc_hash": "12d1cb8d3ec1f3e813a7462abafe8a2238a8109f517f75c835efa3f6e9fdc91c", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 12870, "end": 14511}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "c8e93190-0346-4f8b-be86-e66fcefb4092", "3": "129502a6-4bcf-430f-b361-95eb9a97ddde"}}, "__type__": "1"}, "129502a6-4bcf-430f-b361-95eb9a97ddde": {"__data__": {"text": "(bool): Whether to use checkpointing to save memory. Default: False.\n        local_conv_size: the kernel size of the depthwise convolution between attention and MLP. Default: 3\n        activation: the activation function. Default: nn.GELU\n        out_dim: the output dimension of the layer. Default: dim\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., drop=0.,\n                 drop_path=0., downsample=None, use_checkpoint=False,\n                 local_conv_size=3,\n                 activation=nn.GELU,\n                 out_dim=None,\n                 ):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            TinyViTBlock(dim=dim, input_resolution=input_resolution,\n                         num_heads=num_heads, window_size=window_size,\n                         mlp_ratio=mlp_ratio,\n                         drop=drop,\n                         drop_path=drop_path[i] if isinstance(\n                             drop_path, list) else drop_path,\n                         local_conv_size=local_conv_size,\n                         activation=activation,\n                         )\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not", "doc_id": "129502a6-4bcf-430f-b361-95eb9a97ddde", "embedding": null, "doc_hash": "de243ad6c28a1ec258d01cdcb436df5fe6afc75aa2523e25238b6e12dc6b5f91", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 14375, "end": 15808}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "b03f73d1-85bd-4275-b1d9-375923cd615d", "3": "47718423-3d3c-4b6f-98e8-60f5eafd4a5c"}}, "__type__": "1"}, "47718423-3d3c-4b6f-98e8-60f5eafd4a5c": {"__data__": {"text": "                 )\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(\n                input_resolution, dim=dim, out_dim=out_dim, activation=activation)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n\n\nclass TinyViT(nn.Module):\n    def __init__(self, img_size=224, in_chans=3, num_classes=1000,\n                 embed_dims=[96, 192, 384, 768], depths=[2, 2, 6, 2],\n                 num_heads=[3, 6, 12, 24],\n                 window_sizes=[7, 7, 14, 7],\n                 mlp_ratio=4.,\n                 drop_rate=0.,\n                 drop_path_rate=0.1,\n                 use_checkpoint=False,\n                 mbconv_expand_ratio=4.0,\n                 local_conv_size=3,\n                 layer_lr_decay=1.0,\n                 ):\n        super().__init__()\n\n        self.num_classes = num_classes\n      ", "doc_id": "47718423-3d3c-4b6f-98e8-60f5eafd4a5c", "embedding": null, "doc_hash": "0baa70dea62225df631a61e30337ad8d693262207361f92defb42c13e9fa1c21", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 15929, "end": 17234}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "129502a6-4bcf-430f-b361-95eb9a97ddde", "3": "1aa73f39-003a-4c4f-8886-e782ca38de63"}}, "__type__": "1"}, "1aa73f39-003a-4c4f-8886-e782ca38de63": {"__data__": {"text": "                ):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.depths = depths\n        self.num_layers = len(depths)\n        self.mlp_ratio = mlp_ratio\n\n        activation = nn.GELU\n\n        self.patch_embed = PatchEmbed(in_chans=in_chans,\n                                      embed_dim=embed_dims[0],\n                                      resolution=img_size,\n                                      activation=activation)\n\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate,\n                                                sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            kwargs = dict(dim=embed_dims[i_layer],\n                          input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                            patches_resolution[1] // (2 ** i_layer)),\n                          depth=depths[i_layer],\n                          drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                         ", "doc_id": "1aa73f39-003a-4c4f-8886-e782ca38de63", "embedding": null, "doc_hash": "8b643f118f8210c1846a2e38dfb27f1a6d6f973b9c143f9c4454720c3cfd7a7a", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 17243, "end": 18504}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "47718423-3d3c-4b6f-98e8-60f5eafd4a5c", "3": "b2956a09-0be4-4761-b21c-aaf337e213c9"}}, "__type__": "1"}, "b2956a09-0be4-4761-b21c-aaf337e213c9": {"__data__": {"text": "       drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                          downsample=PatchMerging if (\n                              i_layer < self.num_layers - 1) else None,\n                          use_checkpoint=use_checkpoint,\n                          out_dim=embed_dims[min(\n                              i_layer + 1, len(embed_dims) - 1)],\n                          activation=activation,\n                          )\n            if i_layer == 0:\n                layer = ConvLayer(\n                    conv_expand_ratio=mbconv_expand_ratio,\n                    **kwargs,\n                )\n            else:\n                layer = BasicLayer(\n                    num_heads=num_heads[i_layer],\n                    window_size=window_sizes[i_layer],\n                    mlp_ratio=self.mlp_ratio,\n                    drop=drop_rate,\n                    local_conv_size=local_conv_size,\n                    **kwargs)\n            self.layers.append(layer)\n\n        # Classifier head\n        self.norm_head = nn.LayerNorm(embed_dims[-1])\n        self.head = nn.Linear(\n            embed_dims[-1], num_classes) if num_classes > 0 else", "doc_id": "b2956a09-0be4-4761-b21c-aaf337e213c9", "embedding": null, "doc_hash": "68e608a7c764c0fdbb8da6d51535e7913d120ef2349e0e6971e7b23b767a72c1", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 18495, "end": 19652}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "1aa73f39-003a-4c4f-8886-e782ca38de63", "3": "d8b78180-b196-4360-b4df-703c76c3ffc6"}}, "__type__": "1"}, "d8b78180-b196-4360-b4df-703c76c3ffc6": {"__data__": {"text": "= nn.LayerNorm(embed_dims[-1])\n        self.head = nn.Linear(\n            embed_dims[-1], num_classes) if num_classes > 0 else torch.nn.Identity()\n\n        # init weights\n        self.apply(self._init_weights)\n        self.set_layer_lr_decay(layer_lr_decay)\n\n    def set_layer_lr_decay(self, layer_lr_decay):\n        decay_rate = layer_lr_decay\n\n        # layers -> blocks (depth)\n        depth = sum(self.depths)\n        lr_scales = [decay_rate ** (depth - i - 1) for i in range(depth)]\n        print(\"LR SCALES:\", lr_scales)\n\n        def _set_lr_scale(m, scale):\n            for p in m.parameters():\n                p.lr_scale = scale\n\n        self.patch_embed.apply(lambda x: _set_lr_scale(x, lr_scales[0]))\n        i = 0\n        for layer in self.layers:\n            for block in layer.blocks:\n                block.apply(lambda x: _set_lr_scale(x, lr_scales[i]))\n                i += 1\n            if layer.downsample is not None:\n                layer.downsample.apply(\n                    lambda x: _set_lr_scale(x, lr_scales[i - 1]))\n        assert i == depth\n        for m in [self.norm_head, self.head]:\n            m.apply(lambda x: _set_lr_scale(x, lr_scales[-1]))\n\n        for k, p in self.named_parameters():\n            p.param_name = k\n\n        def _check_lr_scale(m):\n            for p in m.parameters():\n                assert hasattr(p, 'lr_scale'), p.param_name\n\n       ", "doc_id": "d8b78180-b196-4360-b4df-703c76c3ffc6", "embedding": null, "doc_hash": "e78be04f8e1ceb853b6fbdab97f6c3663a01bcf3aa092dddede6a4bbe52df52f", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 19618, "end": 21008}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "b2956a09-0be4-4761-b21c-aaf337e213c9", "3": "79953aa7-3d8d-494b-bd74-9cd184143299"}}, "__type__": "1"}, "79953aa7-3d8d-494b-bd74-9cd184143299": {"__data__": {"text": "           for p in m.parameters():\n                assert hasattr(p, 'lr_scale'), p.param_name\n\n        self.apply(_check_lr_scale)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay_keywords(self):\n        return {'attention_biases'}\n\n    def forward_features(self, x):\n        # x: (N, C, H, W)\n        x = self.patch_embed(x)\n\n        x = self.layers[0](x)\n        start_i = 1\n\n        for i in range(start_i, len(self.layers)):\n            layer = self.layers[i]\n            x = layer(x)\n\n        x = x.mean(1)\n\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.norm_head(x)\n        x = self.head(x)\n        return x\n\n\n_checkpoint_url_format = \\\n    'https://github.com/wkcn/TinyViT-model-zoo/releases/download/checkpoints/{}.pth'\n\n\ndef _create_tiny_vit(variant, pretrained=False, **kwargs):\n    # pretrained_type: 22kto1k_distill, 1k, 22k_distill\n    pretrained_type = kwargs.pop('pretrained_type', '22kto1k_distill')\n    assert pretrained_type in ['22kto1k_distill', '1k', '22k_distill'], \\\n        'pretrained_type should be one of", "doc_id": "79953aa7-3d8d-494b-bd74-9cd184143299", "embedding": null, "doc_hash": "553df516a89c96dc20b04391823648d6776fe5188820b439ab5e35c2edc2797c", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 21042, "end": 22477}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "d8b78180-b196-4360-b4df-703c76c3ffc6", "3": "86d60fbf-5e24-4fd5-aeec-46ad78509422"}}, "__type__": "1"}, "86d60fbf-5e24-4fd5-aeec-46ad78509422": {"__data__": {"text": "'22kto1k_distill')\n    assert pretrained_type in ['22kto1k_distill', '1k', '22k_distill'], \\\n        'pretrained_type should be one of 22kto1k_distill, 1k, 22k_distill'\n\n    img_size = kwargs.get('img_size', 224)\n    if img_size != 224:\n        pretrained_type = pretrained_type.replace('_', f'_{img_size}_')\n\n    num_classes_pretrained = 21841 if \\\n        pretrained_type  == '22k_distill' else 1000\n\n    variant_without_img_size = '_'.join(variant.split('_')[:-1])\n    cfg = dict(\n        url=_checkpoint_url_format.format(\n            f'{variant_without_img_size}_{pretrained_type}'),\n        num_classes=num_classes_pretrained,\n        classifier='head',\n    )\n\n    def _pretrained_filter_fn(state_dict):\n        state_dict = state_dict['model']\n        # filter out attention_bias_idxs\n        state_dict = {k: v for k, v in state_dict.items() if \\\n            not k.endswith('attention_bias_idxs')}\n        return state_dict\n\n    return build_model_with_cfg(\n        TinyViT, variant, pretrained,\n        default_cfg=cfg,\n        pretrained_filter_fn=_pretrained_filter_fn,\n        **kwargs)\n\n\n@register_model\ndef tiny_vit_5m_224(pretrained=False, **kwargs):\n    model_kwargs = dict(\n        embed_dims=[64, 128, 160, 320],\n        depths=[2, 2, 6, 2],\n        num_heads=[2, 4, 5, 10],\n        window_sizes=[7, 7, 14, 7],\n        drop_path_rate=0.0,\n    )\n    model_kwargs.update(kwargs)\n    return _create_tiny_vit('tiny_vit_5m_224', pretrained, **model_kwargs)\n\n\n@register_model\ndef tiny_vit_11m_224(pretrained=False, **kwargs):\n    model_kwargs = dict(\n    ", "doc_id": "86d60fbf-5e24-4fd5-aeec-46ad78509422", "embedding": null, "doc_hash": "53fb991a8dcd8a9b75c7afa1147c363cae355948b305e67aaa946f73379424f6", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 22430, "end": 23997}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "79953aa7-3d8d-494b-bd74-9cd184143299", "3": "ed83c936-f623-46f6-a8f6-e40741c2db37"}}, "__type__": "1"}, "ed83c936-f623-46f6-a8f6-e40741c2db37": {"__data__": {"text": "pretrained, **model_kwargs)\n\n\n@register_model\ndef tiny_vit_11m_224(pretrained=False, **kwargs):\n    model_kwargs = dict(\n        embed_dims=[64, 128, 256, 448],\n        depths=[2, 2, 6, 2],\n        num_heads=[2, 4, 8, 14],\n        window_sizes=[7, 7, 14, 7],\n        drop_path_rate=0.1,\n    )\n    model_kwargs.update(kwargs)\n    return _create_tiny_vit('tiny_vit_11m_224', pretrained, **model_kwargs)\n\n\n@register_model\ndef tiny_vit_21m_224(pretrained=False, **kwargs):\n    model_kwargs = dict(\n        embed_dims=[96, 192, 384, 576],\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 18],\n        window_sizes=[7, 7, 14, 7],\n        drop_path_rate=0.2,\n    )\n    model_kwargs.update(kwargs)\n    return _create_tiny_vit('tiny_vit_21m_224', pretrained, **model_kwargs)\n\n\n@register_model\ndef tiny_vit_21m_384(pretrained=False, **kwargs):\n    model_kwargs = dict(\n        img_size=384,\n        embed_dims=[96, 192, 384, 576],\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 18],\n        window_sizes=[12, 12, 24, 12],\n        drop_path_rate=0.1,\n    )\n    model_kwargs.update(kwargs)\n    return _create_tiny_vit('tiny_vit_21m_384', pretrained, **model_kwargs)\n\n\n@register_model\ndef tiny_vit_21m_512(pretrained=False, **kwargs):\n    model_kwargs = dict(\n        img_size=512,\n        embed_dims=[96, 192, 384, 576],\n        depths=[2, 2, 6, 2],\n        num_heads=[3, 6, 12, 18],\n        window_sizes=[16, 16, 32, 16],\n        drop_path_rate=0.1,\n", "doc_id": "ed83c936-f623-46f6-a8f6-e40741c2db37", "embedding": null, "doc_hash": "6288975b6ee4bd1c841ff9fb60ceb433832dad36cdf54a89f44625ecfce0db96", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 23998, "end": 25457}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "86d60fbf-5e24-4fd5-aeec-46ad78509422", "3": "1de98e46-838d-464b-89a6-5ef514bf5db5"}}, "__type__": "1"}, "1de98e46-838d-464b-89a6-5ef514bf5db5": {"__data__": {"text": "6, 2],\n        num_heads=[3, 6, 12, 18],\n        window_sizes=[16, 16, 32, 16],\n        drop_path_rate=0.1,\n    )\n    model_kwargs.update(kwargs)\n    return _create_tiny_vit('tiny_vit_21m_512', pretrained, **model_kwargs)\n", "doc_id": "1de98e46-838d-464b-89a6-5ef514bf5db5", "embedding": null, "doc_hash": "aa8667d9449951e875bfb60fb773587ac80976a61269f532f1bbf727103cd466", "extra_info": {"file_path": "TinyViT/models/tiny_vit.py", "file_name": "tiny_vit.py"}, "node_info": {"start": 25461, "end": 25683}, "relationships": {"1": "d88fbdc430c5856c82c8c43ca55bdf3cad9ee9de", "2": "ed83c936-f623-46f6-a8f6-e40741c2db37"}}, "__type__": "1"}, "a1b44000-5750-4ce9-8ffa-4be6eed05285": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT Utils\n# Copyright (c) 2022 Microsoft\n# --------------------------------------------------------\n\nimport torch\nimport torch.distributed as dist\n\n\ndef get_dist_backend():\n    if not dist.is_available():\n        return None\n    if not dist.is_initialized():\n        return None\n    return dist.get_backend()\n\n\nclass AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self._use_gpu = get_dist_backend() == 'nccl'\n        self.reset()\n\n    def reset(self):\n        # local\n        self._val = 0\n        self._sum = 0\n        self._count = 0\n        # global\n        self._history_avg = 0\n        self._history_count = 0\n        self._avg = None\n\n    def update(self, val, n=1):\n        self._val = val\n        self._sum += val * n\n        self._count += n\n        self._avg = None\n\n    @property\n    def val(self):\n        return self._val\n\n    @property\n    def count(self):\n        return self._count + self._history_count\n\n    @property\n    def avg(self):\n        if self._avg is None:\n            # compute avg\n            r = self._history_count / max(1, self._history_count + self._count)\n            _avg = self._sum / max(1, self._count)\n            self._avg = r * self._history_avg + (1.0 - r) * _avg\n        return self._avg\n\n    def sync(self):\n        buf = torch.tensor([self._sum, self._count],\n                           dtype=torch.float32)\n        if self._use_gpu:\n            buf = buf.cuda()\n        dist.all_reduce(buf, op=dist.ReduceOp.SUM)\n        _sum, _count = buf.tolist()\n        _avg = _sum", "doc_id": "a1b44000-5750-4ce9-8ffa-4be6eed05285", "embedding": null, "doc_hash": "68edc1d878c3c60f3dba01417ec6467be60d15500d66d3d63d6a3dbcf0ae4d8e", "extra_info": {"file_path": "TinyViT/my_meter.py", "file_name": "my_meter.py"}, "node_info": {"start": 0, "end": 1639}, "relationships": {"1": "6adc3ba5024f48d99774adf85a588b43ffa72bb4", "3": "408a2bf2-00bb-4002-a978-6997aeb4f3f9"}}, "__type__": "1"}, "408a2bf2-00bb-4002-a978-6997aeb4f3f9": {"__data__": {"text": "       dist.all_reduce(buf, op=dist.ReduceOp.SUM)\n        _sum, _count = buf.tolist()\n        _avg = _sum / max(1, _count)\n        r = self._history_count / max(1, self._history_count + _count)\n\n        self._history_avg = r * self._history_avg + (1.0 - r) * _avg\n        self._history_count += _count\n\n        self._sum = 0\n        self._count = 0\n\n        self._avg = None\n", "doc_id": "408a2bf2-00bb-4002-a978-6997aeb4f3f9", "embedding": null, "doc_hash": "f0ac0106248c87ffb3a665591b6dcee42549263f4a967eca439d20292c91afaa", "extra_info": {"file_path": "TinyViT/my_meter.py", "file_name": "my_meter.py"}, "node_info": {"start": 1534, "end": 1909}, "relationships": {"1": "6adc3ba5024f48d99774adf85a588b43ffa72bb4", "2": "a1b44000-5750-4ce9-8ffa-4be6eed05285"}}, "__type__": "1"}, "340cb617-a242-4eda-b8ad-d601eb6bafe6": {"__data__": {"text": "# --------------------------------------------------------\n# Optimizer\n# Copyright (c) 2022 Microsoft\n# Based on the code: Swin Transformer\n#   (https://github.com/microsoft/swin-transformer)\n# --------------------------------------------------------\n\nfrom torch import optim as optim\n# Modified for TinyViT\nfrom tinyvit_utils import divide_param_groups_by_lr_scale\n\n\ndef build_optimizer(config, model):\n    \"\"\"\n    Build optimizer, set weight decay of normalization to 0 by default.\n    \"\"\"\n    skip = {}\n    skip_keywords = {}\n    if hasattr(model, 'no_weight_decay'):\n        skip = model.no_weight_decay()\n    if hasattr(model, 'no_weight_decay_keywords'):\n        skip_keywords = model.no_weight_decay_keywords()\n    parameters = set_weight_decay(model, skip, skip_keywords)\n\n    # Modified for TinyViT\n    parameters = divide_param_groups_by_lr_scale(parameters)\n\n    opt_lower = config.TRAIN.OPTIMIZER.NAME.lower()\n    optimizer = None\n    if opt_lower == 'sgd':\n        optimizer = optim.SGD(parameters, momentum=config.TRAIN.OPTIMIZER.MOMENTUM, nesterov=True,\n                              lr=config.TRAIN.BASE_LR, weight_decay=config.TRAIN.WEIGHT_DECAY)\n    elif opt_lower == 'adamw':\n        optimizer = optim.AdamW(parameters, eps=config.TRAIN.OPTIMIZER.EPS, betas=config.TRAIN.OPTIMIZER.BETAS,\n                                lr=config.TRAIN.BASE_LR, weight_decay=config.TRAIN.WEIGHT_DECAY)\n\n    return optimizer\n\n\ndef set_weight_decay(model, skip_list=(), skip_keywords=()):\n    has_decay = []\n    no_decay = []\n\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue  # frozen weights\n        if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n                check_keywords_in_name(name, skip_keywords):\n", "doc_id": "340cb617-a242-4eda-b8ad-d601eb6bafe6", "embedding": null, "doc_hash": "217977a68bca578233683b7c414b87c80c236889406c1ca9094b6059bf2f3ea9", "extra_info": {"file_path": "TinyViT/optimizer.py", "file_name": "optimizer.py"}, "node_info": {"start": 0, "end": 1799}, "relationships": {"1": "f11be944ef8b97431d556374a854175bf1b872ad", "3": "4c45c491-13a9-4005-a6ed-ffa4f2018eac"}}, "__type__": "1"}, "4c45c491-13a9-4005-a6ed-ffa4f2018eac": {"__data__": {"text": " if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n                check_keywords_in_name(name, skip_keywords):\n            no_decay.append(param)\n        else:\n            has_decay.append(param)\n    return [{'params': has_decay},\n            {'params': no_decay, 'weight_decay': 0.}]\n\n\ndef check_keywords_in_name(name, keywords=()):\n    isin = False\n    for keyword in keywords:\n        if keyword in name:\n            isin = True\n    return isin\n", "doc_id": "4c45c491-13a9-4005-a6ed-ffa4f2018eac", "embedding": null, "doc_hash": "e2b9d627ef2cfa8e950b812b59bd56b6b21099fb32a2ba1873ecb4426a80fa79", "extra_info": {"file_path": "TinyViT/optimizer.py", "file_name": "optimizer.py"}, "node_info": {"start": 1658, "end": 2136}, "relationships": {"1": "f11be944ef8b97431d556374a854175bf1b872ad", "2": "340cb617-a242-4eda-b8ad-d601eb6bafe6"}}, "__type__": "1"}, "3968e46e-1efb-4e96-8849-8bf92f3b6c96": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT Save Teacher Logits\n# Copyright (c) 2022 Microsoft\n# Based on the code: Swin Transformer\n#   (https://github.com/microsoft/swin-transformer)\n# Save teacher logits\n# --------------------------------------------------------\n\nimport os\nimport time\nimport random\nimport argparse\nimport datetime\nfrom collections import defaultdict\nimport numpy as np\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\n\nfrom timm.utils import accuracy\nfrom my_meter import AverageMeter\n\nfrom config import get_config\nfrom models import build_model\nfrom data import build_loader\nfrom logger import create_logger\nfrom utils import load_checkpoint, NativeScalerWithGradNormCount, add_common_args\n\nfrom models.remap_layer import RemapLayer\nremap_layer_22kto1k = RemapLayer('./imagenet_1kto22k.txt')\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser(\n        'TinyViT saving sparse logits script', add_help=False)\n    add_common_args(parser)\n    parser.add_argument('--check-saved-logits',\n                        action='store_true', help='Check saved logits')\n    parser.add_argument('--skip-eval',\n                        action='store_true', help='Skip evaluation')\n\n    args = parser.parse_args()\n\n    config = get_config(args)\n\n    return args, config\n\n\ndef main(config):\n    dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn = build_loader(\n        config)\n\n    logger.info(f\"Creating model:{config.MODEL.TYPE}/{config.MODEL.NAME}\")\n    model = build_model(config)\n    model.cuda()\n\n    logger.info(str(model))\n\n    model = torch.nn.parallel.DistributedDataParallel(\n        model, device_ids=[config.LOCAL_RANK], broadcast_buffers=False)\n    model_without_ddp = model.module\n\n    n_parameters = sum(p.numel()\n                       for p in model.parameters() if p.requires_grad)\n    logger.info(f\"number of params: {n_parameters}\")\n\n    optimizer = None\n    lr_scheduler = None\n\n    assert config.MODEL.RESUME\n    loss_scaler =", "doc_id": "3968e46e-1efb-4e96-8849-8bf92f3b6c96", "embedding": null, "doc_hash": "1bebe808462446eb386f40be1f06a53c0efb377b64f651809011c68133793666", "extra_info": {"file_path": "TinyViT/save_logits.py", "file_name": "save_logits.py"}, "node_info": {"start": 0, "end": 2043}, "relationships": {"1": "454ab3769cc57ab87c75c448164278d5eea20f68", "3": "6701df11-6331-44f3-bf2f-7755215a1ccb"}}, "__type__": "1"}, "6701df11-6331-44f3-bf2f-7755215a1ccb": {"__data__": {"text": "   logger.info(f\"number of params: {n_parameters}\")\n\n    optimizer = None\n    lr_scheduler = None\n\n    assert config.MODEL.RESUME\n    loss_scaler = NativeScalerWithGradNormCount()\n    load_checkpoint(config, model_without_ddp, optimizer,\n                    lr_scheduler, loss_scaler, logger)\n    if not args.skip_eval and not args.check_saved_logits:\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(\n            f\"Accuracy of the network on the {len(dataset_val)} test images: top-1 acc: {acc1:.1f}%, top-5 acc: {acc5:.1f}%\")\n\n    if args.check_saved_logits:\n        logger.info(\"Start checking logits\")\n    else:\n        logger.info(\"Start saving logits\")\n\n    start_time = time.time()\n    for epoch in range(config.TRAIN.START_EPOCH, config.TRAIN.EPOCHS):\n        dataset_train.set_epoch(epoch)\n        data_loader_train.sampler.set_epoch(epoch)\n\n        if args.check_saved_logits:\n            check_logits_one_epoch(\n                config, model, data_loader_train, epoch, mixup_fn=mixup_fn)\n        else:\n            save_logits_one_epoch(\n                config, model, data_loader_train, epoch, mixup_fn=mixup_fn)\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info('Saving logits time {}'.format(total_time_str))\n\n\n@torch.no_grad()\ndef save_logits_one_epoch(config, model, data_loader, epoch, mixup_fn):\n    model.eval()\n\n    num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    meters = defaultdict(AverageMeter)\n\n    start = time.time()\n    end = time.time()\n    topk =", "doc_id": "6701df11-6331-44f3-bf2f-7755215a1ccb", "embedding": null, "doc_hash": "8dfa262badd22dfce79dcd69129aba89fa30cff21a2ac6f64287813cb0a8b464", "extra_info": {"file_path": "TinyViT/save_logits.py", "file_name": "save_logits.py"}, "node_info": {"start": 1925, "end": 3535}, "relationships": {"1": "454ab3769cc57ab87c75c448164278d5eea20f68", "2": "3968e46e-1efb-4e96-8849-8bf92f3b6c96", "3": "39b3974a-36c7-40f3-a3d6-a959ccf7fae2"}}, "__type__": "1"}, "39b3974a-36c7-40f3-a3d6-a959ccf7fae2": {"__data__": {"text": " num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    meters = defaultdict(AverageMeter)\n\n    start = time.time()\n    end = time.time()\n    topk = config.DISTILL.LOGITS_TOPK\n\n    logits_manager = data_loader.dataset.get_manager()\n\n    for idx, ((samples, targets), (keys, seeds)) in enumerate(data_loader):\n        samples = samples.cuda(non_blocking=True)\n        targets = targets.cuda(non_blocking=True)\n\n        if mixup_fn is not None:\n            samples, targets = mixup_fn(samples, targets, seeds)\n            original_targets = targets.argmax(dim=1)\n        else:\n            original_targets = targets\n\n        with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n            outputs = model(samples)\n\n        acc1, acc5 = accuracy(outputs, original_targets, topk=(1, 5))\n        real_batch_size = len(samples)\n        meters['teacher_acc1'].update(acc1.item(), real_batch_size)\n        meters['teacher_acc5'].update(acc5.item(), real_batch_size)\n\n        # save teacher logits\n        softmax_prob = torch.softmax(outputs, -1)\n\n        torch.cuda.synchronize()\n\n        write_tic = time.time()\n        values, indices = softmax_prob.topk(\n            k=topk, dim=-1, largest=True, sorted=True)\n\n        cpu_device = torch.device('cpu')\n        values = values.detach().to(device=cpu_device, dtype=torch.float16)\n        indices = indices.detach().to(device=cpu_device, dtype=torch.int16)\n\n        seeds = seeds.numpy()\n        values = values.numpy()\n        indices = indices.numpy()\n\n        # check data type\n        assert seeds.dtype ==", "doc_id": "39b3974a-36c7-40f3-a3d6-a959ccf7fae2", "embedding": null, "doc_hash": "d4bc2353f9e17ace22accbc31c26a76863ec667982cd9dbf7a549c3b0518ee53", "extra_info": {"file_path": "TinyViT/save_logits.py", "file_name": "save_logits.py"}, "node_info": {"start": 3529, "end": 5097}, "relationships": {"1": "454ab3769cc57ab87c75c448164278d5eea20f68", "2": "6701df11-6331-44f3-bf2f-7755215a1ccb", "3": "2cb85a72-9586-47fa-a1c9-2413e955fb86"}}, "__type__": "1"}, "2cb85a72-9586-47fa-a1c9-2413e955fb86": {"__data__": {"text": "       values = values.numpy()\n        indices = indices.numpy()\n\n        # check data type\n        assert seeds.dtype == np.int32, seeds.dtype\n        assert indices.dtype == np.int16, indices.dtype\n        assert values.dtype == np.float16, values.dtype\n\n        for key, seed, indice, value in zip(keys, seeds, indices, values):\n            bstr = seed.tobytes() + indice.tobytes() + value.tobytes()\n            logits_manager.write(key, bstr)\n        meters['write_time'].update(time.time() - write_tic)\n\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            etas = batch_time.avg * (num_steps - idx)\n            extra_meters_str = ''\n            for k, v in meters.items():\n                extra_meters_str += f'{k} {v.val:.4f} ({v.avg:.4f})\\t'\n            logger.info(\n                f'Save: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]\\t'\n                f'eta {datetime.timedelta(seconds=int(etas))}\\t'\n                f'time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'{extra_meters_str}'\n                f'mem {memory_used:.0f}MB')\n\n    epoch_time = time.time() - start\n    logger.info(\n        f\"EPOCH {epoch} save logits takes {datetime.timedelta(seconds=int(epoch_time))}\")\n\n\n@torch.no_grad()\ndef", "doc_id": "2cb85a72-9586-47fa-a1c9-2413e955fb86", "embedding": null, "doc_hash": "cd19eded39882f1dde82ebcf8009991196dc56641d18d90ad95dfc7539f8a370", "extra_info": {"file_path": "TinyViT/save_logits.py", "file_name": "save_logits.py"}, "node_info": {"start": 5143, "end": 6531}, "relationships": {"1": "454ab3769cc57ab87c75c448164278d5eea20f68", "2": "39b3974a-36c7-40f3-a3d6-a959ccf7fae2", "3": "ee10ad96-45b1-436e-aac6-59fbf7d3f280"}}, "__type__": "1"}, "ee10ad96-45b1-436e-aac6-59fbf7d3f280": {"__data__": {"text": "- start\n    logger.info(\n        f\"EPOCH {epoch} save logits takes {datetime.timedelta(seconds=int(epoch_time))}\")\n\n\n@torch.no_grad()\ndef check_logits_one_epoch(config, model, data_loader, epoch, mixup_fn):\n    model.eval()\n\n    num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    meters = defaultdict(AverageMeter)\n\n    start = time.time()\n    end = time.time()\n    topk = config.DISTILL.LOGITS_TOPK\n\n    for idx, ((samples, targets), (saved_logits_index, saved_logits_value, seeds)) in enumerate(data_loader):\n        samples = samples.cuda(non_blocking=True)\n        targets = targets.cuda(non_blocking=True)\n\n        if mixup_fn is not None:\n            samples, targets = mixup_fn(samples, targets, seeds)\n\n        with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n            outputs = model(samples)\n\n        softmax_prob = torch.softmax(outputs, -1)\n\n        torch.cuda.synchronize()\n\n        values, indices = softmax_prob.topk(\n            k=topk, dim=-1, largest=True, sorted=True)\n\n        meters['error'].update(\n            (values - saved_logits_value.cuda()).abs().mean().item())\n        meters['diff_rate'].update(torch.count_nonzero(\n            (indices != saved_logits_index.cuda())).item() / indices.numel())\n\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            etas = batch_time.avg * (num_steps - idx)\n            extra_meters_str = ''\n            for k,", "doc_id": "ee10ad96-45b1-436e-aac6-59fbf7d3f280", "embedding": null, "doc_hash": "3f6a5912d0368b4a061cc039749dccb2b244040ef2e6f2402d4785f8a5574bc1", "extra_info": {"file_path": "TinyViT/save_logits.py", "file_name": "save_logits.py"}, "node_info": {"start": 6494, "end": 8044}, "relationships": {"1": "454ab3769cc57ab87c75c448164278d5eea20f68", "2": "2cb85a72-9586-47fa-a1c9-2413e955fb86", "3": "4467b060-d6a6-4f99-81dd-baa8a35ce6e4"}}, "__type__": "1"}, "4467b060-d6a6-4f99-81dd-baa8a35ce6e4": {"__data__": {"text": "           etas = batch_time.avg * (num_steps - idx)\n            extra_meters_str = ''\n            for k, v in meters.items():\n                extra_meters_str += f'{k} {v.val:.4f} ({v.avg:.4f})\\t'\n            logger.info(\n                f'Check: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]\\t'\n                f'eta {datetime.timedelta(seconds=int(etas))}\\t'\n                f'time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'{extra_meters_str}'\n                f'mem {memory_used:.0f}MB')\n\n    epoch_time = time.time() - start\n    logger.info(\n        f\"EPOCH {epoch} check logits takes {datetime.timedelta(seconds=int(epoch_time))}\")\n\n\n@torch.no_grad()\ndef validate(config, data_loader, model, num_classes=1000):\n    criterion = torch.nn.CrossEntropyLoss()\n    model.eval()\n\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    acc1_meter = AverageMeter()\n    acc5_meter = AverageMeter()\n\n    end = time.time()\n    for idx, (images, target) in enumerate(data_loader):\n        images = images.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n        # compute output\n        with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n            output = model(images)\n\n        if num_classes == 1000:\n            output_num_classes = output.size(-1)\n            if output_num_classes == 21841:\n                output = remap_layer_22kto1k(output)\n\n        # measure accuracy and record", "doc_id": "4467b060-d6a6-4f99-81dd-baa8a35ce6e4", "embedding": null, "doc_hash": "ca0e6288b2dd65f51c2b01c1e46563a419e161dbbc75c25f260cc83a72e763ef", "extra_info": {"file_path": "TinyViT/save_logits.py", "file_name": "save_logits.py"}, "node_info": {"start": 8103, "end": 9556}, "relationships": {"1": "454ab3769cc57ab87c75c448164278d5eea20f68", "2": "ee10ad96-45b1-436e-aac6-59fbf7d3f280", "3": "91dc85b1-470c-4a02-b95e-d093a9f60b78"}}, "__type__": "1"}, "91dc85b1-470c-4a02-b95e-d093a9f60b78": {"__data__": {"text": "        if output_num_classes == 21841:\n                output = remap_layer_22kto1k(output)\n\n        # measure accuracy and record loss\n        loss = criterion(output, target)\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        loss_meter.update(loss.item(), target.size(0))\n        acc1_meter.update(acc1.item(), target.size(0))\n        acc5_meter.update(acc5.item(), target.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            logger.info(\n                f'Test: [{idx}/{len(data_loader)}]\\t'\n                f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                f'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'Acc@1 {acc1_meter.val:.3f} ({acc1_meter.avg:.3f})\\t'\n                f'Acc@5 {acc5_meter.val:.3f} ({acc5_meter.avg:.3f})\\t'\n                f'Mem {memory_used:.0f}MB')\n\n    acc1_meter.sync()\n    acc5_meter.sync()\n    logger.info(f' * Acc@1 {acc1_meter.avg:.3f} Acc@5 {acc5_meter.avg:.3f}')\n    return acc1_meter.avg, acc5_meter.avg, loss_meter.avg\n\n\nif __name__ == '__main__':\n    args, config = parse_option()\n\n    config.defrost()\n    assert len(\n        config.DISTILL.TEACHER_LOGITS_PATH) > 0, \"Please fill in the config", "doc_id": "91dc85b1-470c-4a02-b95e-d093a9f60b78", "embedding": null, "doc_hash": "423ff2d67a40f218157d9e50889e8dfdfaea352c9b66402cd0ccd56ed8f9cbb6", "extra_info": {"file_path": "TinyViT/save_logits.py", "file_name": "save_logits.py"}, "node_info": {"start": 9528, "end": 10917}, "relationships": {"1": "454ab3769cc57ab87c75c448164278d5eea20f68", "2": "4467b060-d6a6-4f99-81dd-baa8a35ce6e4", "3": "a1b9d042-b426-4729-a483-72c140af999b"}}, "__type__": "1"}, "a1b9d042-b426-4729-a483-72c140af999b": {"__data__": {"text": "   args, config = parse_option()\n\n    config.defrost()\n    assert len(\n        config.DISTILL.TEACHER_LOGITS_PATH) > 0, \"Please fill in the config DISTILL.TEACHER_LOGITS_PATH\"\n    config.DISTILL.ENABLED = True\n    if not args.check_saved_logits:\n        config.DISTILL.SAVE_TEACHER_LOGITS = True\n    config.freeze()\n\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        rank = int(os.environ[\"RANK\"])\n        world_size = int(os.environ['WORLD_SIZE'])\n        print(f\"RANK and WORLD_SIZE in environ: {rank}/{world_size}\")\n    else:\n        rank = -1\n        world_size = -1\n\n    torch.cuda.set_device(config.LOCAL_RANK)\n    torch.distributed.init_process_group(\n        backend='nccl', init_method='env://', world_size=world_size, rank=rank)\n    torch.distributed.barrier()\n\n    # The seed changes with config, rank, world_size and epoch\n    seed = config.SEED + dist.get_rank() + config.TRAIN.START_EPOCH * \\\n        dist.get_world_size()\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    cudnn.benchmark = True\n\n    os.makedirs(config.OUTPUT, exist_ok=True)\n    logger = create_logger(output_dir=config.OUTPUT,\n                           dist_rank=dist.get_rank(), name=f\"{config.MODEL.NAME}\")\n\n    if dist.get_rank() == 0:\n        path = os.path.join(config.OUTPUT, \"config.json\")\n        with open(path, \"w\") as f:\n            f.write(config.dump())\n        logger.info(f\"Full config saved to {path}\")\n\n    # print config\n    logger.info(config.dump())\n\n    main(config)\n", "doc_id": "a1b9d042-b426-4729-a483-72c140af999b", "embedding": null, "doc_hash": "8ab29d5d63c336a754686c472d6cb5ecb112faecd0f18db588aca6063f6e8598", "extra_info": {"file_path": "TinyViT/save_logits.py", "file_name": "save_logits.py"}, "node_info": {"start": 10861, "end": 12413}, "relationships": {"1": "454ab3769cc57ab87c75c448164278d5eea20f68", "2": "91dc85b1-470c-4a02-b95e-d093a9f60b78"}}, "__type__": "1"}, "d0f0b28b-31f2-43d0-bd8d-0e9e6f5f39f4": {"__data__": {"text": "import unittest\nimport os\nimport sys\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport torch\nimport timm\nfrom models import tiny_vit\n\n\nclass ModelsTestCase(unittest.TestCase):\n    \"\"\"Test for models.py\"\"\"\n    def setUp(self):\n        self.ckpt_names = [\n            ('tiny_vit_5m_224', ['22k_distill', '22kto1k_distill', '1k']),\n            ('tiny_vit_11m_224', ['22k_distill', '22kto1k_distill', '1k']),\n            ('tiny_vit_21m_224', ['22k_distill', '22kto1k_distill', '1k']),\n            ('tiny_vit_21m_384', ['22kto1k_distill']),\n            ('tiny_vit_21m_512', ['22kto1k_distill']),\n        ]\n\n    def test_load_model(self):\n        \"\"\"Test for load_model\"\"\"\n        for variant, pretrained_types in self.ckpt_names:\n            # empty load\n            with self.subTest(variant=variant, pretrained_type='empty'):\n                model = timm.create_model(variant)\n                assert model.head.weight.shape[0] == 1000\n            # load pretrained\n            for pretrained_type in pretrained_types:\n                with self.subTest(variant=variant, pretrained_type=pretrained_type):\n                    pretrained_num_classes = 21841 if pretrained_type == '22k_distill' else 1000\n                    model = timm.create_model(variant, pretrained=True, num_classes=pretrained_num_classes)\n                    assert model.head.weight.shape[0] == pretrained_num_classes\n                    model = timm.create_model(variant, pretrained=True, pretrained_type=pretrained_type)\n   ", "doc_id": "d0f0b28b-31f2-43d0-bd8d-0e9e6f5f39f4", "embedding": null, "doc_hash": "a119fb9ed403579eca8789dddedd878695eed3bae0dd5f241436ce061f585976", "extra_info": {"file_path": "TinyViT/tests/test_models.py", "file_name": "test_models.py"}, "node_info": {"start": 0, "end": 1531}, "relationships": {"1": "2b9eabbd39588943350b9978b45bb5b23103fa62", "3": "32e64f03-0e47-456e-a6a3-079afb09c1c9"}}, "__type__": "1"}, "32e64f03-0e47-456e-a6a3-079afb09c1c9": {"__data__": {"text": "== pretrained_num_classes\n                    model = timm.create_model(variant, pretrained=True, pretrained_type=pretrained_type)\n                    assert model.head.weight.shape[0] == pretrained_num_classes\n\n    def test_finetune(self):\n        pretrained_num_classes = 1000\n        finetune_num_classes = 100\n        model1 = timm.create_model('tiny_vit_5m_224', pretrained=True, pretrained_type='22kto1k_distill')\n        model2 = timm.create_model('tiny_vit_5m_224', pretrained=True, pretrained_type='22kto1k_distill',\n            num_classes=finetune_num_classes)\n        state_dict_1 = model1.state_dict()\n        state_dict_2 = model2.state_dict()\n        keys = list(state_dict_1.keys())\n        head_keys = ['head.weight', 'head.bias']\n        for name in head_keys:\n            self.assertEqual(state_dict_1.pop(name).shape[0], pretrained_num_classes)\n            self.assertEqual(state_dict_2.pop(name).shape[0], finetune_num_classes)\n        for key in keys:\n            if key not in head_keys:\n                self.assertTrue(torch.equal(state_dict_1[key], state_dict_2[key]))\n\n    def test_forward(self):\n        for variant, _ in self.ckpt_names:\n            with self.subTest(variant=variant):\n                model = timm.create_model(variant)\n                img_size = int(variant.split('_')[-1])\n                img = torch.randn(1, 3, img_size, img_size)\n                out = model(img)\n                assert out.shape[-1] == 1000\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "doc_id": "32e64f03-0e47-456e-a6a3-079afb09c1c9", "embedding": null, "doc_hash": "d237d046e57d5ad1b245de65ab38f2bd345e1c6b97022d63cd0a8b8fc40346bb", "extra_info": {"file_path": "TinyViT/tests/test_models.py", "file_name": "test_models.py"}, "node_info": {"start": 1397, "end": 2904}, "relationships": {"1": "2b9eabbd39588943350b9978b45bb5b23103fa62", "2": "d0f0b28b-31f2-43d0-bd8d-0e9e6f5f39f4"}}, "__type__": "1"}, "49be0348-152e-407c-b516-73ef761984e6": {"__data__": {"text": "# ---------------------------------------------------------------\n# TinyViT Utils\n# Copyright (c) 2022 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Based on the code: Swin Transformer\n#   (https://github.com/microsoft/swin-transformer)\n# Add `LRSchedulerWrapper` and `divide_param_groups_by_lr_scale`\n# ---------------------------------------------------------------\n\nimport copy\nimport torch.distributed as dist\n\n\ndef is_main_process():\n    return dist.get_rank() == 0\n\n\nclass LRSchedulerWrapper:\n    \"\"\"\n    LR Scheduler Wrapper\n\n    This class attaches the pre-hook on the `step` functions (including `step`, `step_update`, `step_frac`) of a lr scheduler.\n    When `step` functions are called, the learning rates of all layers are updated.\n\n    Usage:\n    ```\n        lr_scheduler = LRSchedulerWrapper(lr_scheduler, optimizer)\n    ```\n    \"\"\"\n\n    def __init__(self, lr_scheduler, optimizer):\n        self.lr_scheduler = lr_scheduler\n        self.optimizer = optimizer\n\n    def step(self, epoch):\n        self.lr_scheduler.step(epoch)\n        self.update_lr()\n\n    def step_update(self, it):\n        self.lr_scheduler.step_update(it)\n        self.update_lr()\n\n    def step_frac(self, frac):\n        if hasattr(self.lr_scheduler, 'step_frac'):\n            self.lr_scheduler.step_frac(frac)\n            self.update_lr()\n\n    def update_lr(self):\n        param_groups = self.optimizer.param_groups\n        for group in param_groups:\n            if 'lr_scale' not in group:\n                continue\n            params = group['params']\n            # update lr scale\n            lr_scale = None\n            for p in params:\n                if hasattr(p, 'lr_scale'):\n                    if lr_scale is None:\n                    ", "doc_id": "49be0348-152e-407c-b516-73ef761984e6", "embedding": null, "doc_hash": "545f0c1293d8cbd8e99ca1428a613096b355ce92f4cb338f47bf8a3f6b729b5b", "extra_info": {"file_path": "TinyViT/tinyvit_utils.py", "file_name": "tinyvit_utils.py"}, "node_info": {"start": 0, "end": 1752}, "relationships": {"1": "5d96fea36c825558719097764f90aae4ad97c4fa", "3": "019d2e5f-2ca7-48d7-a2c3-3ba674ebfd06"}}, "__type__": "1"}, "019d2e5f-2ca7-48d7-a2c3-3ba674ebfd06": {"__data__": {"text": "    if hasattr(p, 'lr_scale'):\n                    if lr_scale is None:\n                        lr_scale = p.lr_scale\n                    else:\n                        assert lr_scale == p.lr_scale, (lr_scale, p.lr_scale)\n            if lr_scale != group['lr_scale']:\n                if is_main_process():\n                    print('=' * 30)\n                    print(\"params:\", [e.param_name for e in params])\n                    print(\n                        f\"change lr scale: {group['lr_scale']} to {lr_scale}\")\n            group['lr_scale'] = lr_scale\n            if lr_scale is not None:\n                group['lr'] *= lr_scale\n\n    def state_dict(self):\n        return self.lr_scheduler.state_dict()\n\n    def load_state_dict(self, *args, **kwargs):\n        self.lr_scheduler.load_state_dict(*args, **kwargs)\n\n\ndef divide_param_groups_by_lr_scale(param_groups):\n    \"\"\"\n    Divide parameters with different lr scale into different groups.\n\n    Inputs\n    ------\n    param_groups: a list of dict of torch.nn.Parameter\n    ```\n    # example:\n    param1.lr_scale = param2.lr_scale = param3.lr_scale = 0.6\n    param4.lr_scale = param5.lr_scale = param6.lr_scale = 0.3\n    param_groups = [{'params': [param1, param2, param4]},\n                    {'params': [param3, param5, param6], 'weight_decay': 0.}]\n\n    param_groups = divide_param_groups_by_lr_scale(param_groups)\n    ```\n\n    Outputs\n    -------\n    new_param_groups: a list of dict containing the key", "doc_id": "019d2e5f-2ca7-48d7-a2c3-3ba674ebfd06", "embedding": null, "doc_hash": "bab0d20f7183ec0ba2b11954cd58416997a5926ef96d9abed77fbe54886743ec", "extra_info": {"file_path": "TinyViT/tinyvit_utils.py", "file_name": "tinyvit_utils.py"}, "node_info": {"start": 1710, "end": 3171}, "relationships": {"1": "5d96fea36c825558719097764f90aae4ad97c4fa", "2": "49be0348-152e-407c-b516-73ef761984e6", "3": "b41026ba-5d35-4b89-9e7d-73c497779f0f"}}, "__type__": "1"}, "b41026ba-5d35-4b89-9e7d-73c497779f0f": {"__data__": {"text": "0.}]\n\n    param_groups = divide_param_groups_by_lr_scale(param_groups)\n    ```\n\n    Outputs\n    -------\n    new_param_groups: a list of dict containing the key `lr_scale`\n    ```\n    param_groups = [\n        {'params': [param1, param2], 'lr_scale': 0.6},\n        {'params': [param3], 'weight_decay': 0., 'lr_scale': 0.6}\n        {'params': [param4], 'lr_scale': 0.3},\n        {'params': [param5, param6], 'weight_decay': 0., 'lr_scale': 0.3}\n    ]\n    ```\n    \"\"\"\n    new_groups = []\n    for group in param_groups:\n        params = group.pop('params')\n\n        '''\n        divide parameters to different groups by lr_scale\n        '''\n        lr_scale_groups = dict()\n        for p in params:\n            lr_scale = getattr(p, 'lr_scale', 1.0)\n\n            # create a list if not existed\n            if lr_scale not in lr_scale_groups:\n                lr_scale_groups[lr_scale] = list()\n\n            # add the parameter with `lr_scale` into the specific group.\n            lr_scale_groups[lr_scale].append(p)\n\n        for lr_scale, params in lr_scale_groups.items():\n            # copy other parameter information like `weight_decay`\n            new_group = copy.copy(group)\n            new_group['params'] = params\n            new_group['lr_scale'] = lr_scale\n            new_groups.append(new_group)\n    return new_groups\n\n\ndef set_weight_decay(model):\n    skip_list = {}\n    skip_keywords = {}\n    if hasattr(model, 'no_weight_decay'):\n        skip_list = model.no_weight_decay()\n    if hasattr(model, 'no_weight_decay_keywords'):\n        skip_keywords =", "doc_id": "b41026ba-5d35-4b89-9e7d-73c497779f0f", "embedding": null, "doc_hash": "92f46ac951ad0b99ff275c33e9b87b70d8dedfa06f51e9c1371aced730c7d17e", "extra_info": {"file_path": "TinyViT/tinyvit_utils.py", "file_name": "tinyvit_utils.py"}, "node_info": {"start": 3085, "end": 4642}, "relationships": {"1": "5d96fea36c825558719097764f90aae4ad97c4fa", "2": "019d2e5f-2ca7-48d7-a2c3-3ba674ebfd06", "3": "42a616ce-b425-4c02-97d2-eecd91a56612"}}, "__type__": "1"}, "42a616ce-b425-4c02-97d2-eecd91a56612": {"__data__": {"text": "       skip_list = model.no_weight_decay()\n    if hasattr(model, 'no_weight_decay_keywords'):\n        skip_keywords = model.no_weight_decay_keywords()\n\n    has_decay = []\n    no_decay = []\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue  # frozen weights\n        if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n                check_keywords_in_name(name, skip_keywords):\n            no_decay.append(param)\n        else:\n            has_decay.append(param)\n    return [{'params': has_decay},\n            {'params': no_decay, 'weight_decay': 0.}]\n\n\ndef check_keywords_in_name(name, keywords=()):\n    isin = False\n    for keyword in keywords:\n        if keyword in name:\n            isin = True\n    return isin\n", "doc_id": "42a616ce-b425-4c02-97d2-eecd91a56612", "embedding": null, "doc_hash": "251ef5ee191ae5bfec219eb3324d3ea9a69542aaa05a0f6a48f57a9860953a03", "extra_info": {"file_path": "TinyViT/tinyvit_utils.py", "file_name": "tinyvit_utils.py"}, "node_info": {"start": 4655, "end": 5453}, "relationships": {"1": "5d96fea36c825558719097764f90aae4ad97c4fa", "2": "b41026ba-5d35-4b89-9e7d-73c497779f0f"}}, "__type__": "1"}, "68ac267b-f7bc-4eaf-a8ef-48cd01574e90": {"__data__": {"text": "# --------------------------------------------------------\n# TinyViT Utils (save/load checkpoints, etc.)\n# Copyright (c) 2022 Microsoft\n# Based on the code: Swin Transformer\n#   (https://github.com/microsoft/swin-transformer)\n# Adapted for TinyViT\n# --------------------------------------------------------\n\nimport os\nimport torch\nimport torch.distributed as dist\nimport subprocess\n\n\ndef add_common_args(parser):\n    parser.add_argument('--cfg', type=str, required=True,\n                        metavar=\"FILE\", help='path to config file', )\n    parser.add_argument(\n        \"--opts\",\n        help=\"Modify config options by adding 'KEY VALUE' pairs. \",\n        default=None,\n        nargs='+',\n    )\n\n    # easy config modification\n    parser.add_argument('--batch-size', type=int,\n                        help=\"batch size for single GPU\")\n    parser.add_argument('--data-path', type=str, help='path to dataset')\n    parser.add_argument('--pretrained',\n                        help='pretrained weight from checkpoint, could be imagenet22k pretrained weight')\n    parser.add_argument('--resume', help='resume from checkpoint')\n    parser.add_argument('--accumulation-steps', type=int,\n                        help=\"gradient accumulation steps\")\n    parser.add_argument('--use-checkpoint', action='store_true',\n                        help=\"whether to use gradient checkpointing to save memory\")\n    parser.add_argument('--disable_amp', action='store_true',\n                        help='Disable pytorch amp')\n    parser.add_argument('--output', default='output', type=str, metavar='PATH',\n                        help='root of output folder, the full path is <output>/<model_name>/<tag> (default: output)')\n    parser.add_argument('--tag', help='tag of experiment')\n    parser.add_argument('--eval', action='store_true',\n                        help='Perform evaluation only')\n    parser.add_argument('--only-cpu', action='store_true',\n  ", "doc_id": "68ac267b-f7bc-4eaf-a8ef-48cd01574e90", "embedding": null, "doc_hash": "e759389860e569aa139eeb1684c4459de4251396e00ac892d9cee30673793ba5", "extra_info": {"file_path": "TinyViT/utils.py", "file_name": "utils.py"}, "node_info": {"start": 0, "end": 1936}, "relationships": {"1": "0b54bce457ea9eb3886a328e8c755629fdfb9965", "3": "3198c62b-4dde-4d68-b2bc-9843c82c139f"}}, "__type__": "1"}, "3198c62b-4dde-4d68-b2bc-9843c82c139f": {"__data__": {"text": "action='store_true',\n                        help='Perform evaluation only')\n    parser.add_argument('--only-cpu', action='store_true',\n                        help='Perform evaluation on CPU')\n    parser.add_argument('--throughput', action='store_true',\n                        help='Test throughput only')\n    parser.add_argument('--use-sync-bn', action='store_true',\n                        default=False, help='sync bn')\n    parser.add_argument('--use-wandb', action='store_true',\n                        default=False, help='use wandb to record log')\n\n    # distributed training\n    parser.add_argument(\"--local_rank\", type=int,\n                        help='local rank for DistributedDataParallel')\n\n\ndef load_checkpoint(config, model, optimizer, lr_scheduler, loss_scaler, logger):\n    logger.info(\n        f\"==============> Resuming form {config.MODEL.RESUME}....................\")\n    if config.MODEL.RESUME.startswith('https'):\n        checkpoint = torch.hub.load_state_dict_from_url(\n            config.MODEL.RESUME, map_location='cpu', check_hash=True)\n    else:\n        checkpoint = torch.load(config.MODEL.RESUME, map_location='cpu')\n\n    params = checkpoint['model']\n    now_model_state = model.state_dict()\n    mnames = ['head.weight', 'head.bias']  # (cls, 1024), (cls, )\n    if mnames[-1] in params:\n        ckpt_head_bias = params[mnames[-1]]\n        now_model_bias = now_model_state[mnames[-1]]\n        if ckpt_head_bias.shape != now_model_bias.shape:\n            num_classes = 1000\n\n            if len(ckpt_head_bias) == 21841 and len(now_model_bias) == num_classes:\n                logger.info(\"Convert checkpoint from 21841 to 1k\")\n ", "doc_id": "3198c62b-4dde-4d68-b2bc-9843c82c139f", "embedding": null, "doc_hash": "e5e926e2fbb5cd500b3c07123c0fce5eb61b6562fd581ed9385c9031d68e3340", "extra_info": {"file_path": "TinyViT/utils.py", "file_name": "utils.py"}, "node_info": {"start": 1832, "end": 3488}, "relationships": {"1": "0b54bce457ea9eb3886a328e8c755629fdfb9965", "2": "68ac267b-f7bc-4eaf-a8ef-48cd01574e90", "3": "122fc486-8bbf-4e9d-b6db-5961497ec53e"}}, "__type__": "1"}, "122fc486-8bbf-4e9d-b6db-5961497ec53e": {"__data__": {"text": " if len(ckpt_head_bias) == 21841 and len(now_model_bias) == num_classes:\n                logger.info(\"Convert checkpoint from 21841 to 1k\")\n                # convert 22kto1k\n                fname = './imagenet_1kto22k.txt'\n                with open(fname) as fin:\n                    mapping = torch.Tensor(\n                        list(map(int, fin.readlines()))).to(torch.long)\n                for name in mnames:\n                    v = params[name]\n                    shape = list(v.shape)\n                    shape[0] = num_classes\n                    mean_v = v[mapping[mapping != -1]].mean(0, keepdim=True)\n                    v = torch.cat([v, mean_v], 0)\n                    v = v[mapping]\n                    params[name] = v\n\n    msg = model.load_state_dict(params, strict=False)\n    logger.info(msg)\n    max_accuracy = 0.0\n    if not config.EVAL_MODE:\n        if 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint:\n            if optimizer is not None:\n                optimizer.load_state_dict(checkpoint['optimizer'])\n            if lr_scheduler is not None:\n                lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n            if 'scaler' in checkpoint:\n                loss_scaler.load_state_dict(checkpoint['scaler'])\n            logger.info(\n                f\"=> loaded successfully", "doc_id": "122fc486-8bbf-4e9d-b6db-5961497ec53e", "embedding": null, "doc_hash": "a374f3f2ffb042976d4faded37e27440af5015893e13f01748e7fcaa048eac02", "extra_info": {"file_path": "TinyViT/utils.py", "file_name": "utils.py"}, "node_info": {"start": 3483, "end": 4808}, "relationships": {"1": "0b54bce457ea9eb3886a328e8c755629fdfb9965", "2": "3198c62b-4dde-4d68-b2bc-9843c82c139f", "3": "c9e513c6-8f85-4ad3-831c-90cef62742b1"}}, "__type__": "1"}, "c9e513c6-8f85-4ad3-831c-90cef62742b1": {"__data__": {"text": "        loss_scaler.load_state_dict(checkpoint['scaler'])\n            logger.info(\n                f\"=> loaded successfully '{config.MODEL.RESUME}' (epoch {checkpoint['epoch']})\")\n            if 'max_accuracy' in checkpoint:\n                max_accuracy = checkpoint['max_accuracy']\n\n        if 'epoch' in checkpoint:\n            config.defrost()\n            config.TRAIN.START_EPOCH = checkpoint['epoch'] + 1\n            config.freeze()\n\n    del checkpoint\n    torch.cuda.empty_cache()\n    return max_accuracy\n\n\ndef load_pretrained(config, model, logger):\n    logger.info(\n        f\"==============> Loading weight {config.MODEL.PRETRAINED} for fine-tuning......\")\n    checkpoint = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n    state_dict = checkpoint['model']\n\n    # delete relative_position_index since we always re-init it\n    relative_position_index_keys = [\n        k for k in state_dict.keys() if \"relative_position_index\" in k]\n    for k in relative_position_index_keys:\n        del state_dict[k]\n\n    # delete relative_coords_table since we always re-init it\n    relative_position_index_keys = [\n        k for k in state_dict.keys() if \"relative_coords_table\" in k]\n    for k in relative_position_index_keys:\n        del state_dict[k]\n\n    # delete attn_mask since we always re-init it\n    attn_mask_keys = [k for k in state_dict.keys() if \"attn_mask\" in k]\n    for k in attn_mask_keys:\n        del state_dict[k]\n\n    model_state_dict = model.state_dict()\n\n    # bicubic interpolate relative_position_bias_table if not match\n    relative_position_bias_table_keys = [\n        k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n    for k in relative_position_bias_table_keys:\n        relative_position_bias_table_pretrained = state_dict[k]\n ", "doc_id": "c9e513c6-8f85-4ad3-831c-90cef62742b1", "embedding": null, "doc_hash": "0966129b0bb64c2b40102e14b7c75ed9c31b6d2e0248872c1a7c0549c1a18ef3", "extra_info": {"file_path": "TinyViT/utils.py", "file_name": "utils.py"}, "node_info": {"start": 4835, "end": 6614}, "relationships": {"1": "0b54bce457ea9eb3886a328e8c755629fdfb9965", "2": "122fc486-8bbf-4e9d-b6db-5961497ec53e", "3": "1a90d616-e7a4-4fe1-9130-aff6f2eb5c44"}}, "__type__": "1"}, "1a90d616-e7a4-4fe1-9130-aff6f2eb5c44": {"__data__": {"text": "if \"relative_position_bias_table\" in k]\n    for k in relative_position_bias_table_keys:\n        relative_position_bias_table_pretrained = state_dict[k]\n        relative_position_bias_table_current = model_state_dict[k]\n        L1, nH1 = relative_position_bias_table_pretrained.size()\n        L2, nH2 = relative_position_bias_table_current.size()\n        if nH1 != nH2:\n            logger.warning(f\"Error in loading {k}, passing......\")\n        else:\n            if L1 != L2:\n                # bicubic interpolate relative_position_bias_table if not match\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                relative_position_bias_table_pretrained_resized = torch.nn.functional.interpolate(\n                    relative_position_bias_table_pretrained.permute(1, 0).view(1, nH1, S1, S1), size=(S2, S2),\n                    mode='bicubic')\n                state_dict[k] = relative_position_bias_table_pretrained_resized.view(\n                    nH2, L2).permute(1, 0)\n\n    # bicubic interpolate attention_biases if not match\n    relative_position_bias_table_keys = [\n        k for k in state_dict.keys() if \"attention_biases\" in k]\n    for k in relative_position_bias_table_keys:\n        relative_position_bias_table_pretrained = state_dict[k]\n        relative_position_bias_table_current = model_state_dict[k]\n        nH1, L1 = relative_position_bias_table_pretrained.size()\n        nH2, L2 = relative_position_bias_table_current.size()\n        if nH1 != nH2:\n            logger.warning(f\"Error in", "doc_id": "1a90d616-e7a4-4fe1-9130-aff6f2eb5c44", "embedding": null, "doc_hash": "140ead90969534c7e7b089df5200d8ae72a2fbef360ec6cc86dd4732d60d7a0e", "extra_info": {"file_path": "TinyViT/utils.py", "file_name": "utils.py"}, "node_info": {"start": 6568, "end": 8100}, "relationships": {"1": "0b54bce457ea9eb3886a328e8c755629fdfb9965", "2": "c9e513c6-8f85-4ad3-831c-90cef62742b1", "3": "24bfafb8-6524-456e-bbda-b4dbd04b92b7"}}, "__type__": "1"}, "24bfafb8-6524-456e-bbda-b4dbd04b92b7": {"__data__": {"text": "     nH2, L2 = relative_position_bias_table_current.size()\n        if nH1 != nH2:\n            logger.warning(f\"Error in loading {k}, passing......\")\n        else:\n            if L1 != L2:\n                # bicubic interpolate relative_position_bias_table if not match\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                relative_position_bias_table_pretrained_resized = torch.nn.functional.interpolate(\n                    relative_position_bias_table_pretrained.view(1, nH1, S1, S1), size=(S2, S2),\n                    mode='bicubic')\n                state_dict[k] = relative_position_bias_table_pretrained_resized.view(\n                    nH2, L2)\n\n    # bicubic interpolate absolute_pos_embed if not match\n    absolute_pos_embed_keys = [\n        k for k in state_dict.keys() if \"absolute_pos_embed\" in k]\n    for k in absolute_pos_embed_keys:\n        # dpe\n        absolute_pos_embed_pretrained = state_dict[k]\n        absolute_pos_embed_current = model.state_dict()[k]\n        _, L1, C1 = absolute_pos_embed_pretrained.size()\n        _, L2, C2 = absolute_pos_embed_current.size()\n        if C1 != C1:\n            logger.warning(f\"Error in loading {k}, passing......\")\n        else:\n            if L1 != L2:\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.reshape(\n     ", "doc_id": "24bfafb8-6524-456e-bbda-b4dbd04b92b7", "embedding": null, "doc_hash": "99050237d5104c74bb3838e67a43d09f07a81adb80a6854a78194b29d0abc272", "extra_info": {"file_path": "TinyViT/utils.py", "file_name": "utils.py"}, "node_info": {"start": 8146, "end": 9558}, "relationships": {"1": "0b54bce457ea9eb3886a328e8c755629fdfb9965", "2": "1a90d616-e7a4-4fe1-9130-aff6f2eb5c44", "3": "465f315f-049e-41cb-bab0-8db4f5e820f9"}}, "__type__": "1"}, "465f315f-049e-41cb-bab0-8db4f5e820f9": {"__data__": {"text": "         S2 = int(L2 ** 0.5)\n                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.reshape(\n                    -1, S1, S1, C1)\n                absolute_pos_embed_pretrained = absolute_pos_embed_pretrained.permute(\n                    0, 3, 1, 2)\n                absolute_pos_embed_pretrained_resized = torch.nn.functional.interpolate(\n                    absolute_pos_embed_pretrained, size=(S2, S2), mode='bicubic')\n                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.permute(\n                    0, 2, 3, 1)\n                absolute_pos_embed_pretrained_resized = absolute_pos_embed_pretrained_resized.flatten(\n                    1, 2)\n                state_dict[k] = absolute_pos_embed_pretrained_resized\n\n    # check classifier, if not match, then re-init classifier to zero\n    head_bias_pretrained = state_dict['head.bias']\n    Nc1 = head_bias_pretrained.shape[0]\n    Nc2 = model.head.bias.shape[0]\n    if (Nc1 != Nc2):\n        if Nc1 == 21841 and Nc2 == 1000:\n            logger.info(\"loading ImageNet-21841 weight to ImageNet-1K ......\")\n            map22kto1k_path = f'./imagenet_1kto22k.txt'\n            with open(map22kto1k_path) as fin:\n                mapping = torch.Tensor(\n                    list(map(int, fin.readlines()))).to(torch.long)\n            for name in ['head.weight', 'head.bias']:\n                v = state_dict[name]\n      ", "doc_id": "465f315f-049e-41cb-bab0-8db4f5e820f9", "embedding": null, "doc_hash": "72189cffd313e0795800bd7ef182bc0bcae3543cad3fd1ae31e663fdb561a140", "extra_info": {"file_path": "TinyViT/utils.py", "file_name": "utils.py"}, "node_info": {"start": 9561, "end": 10983}, "relationships": {"1": "0b54bce457ea9eb3886a328e8c755629fdfb9965", "2": "24bfafb8-6524-456e-bbda-b4dbd04b92b7", "3": "84f3b64f-6122-4e62-9705-623d8514f787"}}, "__type__": "1"}, "84f3b64f-6122-4e62-9705-623d8514f787": {"__data__": {"text": "           for name in ['head.weight', 'head.bias']:\n                v = state_dict[name]\n                mean_v = v[mapping[mapping != -1]].mean(0, keepdim=True)\n                v = torch.cat([v, mean_v], 0)\n                v = v[mapping]\n                state_dict[name] = v\n        else:\n            torch.nn.init.constant_(model.head.bias, 0.)\n            torch.nn.init.constant_(model.head.weight, 0.)\n            del state_dict['head.weight']\n            del state_dict['head.bias']\n            logger.warning(\n                f\"Error in loading classifier head, re-init classifier head to 0\")\n\n    msg = model.load_state_dict(state_dict, strict=False)\n    logger.warning(msg)\n\n    logger.info(f\"=> loaded successfully '{config.MODEL.PRETRAINED}'\")\n\n    del checkpoint\n    torch.cuda.empty_cache()\n\n\ndef save_checkpoint(config, epoch, model, max_accuracy, optimizer, lr_scheduler, loss_scaler, logger):\n    save_state = {'model': model.state_dict(),\n                  'optimizer': optimizer.state_dict(),\n                  'lr_scheduler': lr_scheduler.state_dict(),\n                  'max_accuracy': max_accuracy,\n                  'scaler': loss_scaler.state_dict(),\n                  'epoch': epoch,\n                  'config': config}\n\n    save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{epoch}.pth')\n    logger.info(f\"{save_path} saving......\")\n    torch.save(save_state, save_path)\n    logger.info(f\"{save_path} saved !!!\")\n\n\ndef", "doc_id": "84f3b64f-6122-4e62-9705-623d8514f787", "embedding": null, "doc_hash": "582a5dae704f885966b5a6deb5bd6995b8568e29c309aee091132abd0fec14ea", "extra_info": {"file_path": "TinyViT/utils.py", "file_name": "utils.py"}, "node_info": {"start": 11012, "end": 12458}, "relationships": {"1": "0b54bce457ea9eb3886a328e8c755629fdfb9965", "2": "465f315f-049e-41cb-bab0-8db4f5e820f9", "3": "677867c9-eea9-4bd3-86f5-ae5204525c90"}}, "__type__": "1"}, "677867c9-eea9-4bd3-86f5-ae5204525c90": {"__data__": {"text": "   logger.info(f\"{save_path} saving......\")\n    torch.save(save_state, save_path)\n    logger.info(f\"{save_path} saved !!!\")\n\n\ndef auto_resume_helper(output_dir):\n    checkpoints = os.listdir(output_dir)\n    checkpoints = [ckpt for ckpt in checkpoints if ckpt.endswith('pth')]\n    print(f\"All checkpoints founded in {output_dir}: {checkpoints}\")\n    if len(checkpoints) > 0:\n        latest_checkpoint = max([os.path.join(output_dir, d)\n                                for d in checkpoints], key=os.path.getmtime)\n        print(f\"The latest checkpoint founded: {latest_checkpoint}\")\n        resume_file = latest_checkpoint\n    else:\n        resume_file = None\n    return resume_file\n\n\ndef reduce_tensor(tensor, n=None):\n    if n is None:\n        n = dist.get_world_size()\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt = rt / n\n    return rt\n\n\ndef ampscaler_get_grad_norm(parameters, norm_type: float = 2.0) -> torch.Tensor:\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = [p for p in parameters if p.grad is not None]\n    norm_type = float(norm_type)\n    if len(parameters) == 0:\n        return torch.tensor(0.)\n    device = parameters[0].grad.device\n    if norm_type == float('inf'):\n        total_norm = max(p.grad.detach().abs().max().to(device)\n                         for p in parameters)\n    else:\n        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(),\n                                                        norm_type).to(device) for p in parameters]), norm_type)\n    return total_norm\n\n\nclass NativeScalerWithGradNormCount:\n   ", "doc_id": "677867c9-eea9-4bd3-86f5-ae5204525c90", "embedding": null, "doc_hash": "3aec0b5ef06b2be7e6e4ec8220c353619709b01ea9c6afdd50bfdbb470725daa", "extra_info": {"file_path": "TinyViT/utils.py", "file_name": "utils.py"}, "node_info": {"start": 12402, "end": 14033}, "relationships": {"1": "0b54bce457ea9eb3886a328e8c755629fdfb9965", "2": "84f3b64f-6122-4e62-9705-623d8514f787", "3": "a2b4a975-2720-45eb-b279-99a44d2230d3"}}, "__type__": "1"}, "a2b4a975-2720-45eb-b279-99a44d2230d3": {"__data__": {"text": "                       norm_type).to(device) for p in parameters]), norm_type)\n    return total_norm\n\n\nclass NativeScalerWithGradNormCount:\n    state_dict_key = \"amp_scaler\"\n\n    def __init__(self, grad_scaler_enabled=True):\n        self._scaler = torch.cuda.amp.GradScaler(enabled=grad_scaler_enabled)\n\n    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n        self._scaler.scale(loss).backward(create_graph=create_graph)\n        if update_grad:\n            if clip_grad is not None and clip_grad > 0.0:\n                assert parameters is not None\n                # unscale the gradients of optimizer's assigned params in-place\n                self._scaler.unscale_(optimizer)\n                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n            else:\n                self._scaler.unscale_(optimizer)\n                norm = ampscaler_get_grad_norm(parameters)\n            self._scaler.step(optimizer)\n            self._scaler.update()\n        else:\n            norm = None\n        return norm\n\n    def state_dict(self):\n        return self._scaler.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self._scaler.load_state_dict(state_dict)\n\n\ndef is_main_process():\n    return dist.get_rank() == 0\n\n\ndef run_cmd(cmd, default=None):\n    try:\n        return subprocess.check_output(cmd.split(), universal_newlines=True).strip()\n    except:\n        if default is None:\n            raise\n        return default\n\n\ndef get_git_info():\n    return dict(\n        branch=run_cmd('git rev-parse --abbrev-ref HEAD', 'custom'),\n       ", "doc_id": "a2b4a975-2720-45eb-b279-99a44d2230d3", "embedding": null, "doc_hash": "1a15acbda20af2ae32ba331a4a8f192460dbbf4b854effdadcd840187b6a0f75", "extra_info": {"file_path": "TinyViT/utils.py", "file_name": "utils.py"}, "node_info": {"start": 14042, "end": 15662}, "relationships": {"1": "0b54bce457ea9eb3886a328e8c755629fdfb9965", "2": "677867c9-eea9-4bd3-86f5-ae5204525c90", "3": "0c7b77a4-343e-4635-8037-7f2079b85bf0"}}, "__type__": "1"}, "0c7b77a4-343e-4635-8037-7f2079b85bf0": {"__data__": {"text": "       return default\n\n\ndef get_git_info():\n    return dict(\n        branch=run_cmd('git rev-parse --abbrev-ref HEAD', 'custom'),\n        git_hash=run_cmd('git rev-parse --short HEAD', 'custom'),\n    )\n", "doc_id": "0c7b77a4-343e-4635-8037-7f2079b85bf0", "embedding": null, "doc_hash": "7832ed2ee111d05dde28c2db25cbb0fac371053adae1fb299a94ea5038b00530", "extra_info": {"file_path": "TinyViT/utils.py", "file_name": "utils.py"}, "node_info": {"start": 15631, "end": 15833}, "relationships": {"1": "0b54bce457ea9eb3886a328e8c755629fdfb9965", "2": "a2b4a975-2720-45eb-b279-99a44d2230d3"}}, "__type__": "1"}}}